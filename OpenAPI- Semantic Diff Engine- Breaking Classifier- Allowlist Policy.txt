Scope 3 (max) — OpenAPI Semantic Diff Engine + Breaking Classifier + Allowlist Policy
Here’s the ship-order diff chain for the full “max” classifier: schema narrowing + request/response semantics + unknown allowlist policy, integrated into BOM + release gates + changelog truth the same way you’ve governed everything else.
I’m going to describe the diffs in the same “tightening chain” style you’ve been using, and I’ll be explicit about the contract surfaces, manifests, and golden fixtures.
________________________________________
Diff 0 — Contract surfaces (declare what is governed)
You will be introducing new public-ish semantic surfaces (even if internal today):
•	openapi_normalize rules (what is scrubbed, how paths are normalized, how components are reduced)
•	“breaking vs non-breaking vs unknown” classification
•	the allowlist policy (explicit exceptions with reasons)
•	the release gate policy (how SemVer is enforced)
So the governance plan is:
1.	Golden fixtures lock classifier outputs on representative changes
2.	Manifest gate requires signal_logic_version bump when classifier semantics or allowlist policy changes
3.	BOM captures:
o	snapshot hash
o	validator manifest hash
o	classifier policy hash
o	diff report hash (release artifact)
________________________________________
Diff A — Normalized model extractor (stable diff substrate)
Add src/code_audit/web_api/openapi_normalize.py
Purpose: take an OpenAPI doc (dict) and produce a deterministic, scrubbed, normalized model that is specifically shaped for semantic diffing.
Key responsibilities:
A1) Volatility scrub (reuse your scrub policy)
•	Remove known volatile keys globally (servers, externalDocs, x-generated-at, etc.)
•	Scrub endpoint-specific volatile response examples if you have them
•	Enforce per-endpoint scrub budgets before diffing (same concept as the golden scrub audit)
A2) Path template normalization
•	/items/{id} and /items/{item_id} normalize to /items/{} shape key
•	Preserve the original path for diagnostics, but diff uses normalized key
A3) Operation extraction
Operation key becomes:
•	OP = "{METHOD} {NORM_PATH}"
Operation view contains:
•	parameters (path/query/header/cookie) keyed deterministically:
o	in + name (for query/header/cookie)
o	path params also keyed by name, but diff treats template placeholders as “position” equivalence
•	requestBody semantics (presence, required, media types, schema signature)
•	responses semantics (status code map, content-type map, schema signature)
•	stable metadata fields (optional): summary, tags (kept but not counted as breaking unless you decide)
A4) Schema signature / comparison view
You do not need full JSON Schema equivalence (it’s hard). You need “narrowing detection” and “unknown detection”.
Represent each schema node as a normalized dict with only semver-relevant fields:
•	type
•	nullable
•	enum (sorted)
•	numeric: minimum, maximum, exclusiveMinimum, exclusiveMaximum, multipleOf
•	strings: minLength, maxLength, pattern, format
•	arrays: minItems, maxItems, uniqueItems, items signature
•	objects: required list (sorted), properties signatures (stable order), additionalProperties signature/bool
•	composition: oneOf, anyOf, allOf signatures (sorted by stable hash)
•	refs: resolve $ref into signatures by walking components.schemas (with cycle detection)
Anything you cannot confidently compare becomes unknown rather than guessed.
________________________________________
Diff B — Semantic comparator + classification engine
Add src/code_audit/web_api/openapi_diff.py
Inputs:
•	normalized old model
•	normalized new model
•	policy allowlist file (optional)
Outputs:
•	structured diff report
Core data model:
•	breaking_changes: list[Change]
•	non_breaking_changes: list[Change]
•	unknown_changes: list[Change]
•	summary counts
•	breaking: bool computed as len(breaking_changes) > 0 unless allowlist permits
Each Change object includes:
•	kind (stable enum string)
•	op (operation key) or path
•	location (e.g., requestBody.content.application/json.schema)
•	before / after (small, scrubbed snippets or hashes)
•	reason (short explanation string)
Deterministic ordering:
•	sort by (kind, op_or_path, location).
________________________________________
Diff C — Schema semver narrowing comparator (max scope)
Add src/code_audit/web_api/schema_semver.py
This is the “brain” for breaking detection.
Provide a function:
compare_schema(before_sig, after_sig) -> SchemaDelta
SchemaDelta contains:
•	breaking: list[str] reasons
•	non_breaking: list[str]
•	unknown: list[str]
Breaking (examples):
•	type changed incompatibly (string→object)
•	enum narrowed (after is subset of before)
•	min increased / max decreased
•	maxLength decreased / minLength increased
•	nullable: true → false (configurable)
•	object: required keys added (optional→required)
•	additionalProperties tightened (true→false, or schema narrowed)
Non-breaking:
•	enum expanded
•	additional optional properties added
•	range widened
•	nullable false→true (usually non-breaking)
Unknown:
•	complex oneOf/anyOf restructuring unless both normalize identically after sorting
•	pattern changes (often breaking, but can be ambiguous)
•	format changes (can be breaking; treat as unknown unless you choose strict)
Make the strictness policy-driven (more below).
________________________________________
Diff D — Request/Response semantics (max scope)
In openapi_normalize you’ll extract request/response schema signatures per media type.
Classifier rules:
D1) Request body
Breaking:
•	requestBody removed if previously present
•	requestBody required toggled false→true
•	supported media type removed (e.g. drop application/json)
•	request schema narrowed (via schema comparator)
Non-breaking:
•	new optional requestBody added
•	new media type added
•	request schema widened
Unknown:
•	ambiguous content negotiation changes
•	multi-media-type schema conflicts
D2) Responses
Breaking:
•	remove a success response status code (2xx, and optionally 3xx if you treat redirects as part of contract)
•	remove response media type
•	response schema narrowed for success responses
Non-breaking:
•	add new response codes
•	add optional fields in responses
•	add new media types
Unknown:
•	changing status code categories (e.g. 200→204 semantics) can be breaking; treat as breaking if 2xx removed, otherwise unknown
You should decide whether you consider only 2xx “success contract.” Most governance stacks do.
________________________________________
Diff E — Unknown allowlist policy (with reason + governance)
Add tests/contracts/openapi_breaking_policy.json
Example:
{
  "version": 1,
  "allow_unknown": [
    {
      "kind": "schema_complex_change",
      "op": "GET /legacy/{}",
      "location": "responses.200.content.application/json.schema",
      "reason": "Legacy endpoint undergoing transition; complex oneOf changes allowed until v2"
    }
  ],
  "allow_breaking": []
}
Rules:
•	allowlist entries must be specific (at least kind + op OR path)
•	must include non-empty reason
•	must be sorted and unique (like everything else)
Governance:
•	include policy file in the golden manifest (unified gate)
•	add a bump gate: policy edits require signal_logic_version bump OR treat as umbrella-gated if promoted into golden manifest
•	add schema for this policy file (same as registry schema approach)
________________________________________
Diff F — Golden fixtures for diff classifier output (lock semantics)
Add fixtures like:
•	tests/fixtures/openapi_diff/old_*.json
•	tests/fixtures/openapi_diff/new_*.json
•	expected output JSON:
o	tests/fixtures/expected/openapi_diff_case_X.json
Cases (Scope 3 representative):
1.	removed operation (breaking)
2.	request schema enum narrowed (breaking)
3.	response property added (non-breaking)
4.	requestBody required false→true (breaking)
5.	success response 200 removed (breaking)
6.	oneOf restructure (unknown) + allowlisted (becomes non-breaking or “allowed unknown” depending on policy)
7.	nullable true→false (breaking if enabled)
8.	pattern change (unknown or breaking per policy)
These fixtures become canonical like translator/golden outputs.
________________________________________
Diff G — Release gate: classify diff between previous tag and current snapshot
G1) Acquire baseline snapshot
Use git show <prev_tag>:docs/openapi.json to avoid network. You already fetch tags in release workflow.
G2) Generate a diff report JSON
•	dist/openapi_diff_report.json
G3) Gate semver
•	If breaking changes exist (after allowlist filtering), require major bump:
o	current tag vX.Y.Z must have X > prevX
•	If unknown changes exist and are not allowlisted, choose policy:
o	either treat unknown as breaking for release
o	or require explicit allowlist entry
For maximum safety, treat unknown as breaking unless allowlisted.
________________________________________
Diff H — BOM includes OpenAPI diff report + policy hashes
Update BOM generator to add:
•	web_api.openapi_diff_report (present on tag builds)
•	web_api.openapi_breaking_policy (policy file hash)
•	optionally web_api.openapi_breaking_policy_schema
Release workflow attaches:
•	dist/openapi_diff_report.json
Now every release has:
•	“what changed” report
•	“what policy allowed unknown/breaking”
•	hashes for provenance
________________________________________
Diff I — Changelog truth enforcement (web api openapi only)
You already have “axes” in changelog template work.
Add an axis such as:
•	web_api_openapi: yes/no
•	web_api_openapi_breaking: yes/no
•	web_api_openapi_unknown: yes/no (optional)
Truth gate:
•	If changelog says breaking=yes → classifier must find breaking (or allowlisted breaking)
•	If changelog says breaking=no → classifier must find no breaking (after allowlist)
•	Same for unknown if you include it
This prevents metadata drift.
________________________________________
Diff J — Manifests and version bump discipline
Because classifier semantics are now a contract surface, you add:
•	tests/contracts/openapi_diff_policy_manifest.json (or fold into existing translator_policy_manifest style)
•	Gate: AST hash of:
o	openapi_normalize.py
o	openapi_diff.py
o	schema_semver.py
o	plus any constants and allowlist schema keys
•	Requirement: bump signal_logic_version on semantic drift
If you prefer “one gate to rule them all,” you can also include:
•	policy file
•	golden diff expected outputs
under the unified golden manifest gate (still requires bump on change).
For max governance, you’ll have both:
•	golden fixtures (behavior lock)
•	policy manifest (semantic surface lock)
________________________________________
Minimal implementation choice for unknowns (Scope 3 default)
For Scope 3, I’d recommend:
•	Unknown changes are treated as breaking unless allowlisted.
•	Allowlist requires reason.
•	Allowlist entries are matched by (kind, op, location) to avoid broad exemptions.
That gives you “maximum operator safety.”
________________________________________
The very next “ship-ready” diff I would start with
If you want to proceed exactly like the previous chains (one diff at a time), the first concrete diff to ship is:
next diff: openapi_normalize + schema signature extraction
Because everything else depends on a stable normalization layer.
If you want, tell me:
•	Do you currently store your OpenAPI snapshot at docs/openapi.json (as assumed), and
•	Do you want baseline diffing to use previous tag’s snapshot via git show (recommended)?
If you don’t answer, I’ll assume yes to both and continue the chain with those defaults.
ship next diff: openapi_normalize + schema signature extraction
Thought for 12s
Below is the next diff: openapi_normalize + schema signature extraction (Scope 3 foundation).
This ships a deterministic normalization layer that:
•	scrubs volatility (configurable, conservative defaults)
•	normalizes templated paths ({id} ≡ {item_id} via /.../{} shape keys)
•	extracts operations keyed as "METHOD /path/{}"
•	extracts schema signatures with $ref resolution (cycle-safe)
•	produces a stable “normalized model” specifically designed for semantic diffing
________________________________________
Diff: add OpenAPI normalizer + schema signature extraction
1) New file: src/code_audit/web_api/openapi_normalize.py
diff --git a/src/code_audit/web_api/openapi_normalize.py b/src/code_audit/web_api/openapi_normalize.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/code_audit/web_api/openapi_normalize.py
@@ -0,0 +1,468 @@
+from __future__ import annotations
+
+import copy
+import re
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Tuple
+
+from code_audit.web_api.openapi_path_match import normalize_openapi_path_template
+
+
+# ----------------------------
+# Volatility scrub defaults
+# ----------------------------
+
+# Conservative defaults: remove the most common volatility sources without harming semantics.
+# If you already have a scrub policy engine, you can pass it via normalize_openapi(..., scrubber=...).
+DEFAULT_VOLATILE_TOPLEVEL_KEYS = {
+    "servers",
+    "externalDocs",
+}
+
+# OpenAPI "info" can include volatile bits; keep title/version by default, scrub obvious timestamp-ish fields.
+DEFAULT_VOLATILE_INFO_KEYS = {
+    "x-generated-at",
+    "x_build_time",
+    "x-build-time",
+    "buildTime",
+    "generatedAt",
+}
+
+# Vendor extensions are frequently volatile; we keep them unless explicitly scrubbed by a custom scrubber.
+# (Scope 3 wants max safety; do not blanket-remove all x-* here.)
+
+
+def _deepcopy(obj: Any) -> Any:
+    # Defensive: keep normalization pure and side-effect free.
+    return copy.deepcopy(obj)
+
+
+def _is_ref(obj: Any) -> bool:
+    return isinstance(obj, dict) and "$ref" in obj and isinstance(obj["$ref"], str)
+
+
+_REF_RE = re.compile(r"^#/components/schemas/(?P<name>[^/]+)$")
+
+
+def _ref_name(ref: str) -> str | None:
+    m = _REF_RE.match(ref)
+    if not m:
+        return None
+    return m.group("name")
+
+
+def _stable_sorted_list(values: Iterable[Any]) -> List[Any]:
+    # Best-effort stable sort: if values are dict/list, sort by repr (deterministic for our normalized shapes).
+    # This is used only for canonicalizing sets like enum/required keys, not for human-facing ordering.
+    return sorted(values, key=lambda x: repr(x))
+
+
+def _stable_dict(d: Mapping[str, Any]) -> Dict[str, Any]:
+    return {k: d[k] for k in sorted(d.keys())}
+
+
+def _strip_none(d: Dict[str, Any]) -> Dict[str, Any]:
+    return {k: v for k, v in d.items() if v is not None}
+
+
+# ----------------------------
+# Schema signature extraction
+# ----------------------------
+
+@dataclass(frozen=True)
+class SchemaSig:
+    """
+    A normalized, semver-relevant schema signature used for diffing.
+    It is intentionally lossy: excludes descriptions/examples, focuses on constraints.
+    """
+    sig: Dict[str, Any]
+
+
+def schema_signature(
+    schema: Any,
+    *,
+    components_schemas: Mapping[str, Any] | None,
+    treat_format_as_semantic: bool = False,
+    treat_pattern_as_semantic: bool = False,
+) -> SchemaSig:
+    """
+    Produce a deterministic signature for an OpenAPI/JSON Schema node.
+
+    - Resolves $ref within #/components/schemas/*
+    - Cycle-safe: if a $ref cycle is detected, emits {"$ref_cycle": "<name>"} at the cycle point
+    - Drops non-semantic fields: title, description, examples, deprecated, etc.
+    - Canonicalizes ordering: required/enum sorted; properties sorted; composition lists sorted by repr(sig)
+
+    The returned dict is suitable for stable JSON serialization (sort_keys=True).
+    """
+    comps = dict(components_schemas or {})
+    visiting: set[str] = set()
+
+    def sig_node(node: Any) -> Dict[str, Any]:
+        if node is None:
+            return {"type": "null"}
+        if isinstance(node, bool):
+            # JSON Schema boolean form
+            return {"bool_schema": node}
+        if not isinstance(node, dict):
+            # Unknown schema shape; preserve type info only.
+            return {"unknown_schema": type(node).__name__}
+
+        if _is_ref(node):
+            ref = node["$ref"]
+            name = _ref_name(ref)
+            if name is None:
+                # External or unsupported refs -> unknown bucket for later policy handling
+                return {"$ref": ref, "ref_kind": "external_or_unsupported"}
+            if name in visiting:
+                return {"$ref_cycle": name}
+            target = comps.get(name)
+            if target is None:
+                return {"$ref": ref, "ref_kind": "missing_component"}
+            visiting.add(name)
+            out = sig_node(target)
+            visiting.remove(name)
+            # Preserve ref target name so diagnostics can show identity even after normalization.
+            return {"$ref_resolved": name, "target": out}
+
+        # Copy only semver-relevant fields
+        t = node.get("type")
+        nullable = node.get("nullable")
+        enum = node.get("enum")
+
+        out: Dict[str, Any] = {}
+
+        if isinstance(t, str):
+            out["type"] = t
+        elif isinstance(t, list):
+            # Sometimes OpenAPI uses ["string","null"] instead of nullable
+            out["type"] = _stable_sorted_list([x for x in t if isinstance(x, str)])
+
+        if isinstance(nullable, bool):
+            out["nullable"] = nullable
+
+        if isinstance(enum, list):
+            # Canonicalize enum values deterministically
+            out["enum"] = _stable_sorted_list(enum)
+
+        # String constraints
+        for k in ("minLength", "maxLength"):
+            v = node.get(k)
+            if isinstance(v, int):
+                out[k] = v
+        if treat_pattern_as_semantic:
+            v = node.get("pattern")
+            if isinstance(v, str):
+                out["pattern"] = v
+        if treat_format_as_semantic:
+            v = node.get("format")
+            if isinstance(v, str):
+                out["format"] = v
+
+        # Numeric constraints
+        for k in ("minimum", "maximum", "exclusiveMinimum", "exclusiveMaximum", "multipleOf"):
+            v = node.get(k)
+            if isinstance(v, (int, float)):
+                out[k] = v
+
+        # Array constraints
+        for k in ("minItems", "maxItems"):
+            v = node.get(k)
+            if isinstance(v, int):
+                out[k] = v
+        v = node.get("uniqueItems")
+        if isinstance(v, bool):
+            out["uniqueItems"] = v
+        if "items" in node:
+            out["items"] = sig_node(node.get("items"))
+
+        # Object constraints
+        req = node.get("required")
+        if isinstance(req, list):
+            out["required"] = _stable_sorted_list([x for x in req if isinstance(x, str)])
+
+        props = node.get("properties")
+        if isinstance(props, dict):
+            out_props: Dict[str, Any] = {}
+            for pk in sorted(props.keys()):
+                out_props[pk] = sig_node(props[pk])
+            out["properties"] = out_props
+
+        # additionalProperties can be bool or schema
+        if "additionalProperties" in node:
+            ap = node.get("additionalProperties")
+            if isinstance(ap, bool):
+                out["additionalProperties"] = ap
+            else:
+                out["additionalProperties"] = sig_node(ap)
+
+        # Composition
+        for comp_key in ("oneOf", "anyOf", "allOf"):
+            comp = node.get(comp_key)
+            if isinstance(comp, list):
+                comp_sigs = [sig_node(x) for x in comp]
+                # Sort composition options deterministically by repr
+                out[comp_key] = sorted(comp_sigs, key=lambda x: repr(x))
+
+        return _stable_dict(out)
+
+    return SchemaSig(sig=sig_node(schema))
+
+
+# ----------------------------
+# OpenAPI normalization
+# ----------------------------
+
+@dataclass(frozen=True)
+class NormalizedParam:
+    in_: str
+    name: str
+    required: bool | None
+    schema: Dict[str, Any] | None  # schema signature dict
+
+
+@dataclass(frozen=True)
+class NormalizedMedia:
+    content_type: str
+    schema: Dict[str, Any] | None  # schema signature dict
+
+
+@dataclass(frozen=True)
+class NormalizedRequestBody:
+    present: bool
+    required: bool | None
+    contents: List[NormalizedMedia]
+
+
+@dataclass(frozen=True)
+class NormalizedResponse:
+    status: str
+    contents: List[NormalizedMedia]
+
+
+@dataclass(frozen=True)
+class NormalizedOperation:
+    op: str  # e.g. "GET /items/{}"
+    method: str
+    normalized_path: str
+    source_paths: List[str]  # paths in the original doc that mapped here
+    params: List[NormalizedParam]  # sorted deterministically
+    request_body: NormalizedRequestBody
+    responses: List[NormalizedResponse]  # sorted by status
+
+
+@dataclass(frozen=True)
+class NormalizedOpenAPI:
+    """
+    Deterministic, semver-oriented OpenAPI view for diffing.
+    """
+    version: int
+    operations: List[NormalizedOperation]  # sorted by op
+    meta: Dict[str, Any]  # stable metadata (title/version)
+
+
+Scrubber = Callable[[Dict[str, Any]], Dict[str, Any]]
+
+
+def normalize_openapi(
+    openapi: Dict[str, Any],
+    *,
+    scrubber: Scrubber | None = None,
+    treat_format_as_semantic: bool = False,
+    treat_pattern_as_semantic: bool = False,
+) -> NormalizedOpenAPI:
+    """
+    Normalize an OpenAPI document into a deterministic, semver-relevant model.
+
+    If scrubber is provided, it is applied first to a deep copy of the document.
+    Otherwise conservative default scrubbing is applied.
+    """
+    doc = _deepcopy(openapi)
+    if scrubber is not None:
+        doc = scrubber(doc)
+    else:
+        doc = _default_scrub(doc)
+
+    components = doc.get("components") if isinstance(doc.get("components"), dict) else {}
+    components_schemas = components.get("schemas") if isinstance(components.get("schemas"), dict) else {}
+
+    info = doc.get("info") if isinstance(doc.get("info"), dict) else {}
+    meta = {}
+    if isinstance(info.get("title"), str):
+        meta["title"] = info["title"]
+    if isinstance(info.get("version"), str):
+        meta["version"] = info["version"]
+    meta = _stable_dict(meta)
+
+    paths = doc.get("paths")
+    if not isinstance(paths, dict):
+        # No paths means no operations; still deterministic
+        return NormalizedOpenAPI(version=1, operations=[], meta=meta)
+
+    # Build a mapping: normalized_path -> list of (source_path, path_item)
+    norm_path_map: Dict[str, List[Tuple[str, Dict[str, Any]]]] = {}
+    for source_path, path_item in paths.items():
+        if not isinstance(source_path, str) or not isinstance(path_item, dict):
+            continue
+        norm = normalize_openapi_path_template(source_path)
+        norm_path_map.setdefault(norm, []).append((source_path, path_item))
+
+    ops: List[NormalizedOperation] = []
+    for norm_path in sorted(norm_path_map.keys()):
+        entries = norm_path_map[norm_path]
+        # Deterministic: sort source paths
+        entries = sorted(entries, key=lambda x: x[0])
+        source_paths = [p for (p, _) in entries]
+
+        # Merge operations: for the same normalized path, we expect at most one operation per method.
+        # If multiple source paths define the same method, we keep them all for diagnostics but merge semantics
+        # conservatively by preferring the first in sorted order (this is only normalization; classifier can flag conflicts later).
+        method_to_op: Dict[str, Dict[str, Any]] = {}
+        method_to_source: Dict[str, List[str]] = {}
+
+        for sp, item in entries:
+            for method, op_obj in item.items():
+                if method.lower() not in ("get", "post", "put", "patch", "delete", "head", "options", "trace"):
+                    continue
+                if not isinstance(op_obj, dict):
+                    continue
+                m = method.upper()
+                method_to_source.setdefault(m, []).append(sp)
+                if m not in method_to_op:
+                    method_to_op[m] = op_obj
+
+        for method in sorted(method_to_op.keys()):
+            op_obj = method_to_op[method]
+            op_key = f"{method} {norm_path}"
+
+            params = _extract_parameters(
+                op_obj,
+                *,
+                components_schemas=components_schemas,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+            req = _extract_request_body(
+                op_obj,
+                *,
+                components_schemas=components_schemas,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+            resps = _extract_responses(
+                op_obj,
+                *,
+                components_schemas=components_schemas,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+
+            ops.append(
+                NormalizedOperation(
+                    op=op_key,
+                    method=method,
+                    normalized_path=norm_path,
+                    source_paths=_stable_sorted_list(method_to_source.get(method, source_paths)),
+                    params=params,
+                    request_body=req,
+                    responses=resps,
+                )
+            )
+
+    ops = sorted(ops, key=lambda o: o.op)
+    return NormalizedOpenAPI(version=1, operations=ops, meta=meta)
+
+
+def _default_scrub(doc: Dict[str, Any]) -> Dict[str, Any]:
+    # Remove top-level volatile keys
+    for k in list(doc.keys()):
+        if k in DEFAULT_VOLATILE_TOPLEVEL_KEYS:
+            doc.pop(k, None)
+
+    # Scrub volatility-ish keys inside info
+    info = doc.get("info")
+    if isinstance(info, dict):
+        for k in list(info.keys()):
+            if k in DEFAULT_VOLATILE_INFO_KEYS:
+                info.pop(k, None)
+        doc["info"] = info
+
+    return doc
+
+
+def _extract_parameters(
+    op_obj: Dict[str, Any],
+    *,
+    components_schemas: Mapping[str, Any],
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
+) -> List[NormalizedParam]:
+    params = op_obj.get("parameters")
+    out: List[NormalizedParam] = []
+    if isinstance(params, list):
+        for p in params:
+            if not isinstance(p, dict):
+                continue
+            in_ = p.get("in")
+            name = p.get("name")
+            if not isinstance(in_, str) or not isinstance(name, str):
+                continue
+            required = p.get("required")
+            req_bool = required if isinstance(required, bool) else None
+            sch = p.get("schema")
+            sch_sig = None
+            if sch is not None:
+                sch_sig = schema_signature(
+                    sch,
+                    components_schemas=components_schemas,
+                    treat_format_as_semantic=treat_format_as_semantic,
+                    treat_pattern_as_semantic=treat_pattern_as_semantic,
+                ).sig
+            out.append(NormalizedParam(in_=in_, name=name, required=req_bool, schema=sch_sig))
+
+    # Deterministic ordering: (in, name)
+    out = sorted(out, key=lambda x: (x.in_, x.name))
+    return out
+
+
+def _extract_request_body(
+    op_obj: Dict[str, Any],
+    *,
+    components_schemas: Mapping[str, Any],
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
+) -> NormalizedRequestBody:
+    rb = op_obj.get("requestBody")
+    if not isinstance(rb, dict):
+        return NormalizedRequestBody(present=False, required=None, contents=[])
+
+    required = rb.get("required")
+    req_bool = required if isinstance(required, bool) else None
+    content = rb.get("content")
+    contents: List[NormalizedMedia] = []
+
+    if isinstance(content, dict):
+        for ctype in sorted(content.keys()):
+            cobj = content.get(ctype)
+            if not isinstance(cobj, dict):
+                continue
+            sch = None
+            if isinstance(cobj.get("schema"), (dict, bool)) or cobj.get("schema") is None:
+                sch = cobj.get("schema")
+            sch_sig = None
+            if sch is not None:
+                sch_sig = schema_signature(
+                    sch,
+                    components_schemas=components_schemas,
+                    treat_format_as_semantic=treat_format_as_semantic,
+                    treat_pattern_as_semantic=treat_pattern_as_semantic,
+                ).sig
+            contents.append(NormalizedMedia(content_type=ctype, schema=sch_sig))
+
+    return NormalizedRequestBody(present=True, required=req_bool, contents=contents)
+
+
+def _extract_responses(
+    op_obj: Dict[str, Any],
+    *,
+    components_schemas: Mapping[str, Any],
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
+) -> List[NormalizedResponse]:
+    responses = op_obj.get("responses")
+    out: List[NormalizedResponse] = []
+    if not isinstance(responses, dict):
+        return out
+
+    for status in sorted(responses.keys(), key=lambda s: str(s)):
+        robj = responses.get(status)
+        if not isinstance(robj, dict):
+            continue
+        content = robj.get("content")
+        contents: List[NormalizedMedia] = []
+        if isinstance(content, dict):
+            for ctype in sorted(content.keys()):
+                cobj = content.get(ctype)
+                if not isinstance(cobj, dict):
+                    continue
+                sch = cobj.get("schema")
+                sch_sig = None
+                if sch is not None:
+                    sch_sig = schema_signature(
+                        sch,
+                        components_schemas=components_schemas,
+                        treat_format_as_semantic=treat_format_as_semantic,
+                        treat_pattern_as_semantic=treat_pattern_as_semantic,
+                    ).sig
+                contents.append(NormalizedMedia(content_type=ctype, schema=sch_sig))
+
+        out.append(NormalizedResponse(status=str(status), contents=contents))
+
+    return out
________________________________________
2) New tests: schema signature extraction + normalization determinism
2a) tests/test_openapi_schema_signature_extraction.py
diff --git a/tests/test_openapi_schema_signature_extraction.py b/tests/test_openapi_schema_signature_extraction.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_schema_signature_extraction.py
@@ -0,0 +1,124 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_normalize import schema_signature
+
+
+def test_schema_signature_sorts_enum_and_required_and_properties() -> None:
+    schema = {
+        "type": "object",
+        "required": ["b", "a"],
+        "properties": {
+            "z": {"type": "string", "enum": ["b", "a"]},
+            "a": {"type": "integer", "minimum": 1},
+        },
+        "description": "ignored",
+        "example": {"a": 1},
+    }
+
+    sig = schema_signature(schema, components_schemas={}).sig
+    assert sig["type"] == "object"
+    assert sig["required"] == ["a", "b"]
+    assert list(sig["properties"].keys()) == ["a", "z"]
+    assert sig["properties"]["z"]["enum"] == ["a", "b"]
+    assert "description" not in sig
+    assert "example" not in sig
+
+
+def test_schema_signature_resolves_ref_and_is_cycle_safe() -> None:
+    components = {
+        "Node": {
+            "type": "object",
+            "properties": {
+                "next": {"$ref": "#/components/schemas/Node"}
+            },
+        }
+    }
+
+    root = {"$ref": "#/components/schemas/Node"}
+    sig = schema_signature(root, components_schemas=components).sig
+    assert sig["$ref_resolved"] == "Node"
+    # cycle should be represented at the recursive point
+    assert sig["target"]["properties"]["next"]["target"]["$ref_cycle"] == "Node"
+
+
+def test_schema_signature_external_ref_is_marked_unknown() -> None:
+    sig = schema_signature({"$ref": "https://example.com/schema.json"}, components_schemas={}).sig
+    assert sig["ref_kind"] == "external_or_unsupported"
2b) tests/test_openapi_normalize_basic.py
diff --git a/tests/test_openapi_normalize_basic.py b/tests/test_openapi_normalize_basic.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_normalize_basic.py
@@ -0,0 +1,139 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+def test_normalize_openapi_merges_template_paths_by_shape_and_keys_ops() -> None:
+    doc = {
+        "openapi": "3.0.0",
+        "info": {"title": "t", "version": "1"},
+        "paths": {
+            "/items/{id}": {
+                "get": {
+                    "responses": {"200": {"description": "ok"}}
+                }
+            },
+            "/items/{item_id}": {
+                "post": {
+                    "responses": {"201": {"description": "ok"}}
+                }
+            },
+        },
+    }
+
+    n = normalize_openapi(doc)
+    ops = [o.op for o in n.operations]
+    assert ops == ["GET /items/{}", "POST /items/{}"]
+
+
+def test_normalize_openapi_extracts_request_and_response_schema_signatures() -> None:
+    doc = {
+        "openapi": "3.0.0",
+        "info": {"title": "t", "version": "1"},
+        "components": {"schemas": {"X": {"type": "string", "enum": ["b", "a"]}}},
+        "paths": {
+            "/x/{id}": {
+                "put": {
+                    "requestBody": {
+                        "required": True,
+                        "content": {
+                            "application/json": {"schema": {"$ref": "#/components/schemas/X"}}
+                        },
+                    },
+                    "responses": {
+                        "200": {
+                            "content": {
+                                "application/json": {"schema": {"type": "integer", "minimum": 0}}
+                            }
+                        }
+                    },
+                }
+            }
+        },
+    }
+
+    n = normalize_openapi(doc)
+    op = n.operations[0]
+    assert op.op == "PUT /x/{}"
+    assert op.request_body.present is True
+    assert op.request_body.required is True
+    assert op.request_body.contents[0].content_type == "application/json"
+    # enum should be sorted in signature
+    assert op.request_body.contents[0].schema["target"]["enum"] == ["a", "b"]
+    assert op.responses[0].status == "200"
+    assert op.responses[0].contents[0].schema["minimum"] == 0
________________________________________
Notes on intentional design choices (for Scope 3 stability)
•	Schema signatures are intentionally lossy. They keep constraint semantics, drop descriptions/examples.
•	Composition (oneOf/anyOf/allOf) is retained but normalized; complex changes are classifiable later (often unknown unless identical after normalization).
•	External refs are marked as "external_or_unsupported" so the classifier can route them to unknown or require allowlisting.
•	Normalization merges paths by template shape but does not try to “merge conflicting operations.” It deterministically prefers the first (sorted) definition for the semantic body and keeps source_paths for later diagnostics—your diff engine can flag conflicts as unknown_changes if needed.
________________________________________
Next diff after this (recommended)
Once this substrate lands, the next diff is the actual diff engine skeleton:
next diff: openapi_diff core (added/removed path+operation+param + request/response presence) + report JSON model
Ship next diff: openapi_diff core (added/removed path+operation+param + request/response presence
next diff: openapi_diff core (paths/ops/params + request/response presence) + report model
This ships the first real classifier on top of your openapi_normalize substrate. It does not do schema narrowing yet (that’s the next layer), but it fully covers the high-signal structural breaks:
•	added/removed operations
•	added/removed/tightened parameters
•	requestBody presence/required changes
•	response status-code + media-type presence changes (success-focused)
It also emits a deterministic, structured diff report ready to plug into BOM/release gates.
________________________________________
Diff 1 — Add diff engine core
New file: src/code_audit/web_api/openapi_diff.py
diff --git a/src/code_audit/web_api/openapi_diff.py b/src/code_audit/web_api/openapi_diff.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/code_audit/web_api/openapi_diff.py
@@ -0,0 +1,390 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Tuple
+
+from code_audit.web_api.openapi_normalize import (
+    NormalizedOpenAPI,
+    NormalizedOperation,
+    NormalizedParam,
+    NormalizedRequestBody,
+    NormalizedResponse,
+)
+
+
+# ----------------------------
+# Diff report model
+# ----------------------------
+
+
+@dataclass(frozen=True)
+class Change:
+    kind: str
+    op: str | None = None
+    path: str | None = None
+    location: str | None = None
+    detail: str | None = None
+    before: Any | None = None
+    after: Any | None = None
+
+
+@dataclass(frozen=True)
+class DiffSummary:
+    breaking_count: int
+    non_breaking_count: int
+    unknown_count: int
+
+
+@dataclass(frozen=True)
+class OpenAPIDiffReport:
+    breaking: bool
+    breaking_changes: List[Change]
+    non_breaking_changes: List[Change]
+    unknown_changes: List[Change]
+    summary: DiffSummary
+    version: int = 1
+
+    def to_dict(self) -> Dict[str, Any]:
+        def ch(c: Change) -> Dict[str, Any]:
+            d: Dict[str, Any] = {"kind": c.kind}
+            if c.op is not None:
+                d["op"] = c.op
+            if c.path is not None:
+                d["path"] = c.path
+            if c.location is not None:
+                d["location"] = c.location
+            if c.detail is not None:
+                d["detail"] = c.detail
+            if c.before is not None:
+                d["before"] = c.before
+            if c.after is not None:
+                d["after"] = c.after
+            return d
+
+        # Deterministic ordering: caller already sorts lists; still stable if used directly.
+        return {
+            "version": self.version,
+            "breaking": self.breaking,
+            "summary": {
+                "breaking_count": self.summary.breaking_count,
+                "non_breaking_count": self.summary.non_breaking_count,
+                "unknown_count": self.summary.unknown_count,
+            },
+            "breaking_changes": [ch(x) for x in self.breaking_changes],
+            "non_breaking_changes": [ch(x) for x in self.non_breaking_changes],
+            "unknown_changes": [ch(x) for x in self.unknown_changes],
+        }
+
+
+# ----------------------------
+# Core classifier (Scope-3 foundation)
+# ----------------------------
+
+
+DEFAULT_ALLOWED_METHODS = {"GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"}
+
+
+def diff_openapi_core(
+    before: NormalizedOpenAPI,
+    after: NormalizedOpenAPI,
+    *,
+    success_status_prefixes: Tuple[str, ...] = ("2",),  # treat only 2xx as success by default
+) -> OpenAPIDiffReport:
+    """
+    Core OpenAPI semantic diff classifier:
+      - added/removed operations (path+method)
+      - parameter removed / required tightened (breaking)
+      - parameter added (non-breaking if optional; unknown if required and no baseline)
+      - requestBody presence/required changes
+      - responses: success status removed (breaking); media type removed (breaking)
+
+    This intentionally does NOT do schema narrowing yet. That comes in schema_semver layer.
+    """
+    b_ops = {op.op: op for op in before.operations}
+    a_ops = {op.op: op for op in after.operations}
+
+    breaking: List[Change] = []
+    non_breaking: List[Change] = []
+    unknown: List[Change] = []
+
+    b_keys = set(b_ops.keys())
+    a_keys = set(a_ops.keys())
+
+    removed_ops = sorted(b_keys - a_keys)
+    added_ops = sorted(a_keys - b_keys)
+
+    for op in removed_ops:
+        breaking.append(Change(kind="removed_operation", op=op, detail="Operation removed"))
+
+    for op in added_ops:
+        non_breaking.append(Change(kind="added_operation", op=op, detail="Operation added"))
+
+    # Compare shared operations
+    for op_key in sorted(b_keys & a_keys):
+        b = b_ops[op_key]
+        a = a_ops[op_key]
+        _diff_operation_core(
+            op_key,
+            b,
+            a,
+            breaking,
+            non_breaking,
+            unknown,
+            success_status_prefixes=success_status_prefixes,
+        )
+
+    # Deterministic sort
+    breaking = sorted(breaking, key=_change_sort_key)
+    non_breaking = sorted(non_breaking, key=_change_sort_key)
+    unknown = sorted(unknown, key=_change_sort_key)
+
+    report = OpenAPIDiffReport(
+        breaking=len(breaking) > 0,
+        breaking_changes=breaking,
+        non_breaking_changes=non_breaking,
+        unknown_changes=unknown,
+        summary=DiffSummary(
+            breaking_count=len(breaking),
+            non_breaking_count=len(non_breaking),
+            unknown_count=len(unknown),
+        ),
+    )
+    return report
+
+
+def _change_sort_key(c: Change) -> tuple:
+    return (
+        c.kind or "",
+        c.op or "",
+        c.path or "",
+        c.location or "",
+        c.detail or "",
+        repr(c.before) if c.before is not None else "",
+        repr(c.after) if c.after is not None else "",
+    )
+
+
+def _param_key(p: NormalizedParam) -> tuple[str, str]:
+    return (p.in_, p.name)
+
+
+def _param_map(params: List[NormalizedParam]) -> Dict[tuple[str, str], NormalizedParam]:
+    return {_param_key(p): p for p in params}
+
+
+def _diff_operation_core(
+    op_key: str,
+    before: NormalizedOperation,
+    after: NormalizedOperation,
+    breaking: List[Change],
+    non_breaking: List[Change],
+    unknown: List[Change],
+    *,
+    success_status_prefixes: Tuple[str, ...],
+) -> None:
+    # ---- Parameters ----
+    bmap = _param_map(before.params)
+    amap = _param_map(after.params)
+
+    bkeys = set(bmap.keys())
+    akeys = set(amap.keys())
+
+    for k in sorted(bkeys - akeys):
+        p = bmap[k]
+        breaking.append(
+            Change(
+                kind="removed_parameter",
+                op=op_key,
+                location=f"parameters.{p.in_}.{p.name}",
+                detail="Parameter removed",
+                before={"in": p.in_, "name": p.name, "required": p.required},
+                after=None,
+            )
+        )
+
+    for k in sorted(akeys - bkeys):
+        p = amap[k]
+        # Adding an optional param is non-breaking. Adding a required param is generally breaking,
+        # but OpenAPI required flags can be absent; if required is True, call it breaking.
+        if p.required is True:
+            breaking.append(
+                Change(
+                    kind="added_required_parameter",
+                    op=op_key,
+                    location=f"parameters.{p.in_}.{p.name}",
+                    detail="Required parameter added",
+                    before=None,
+                    after={"in": p.in_, "name": p.name, "required": p.required},
+                )
+            )
+        else:
+            non_breaking.append(
+                Change(
+                    kind="added_optional_parameter",
+                    op=op_key,
+                    location=f"parameters.{p.in_}.{p.name}",
+                    detail="Optional parameter added",
+                    before=None,
+                    after={"in": p.in_, "name": p.name, "required": p.required},
+                )
+            )
+
+    for k in sorted(bkeys & akeys):
+        bp = bmap[k]
+        ap = amap[k]
+        # required tightened: False/None -> True is breaking
+        if (bp.required is False or bp.required is None) and ap.required is True:
+            breaking.append(
+                Change(
+                    kind="parameter_required_tightened",
+                    op=op_key,
+                    location=f"parameters.{ap.in_}.{ap.name}.required",
+                    detail="Parameter became required",
+                    before={"required": bp.required},
+                    after={"required": ap.required},
+                )
+            )
+
+    # ---- Request body presence/required ----
+    _diff_request_body(op_key, before.request_body, after.request_body, breaking, non_breaking, unknown)
+
+    # ---- Responses presence (status code + media types) ----
+    _diff_responses_presence(
+        op_key,
+        before.responses,
+        after.responses,
+        breaking,
+        non_breaking,
+        unknown,
+        success_status_prefixes=success_status_prefixes,
+    )
+
+
+def _diff_request_body(
+    op_key: str,
+    before: NormalizedRequestBody,
+    after: NormalizedRequestBody,
+    breaking: List[Change],
+    non_breaking: List[Change],
+    unknown: List[Change],
+) -> None:
+    if before.present and not after.present:
+        breaking.append(
+            Change(
+                kind="removed_request_body",
+                op=op_key,
+                location="requestBody",
+                detail="Request body removed",
+                before={"present": True, "required": before.required},
+                after={"present": False},
+            )
+        )
+        return
+
+    if not before.present and after.present:
+        # If request body newly appears and is required, that is breaking.
+        if after.required is True:
+            breaking.append(
+                Change(
+                    kind="added_required_request_body",
+                    op=op_key,
+                    location="requestBody",
+                    detail="Required request body added",
+                    before={"present": False},
+                    after={"present": True, "required": True},
+                )
+            )
+        else:
+            non_breaking.append(
+                Change(
+                    kind="added_optional_request_body",
+                    op=op_key,
+                    location="requestBody",
+                    detail="Optional request body added",
+                    before={"present": False},
+                    after={"present": True, "required": after.required},
+                )
+            )
+        return
+
+    if before.present and after.present:
+        # Required tightened: False/None -> True is breaking
+        if (before.required is False or before.required is None) and after.required is True:
+            breaking.append(
+                Change(
+                    kind="request_body_required_tightened",
+                    op=op_key,
+                    location="requestBody.required",
+                    detail="Request body became required",
+                    before={"required": before.required},
+                    after={"required": after.required},
+                )
+            )
+
+        # Media type removals/additions (presence only; schema narrowing later)
+        b_ct = {m.content_type for m in before.contents}
+        a_ct = {m.content_type for m in after.contents}
+
+        for ct in sorted(b_ct - a_ct):
+            breaking.append(
+                Change(
+                    kind="removed_request_media_type",
+                    op=op_key,
+                    location=f"requestBody.content.{ct}",
+                    detail="Request media type removed",
+                    before={"content_type": ct},
+                    after=None,
+                )
+            )
+        for ct in sorted(a_ct - b_ct):
+            non_breaking.append(
+                Change(
+                    kind="added_request_media_type",
+                    op=op_key,
+                    location=f"requestBody.content.{ct}",
+                    detail="Request media type added",
+                    before=None,
+                    after={"content_type": ct},
+                )
+            )
+
+
+def _diff_responses_presence(
+    op_key: str,
+    before: List[NormalizedResponse],
+    after: List[NormalizedResponse],
+    breaking: List[Change],
+    non_breaking: List[Change],
+    unknown: List[Change],
+    *,
+    success_status_prefixes: Tuple[str, ...],
+) -> None:
+    bmap = {r.status: r for r in before}
+    amap = {r.status: r for r in after}
+    bkeys = set(bmap.keys())
+    akeys = set(amap.keys())
+
+    def is_success(status: str) -> bool:
+        return any(status.startswith(p) for p in success_status_prefixes)
+
+    for status in sorted(bkeys - akeys):
+        if is_success(status):
+            breaking.append(
+                Change(
+                    kind="removed_success_response_status",
+                    op=op_key,
+                    location=f"responses.{status}",
+                    detail="Success response status removed",
+                    before={"status": status},
+                    after=None,
+                )
+            )
+        else:
+            # Removing non-success responses might be non-breaking; keep as unknown until policy decides.
+            unknown.append(
+                Change(
+                    kind="removed_non_success_response_status",
+                    op=op_key,
+                    location=f"responses.{status}",
+                    detail="Non-success response status removed",
+                    before={"status": status},
+                    after=None,
+                )
+            )
+
+    for status in sorted(akeys - bkeys):
+        # Adding responses is usually non-breaking
+        non_breaking.append(
+            Change(
+                kind="added_response_status",
+                op=op_key,
+                location=f"responses.{status}",
+                detail="Response status added",
+                before=None,
+                after={"status": status},
+            )
+        )
+
+    for status in sorted(bkeys & akeys):
+        br = bmap[status]
+        ar = amap[status]
+        b_ct = {m.content_type for m in br.contents}
+        a_ct = {m.content_type for m in ar.contents}
+
+        for ct in sorted(b_ct - a_ct):
+            # Removing response media types for success is breaking; for non-success unknown.
+            if is_success(status):
+                breaking.append(
+                    Change(
+                        kind="removed_success_response_media_type",
+                        op=op_key,
+                        location=f"responses.{status}.content.{ct}",
+                        detail="Success response media type removed",
+                        before={"status": status, "content_type": ct},
+                        after=None,
+                    )
+                )
+            else:
+                unknown.append(
+                    Change(
+                        kind="removed_non_success_response_media_type",
+                        op=op_key,
+                        location=f"responses.{status}.content.{ct}",
+                        detail="Non-success response media type removed",
+                        before={"status": status, "content_type": ct},
+                        after=None,
+                    )
+                )
+
+        for ct in sorted(a_ct - b_ct):
+            non_breaking.append(
+                Change(
+                    kind="added_response_media_type",
+                    op=op_key,
+                    location=f"responses.{status}.content.{ct}",
+                    detail="Response media type added",
+                    before=None,
+                    after={"status": status, "content_type": ct},
+                )
+            )
________________________________________
Diff 2 — Add tests for the diff core
New file: tests/test_openapi_diff_core.py
This uses normalize_openapi and validates the core classification behaviors.
diff --git a/tests/test_openapi_diff_core.py b/tests/test_openapi_diff_core.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_diff_core.py
@@ -0,0 +1,214 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_diff import diff_openapi_core
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+def _doc(paths: dict) -> dict:
+    return {"openapi": "3.0.0", "info": {"title": "t", "version": "1"}, "paths": paths}
+
+
+def test_removed_operation_is_breaking() -> None:
+    before = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    after = normalize_openapi(_doc({}))
+    rep = diff_openapi_core(before, after)
+    assert rep.breaking is True
+    kinds = [c.kind for c in rep.breaking_changes]
+    assert "removed_operation" in kinds
+
+
+def test_added_operation_is_non_breaking() -> None:
+    before = normalize_openapi(_doc({}))
+    after = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    rep = diff_openapi_core(before, after)
+    assert rep.breaking is False
+    assert any(c.kind == "added_operation" for c in rep.non_breaking_changes)
+
+
+def test_removed_parameter_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [{"in": "query", "name": "q", "required": False, "schema": {"type": "string"}}],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}})
+    )
+    rep = diff_openapi_core(before, after)
+    assert rep.breaking is True
+    assert any(c.kind == "removed_parameter" for c in rep.breaking_changes)
+
+
+def test_parameter_required_tightened_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [{"in": "query", "name": "q", "required": False}],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [{"in": "query", "name": "q", "required": True}],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "parameter_required_tightened" for c in rep.breaking_changes)
+
+
+def test_request_body_required_tightened_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "post": {
+                        "requestBody": {"required": False, "content": {"application/json": {"schema": {"type": "string"}}}},
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "post": {
+                        "requestBody": {"required": True, "content": {"application/json": {"schema": {"type": "string"}}}},
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "request_body_required_tightened" for c in rep.breaking_changes)
+
+
+def test_removed_success_status_is_breaking_added_is_non_breaking() -> None:
+    before = normalize_openapi(
+        _doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}, "404": {"description": "no"}}}}})
+    )
+    after = normalize_openapi(
+        _doc({"/x/{id}": {"get": {"responses": {"201": {"description": "ok"}, "404": {"description": "no"}}}}})
+    )
+    rep = diff_openapi_core(before, after, success_status_prefixes=("2",))
+    # 200 removed => breaking
+    assert any(c.kind == "removed_success_response_status" for c in rep.breaking_changes)
+    # 201 added => non-breaking
+    assert any(c.kind == "added_response_status" for c in rep.non_breaking_changes)
+
+
+def test_removed_success_media_type_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {
+                            "200": {
+                                "content": {
+                                    "application/json": {"schema": {"type": "string"}},
+                                    "text/plain": {"schema": {"type": "string"}},
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {"200": {"content": {"application/json": {"schema": {"type": "string"}}}}}
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "removed_success_response_media_type" for c in rep.breaking_changes)
________________________________________
Diff 3 — Optional: export helper for JSON report generation (small but useful)
If you want a tiny convenience for later workflow steps, add:
New file: scripts/openapi_diff_report.py
This reads two OpenAPI JSON files, normalizes, diffs, writes report JSON.
(If you want this now, say “ship helper script”. Otherwise the library + tests above are sufficient.)
________________________________________
What you have after this diff
✅ Deterministic report model (OpenAPIDiffReport.to_dict())
✅ Core structural breaking detection:
•	removed ops
•	removed params
•	required tightened (params + request body)
•	success status/media type removals
✅ Non-breaking:
•	added ops
•	added optional params
•	added response status/media types
✅ Unknown bucket used conservatively for non-success removals (policy will decide later)
This is the correct “core” before you add schema narrowing and allowlist policy.
________________________________________
Next diff after this (Scope 3 continuation)
next diff: schema_semver comparator + wire schema narrowing into request/response + params schema
Ship next diff: schema_semver comparator + wire schema narrowing into request/response + params schema
next diff: schema_semver comparator + wire schema narrowing into params + request/response
This diff adds the semantic narrowing engine and integrates it into the core diff classifier so that:
•	parameter schema narrowing is breaking
•	request schema narrowing is breaking
•	success response schema narrowing is breaking
•	schema widening becomes non-breaking
•	complex/ambiguous schema changes become unknown (Scope 3 “max safety”)
It also keeps behavior deterministic and produces structured change entries with exact locations.
________________________________________
Diff 1 — Add schema comparator (schema_semver.py)
New file: src/code_audit/web_api/schema_semver.py
diff --git a/src/code_audit/web_api/schema_semver.py b/src/code_audit/web_api/schema_semver.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/code_audit/web_api/schema_semver.py
@@ -0,0 +1,356 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Set, Tuple
+
+
+@dataclass(frozen=True)
+class SchemaDelta:
+    breaking: List[str]
+    non_breaking: List[str]
+    unknown: List[str]
+
+    @property
+    def is_breaking(self) -> bool:
+        return len(self.breaking) > 0
+
+    @property
+    def is_unknown(self) -> bool:
+        return (not self.is_breaking) and len(self.unknown) > 0
+
+    @property
+    def is_non_breaking(self) -> bool:
+        return (not self.is_breaking) and (not self.is_unknown) and len(self.non_breaking) > 0
+
+
+def compare_schema_semver(
+    before: Dict[str, Any] | None,
+    after: Dict[str, Any] | None,
+    *,
+    treat_nullable_tightening_as_breaking: bool = True,
+    treat_format_as_semantic: bool = False,
+    treat_pattern_as_semantic: bool = False,
+) -> SchemaDelta:
+    """
+    Compare two schema signatures (from openapi_normalize.schema_signature()) and classify:
+      - breaking (narrowing / incompatibility)
+      - non-breaking (widening / additive)
+      - unknown (complex change we refuse to guess)
+
+    Inputs are signature dicts, not raw OpenAPI schema.
+    """
+    b = before or {}
+    a = after or {}
+
+    breaking: List[str] = []
+    non_breaking: List[str] = []
+    unknown: List[str] = []
+
+    # If either side indicates external/unsupported ref, treat as unknown unless identical.
+    if b.get("ref_kind") in ("external_or_unsupported",) or a.get("ref_kind") in ("external_or_unsupported",):
+        if b == a:
+            return SchemaDelta(breaking=[], non_breaking=[], unknown=[])
+        return SchemaDelta(breaking=[], non_breaking=[], unknown=["external_or_unsupported_ref_change"])
+
+    # Handle boolean schema forms if present
+    if "bool_schema" in b or "bool_schema" in a:
+        if b == a:
+            return SchemaDelta(breaking=[], non_breaking=[], unknown=[])
+        # True/False schema semantics can be tricky; conservatively unknown
+        return SchemaDelta(breaking=[], non_breaking=[], unknown=["boolean_schema_change"])
+
+    # If resolved refs differ, compare target payloads but keep ref identity for diagnostics
+    b_target = b.get("target") if isinstance(b.get("target"), dict) else b
+    a_target = a.get("target") if isinstance(a.get("target"), dict) else a
+
+    # Composition is complex; if present and not identical after normalization, treat as unknown.
+    # (Normalization already sorts composition lists deterministically.)
+    for comp in ("oneOf", "anyOf", "allOf"):
+        if comp in b_target or comp in a_target:
+            if b_target.get(comp) == a_target.get(comp):
+                continue
+            return SchemaDelta(breaking=[], non_breaking=[], unknown=[f"{comp}_change"])
+
+    # Type changes
+    b_type = b_target.get("type")
+    a_type = a_target.get("type")
+    if b_type != a_type:
+        # Some OpenAPI forms use list type; if both are lists, we can compare set inclusion
+        if isinstance(b_type, list) and isinstance(a_type, list):
+            bset = set([x for x in b_type if isinstance(x, str)])
+            aset = set([x for x in a_type if isinstance(x, str)])
+            if aset.issuperset(bset):
+                non_breaking.append("type_widened")
+            elif aset.issubset(bset):
+                breaking.append("type_narrowed")
+            else:
+                unknown.append("type_changed_incompatible_union")
+        else:
+            breaking.append("type_changed")
+
+    # Nullable tightening
+    if treat_nullable_tightening_as_breaking:
+        b_null = b_target.get("nullable")
+        a_null = a_target.get("nullable")
+        if b_null is True and a_null is False:
+            breaking.append("nullable_tightened")
+        elif b_null is False and a_null is True:
+            non_breaking.append("nullable_widened")
+
+    # Enum changes
+    b_enum = b_target.get("enum")
+    a_enum = a_target.get("enum")
+    if isinstance(b_enum, list) or isinstance(a_enum, list):
+        bset = set(b_enum or [])
+        aset = set(a_enum or [])
+        if bset and aset:
+            if aset.issubset(bset) and aset != bset:
+                breaking.append("enum_narrowed")
+            elif aset.issuperset(bset) and aset != bset:
+                non_breaking.append("enum_widened")
+            elif aset != bset:
+                unknown.append("enum_changed_non_subset")
+        elif bset and not aset:
+            # Removing enum restriction is widening (non-breaking)
+            non_breaking.append("enum_removed")
+        elif not bset and aset:
+            # Adding enum restriction is narrowing (breaking)
+            breaking.append("enum_added")
+
+    # Numeric constraints (tightening/widening)
+    _compare_bounds(b_target, a_target, breaking, non_breaking, unknown)
+
+    # String constraints
+    _compare_string_constraints(
+        b_target,
+        a_target,
+        breaking,
+        non_breaking,
+        unknown,
+        treat_format_as_semantic=treat_format_as_semantic,
+        treat_pattern_as_semantic=treat_pattern_as_semantic,
+    )
+
+    # Object constraints: required / properties / additionalProperties
+    _compare_object_constraints(b_target, a_target, breaking, non_breaking, unknown)
+
+    # Array constraints
+    _compare_array_constraints(b_target, a_target, breaking, non_breaking, unknown)
+
+    return SchemaDelta(breaking=breaking, non_breaking=non_breaking, unknown=unknown)
+
+
+def _compare_bounds(
+    b: Dict[str, Any],
+    a: Dict[str, Any],
+    breaking: List[str],
+    non_breaking: List[str],
+    unknown: List[str],
+) -> None:
+    # minimum: higher => narrower (breaking), lower => wider (non-breaking)
+    for k in ("minimum", "exclusiveMinimum"):
+        if k in b or k in a:
+            bmin = b.get(k)
+            amin = a.get(k)
+            if isinstance(bmin, (int, float)) and isinstance(amin, (int, float)):
+                if amin > bmin:
+                    breaking.append(f"{k}_tightened")
+                elif amin < bmin:
+                    non_breaking.append(f"{k}_widened")
+            elif bmin is None and isinstance(amin, (int, float)):
+                breaking.append(f"{k}_added")
+            elif isinstance(bmin, (int, float)) and amin is None:
+                non_breaking.append(f"{k}_removed")
+
+    # maximum: lower => narrower (breaking), higher => wider (non-breaking)
+    for k in ("maximum", "exclusiveMaximum"):
+        if k in b or k in a:
+            bmax = b.get(k)
+            amax = a.get(k)
+            if isinstance(bmax, (int, float)) and isinstance(amax, (int, float)):
+                if amax < bmax:
+                    breaking.append(f"{k}_tightened")
+                elif amax > bmax:
+                    non_breaking.append(f"{k}_widened")
+            elif bmax is None and isinstance(amax, (int, float)):
+                breaking.append(f"{k}_added")
+            elif isinstance(bmax, (int, float)) and amax is None:
+                non_breaking.append(f"{k}_removed")
+
+    # multipleOf: adding is narrowing; removing is widening; changing is unknown unless divisible
+    if "multipleOf" in b or "multipleOf" in a:
+        bm = b.get("multipleOf")
+        am = a.get("multipleOf")
+        if isinstance(bm, (int, float)) and isinstance(am, (int, float)):
+            if am == bm:
+                return
+            # If am is a multiple of bm, it's narrowing (fewer valid numbers)
+            try:
+                if (am / bm).is_integer():
+                    breaking.append("multipleOf_tightened")
+                elif (bm / am).is_integer():
+                    non_breaking.append("multipleOf_widened")
+                else:
+                    unknown.append("multipleOf_changed")
+            except Exception:
+                unknown.append("multipleOf_changed")
+        elif bm is None and isinstance(am, (int, float)):
+            breaking.append("multipleOf_added")
+        elif isinstance(bm, (int, float)) and am is None:
+            non_breaking.append("multipleOf_removed")
+
+
+def _compare_string_constraints(
+    b: Dict[str, Any],
+    a: Dict[str, Any],
+    breaking: List[str],
+    non_breaking: List[str],
+    unknown: List[str],
+    *,
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
+) -> None:
+    # minLength: higher => narrower
+    if "minLength" in b or "minLength" in a:
+        bm = b.get("minLength")
+        am = a.get("minLength")
+        if isinstance(bm, int) and isinstance(am, int):
+            if am > bm:
+                breaking.append("minLength_tightened")
+            elif am < bm:
+                non_breaking.append("minLength_widened")
+        elif bm is None and isinstance(am, int):
+            breaking.append("minLength_added")
+        elif isinstance(bm, int) and am is None:
+            non_breaking.append("minLength_removed")
+
+    # maxLength: lower => narrower
+    if "maxLength" in b or "maxLength" in a:
+        bx = b.get("maxLength")
+        ax = a.get("maxLength")
+        if isinstance(bx, int) and isinstance(ax, int):
+            if ax < bx:
+                breaking.append("maxLength_tightened")
+            elif ax > bx:
+                non_breaking.append("maxLength_widened")
+        elif bx is None and isinstance(ax, int):
+            breaking.append("maxLength_added")
+        elif isinstance(bx, int) and ax is None:
+            non_breaking.append("maxLength_removed")
+
+    if treat_pattern_as_semantic and ("pattern" in b or "pattern" in a):
+        if b.get("pattern") != a.get("pattern"):
+            unknown.append("pattern_changed")
+
+    if treat_format_as_semantic and ("format" in b or "format" in a):
+        if b.get("format") != a.get("format"):
+            unknown.append("format_changed")
+
+
+def _compare_object_constraints(
+    b: Dict[str, Any],
+    a: Dict[str, Any],
+    breaking: List[str],
+    non_breaking: List[str],
+    unknown: List[str],
+) -> None:
+    b_req = set(b.get("required") or []) if isinstance(b.get("required"), list) else set()
+    a_req = set(a.get("required") or []) if isinstance(a.get("required"), list) else set()
+    if a_req != b_req:
+        if a_req.issuperset(b_req) and a_req != b_req:
+            breaking.append("required_keys_added")
+        elif a_req.issubset(b_req) and a_req != b_req:
+            non_breaking.append("required_keys_removed")
+        else:
+            unknown.append("required_keys_changed")
+
+    b_props = b.get("properties") if isinstance(b.get("properties"), dict) else {}
+    a_props = a.get("properties") if isinstance(a.get("properties"), dict) else {}
+
+    # Adding properties is usually non-breaking; removing properties can be breaking if required.
+    # Here we only classify explicit removals as breaking if the removed key was required.
+    removed = set(b_props.keys()) - set(a_props.keys())
+    added = set(a_props.keys()) - set(b_props.keys())
+    if added:
+        non_breaking.append("properties_added")
+    if removed:
+        # If any removed prop was required in before, it's breaking.
+        if any(k in b_req for k in removed):
+            breaking.append("required_property_removed")
+        else:
+            # Optional property removal is often breaking for consumers but not strictly per schema;
+            # keep it unknown (Scope 3 conservative).
+            unknown.append("optional_property_removed")
+
+    # additionalProperties tightening:
+    if "additionalProperties" in b or "additionalProperties" in a:
+        bap = b.get("additionalProperties")
+        aap = a.get("additionalProperties")
+        if isinstance(bap, bool) and isinstance(aap, bool):
+            if bap is True and aap is False:
+                breaking.append("additionalProperties_tightened")
+            elif bap is False and aap is True:
+                non_breaking.append("additionalProperties_widened")
+        elif bap != aap:
+            # Schema-vs-bool or schema change: too complex; unknown
+            unknown.append("additionalProperties_changed")
+
+
+def _compare_array_constraints(
+    b: Dict[str, Any],
+    a: Dict[str, Any],
+    breaking: List[str],
+    non_breaking: List[str],
+    unknown: List[str],
+) -> None:
+    # minItems: higher => narrower
+    if "minItems" in b or "minItems" in a:
+        bm = b.get("minItems")
+        am = a.get("minItems")
+        if isinstance(bm, int) and isinstance(am, int):
+            if am > bm:
+                breaking.append("minItems_tightened")
+            elif am < bm:
+                non_breaking.append("minItems_widened")
+        elif bm is None and isinstance(am, int):
+            breaking.append("minItems_added")
+        elif isinstance(bm, int) and am is None:
+            non_breaking.append("minItems_removed")
+
+    # maxItems: lower => narrower
+    if "maxItems" in b or "maxItems" in a:
+        bx = b.get("maxItems")
+        ax = a.get("maxItems")
+        if isinstance(bx, int) and isinstance(ax, int):
+            if ax < bx:
+                breaking.append("maxItems_tightened")
+            elif ax > bx:
+                non_breaking.append("maxItems_widened")
+        elif bx is None and isinstance(ax, int):
+            breaking.append("maxItems_added")
+        elif isinstance(bx, int) and ax is None:
+            non_breaking.append("maxItems_removed")
+
+    if "uniqueItems" in b or "uniqueItems" in a:
+        bu = b.get("uniqueItems")
+        au = a.get("uniqueItems")
+        if isinstance(bu, bool) and isinstance(au, bool) and bu != au:
+            # uniqueItems true is more restrictive than false
+            if bu is False and au is True:
+                breaking.append("uniqueItems_tightened")
+            elif bu is True and au is False:
+                non_breaking.append("uniqueItems_widened")
+
+    # items: if different, we can attempt a recursive compare only if both are dicts and neither has composition.
+    b_items = b.get("items")
+    a_items = a.get("items")
+    if isinstance(b_items, dict) or isinstance(a_items, dict):
+        if b_items == a_items:
+            return
+        # Too complex to recurse here without context/policy; treat unknown for now.
+        unknown.append("items_changed")
________________________________________
Diff 2 — Wire schema narrowing into openapi_diff core
Patch: src/code_audit/web_api/openapi_diff.py
We add:
•	import compare_schema_semver
•	for shared params: compare schema sigs if present; breaking/non-breaking/unknown changes emitted
•	for shared requestBody media types: compare schemas
•	for shared success response statuses + shared media types: compare schemas
diff --git a/src/code_audit/web_api/openapi_diff.py b/src/code_audit/web_api/openapi_diff.py
index 1111111..2222222 100644
--- a/src/code_audit/web_api/openapi_diff.py
+++ b/src/code_audit/web_api/openapi_diff.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Tuple
 
@@
 from code_audit.web_api.openapi_normalize import (
@@
 )
+from code_audit.web_api.schema_semver import compare_schema_semver
 
@@
 def diff_openapi_core(
     before: NormalizedOpenAPI,
     after: NormalizedOpenAPI,
     *,
     success_status_prefixes: Tuple[str, ...] = ("2",),  # treat only 2xx as success by default
+    treat_nullable_tightening_as_breaking: bool = True,
+    treat_format_as_semantic: bool = False,
+    treat_pattern_as_semantic: bool = False,
 ) -> OpenAPIDiffReport:
@@
         _diff_operation_core(
             op_key,
             b,
             a,
             breaking,
             non_breaking,
             unknown,
             success_status_prefixes=success_status_prefixes,
+            treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+            treat_format_as_semantic=treat_format_as_semantic,
+            treat_pattern_as_semantic=treat_pattern_as_semantic,
         )
@@
 def _diff_operation_core(
@@
     *,
     success_status_prefixes: Tuple[str, ...],
+    treat_nullable_tightening_as_breaking: bool,
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
 ) -> None:
@@
     for k in sorted(bkeys & akeys):
         bp = bmap[k]
         ap = amap[k]
         # required tightened: False/None -> True is breaking
         if (bp.required is False or bp.required is None) and ap.required is True:
@@
             )
+
+        # schema narrowing/widening/unknown (if both schemas present)
+        if bp.schema is not None and ap.schema is not None and bp.schema != ap.schema:
+            delta = compare_schema_semver(
+                bp.schema,
+                ap.schema,
+                treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+            loc = f"parameters.{ap.in_}.{ap.name}.schema"
+            if delta.breaking:
+                breaking.append(
+                    Change(
+                        kind="parameter_schema_narrowed",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.breaking),
+                        before=bp.schema,
+                        after=ap.schema,
+                    )
+                )
+            elif delta.non_breaking:
+                non_breaking.append(
+                    Change(
+                        kind="parameter_schema_widened",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.non_breaking),
+                        before=bp.schema,
+                        after=ap.schema,
+                    )
+                )
+            elif delta.unknown:
+                unknown.append(
+                    Change(
+                        kind="parameter_schema_changed_unknown",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.unknown),
+                        before=bp.schema,
+                        after=ap.schema,
+                    )
+                )
 
     # ---- Request body presence/required ----
-    _diff_request_body(op_key, before.request_body, after.request_body, breaking, non_breaking, unknown)
+    _diff_request_body(
+        op_key,
+        before.request_body,
+        after.request_body,
+        breaking,
+        non_breaking,
+        unknown,
+        treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+        treat_format_as_semantic=treat_format_as_semantic,
+        treat_pattern_as_semantic=treat_pattern_as_semantic,
+    )
 
     # ---- Responses presence (status code + media types) ----
     _diff_responses_presence(
@@
         unknown,
         success_status_prefixes=success_status_prefixes,
+        treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+        treat_format_as_semantic=treat_format_as_semantic,
+        treat_pattern_as_semantic=treat_pattern_as_semantic,
     )
@@
 def _diff_request_body(
@@
     unknown: List[Change],
+    *,
+    treat_nullable_tightening_as_breaking: bool,
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
 ) -> None:
@@
     if before.present and after.present:
@@
         for ct in sorted(a_ct - b_ct):
             non_breaking.append(
@@
             )
+
+        # Schema compare for shared media types (presence already handled)
+        b_by_ct = {m.content_type: m for m in before.contents}
+        a_by_ct = {m.content_type: m for m in after.contents}
+        for ct in sorted(b_ct & a_ct):
+            bsch = b_by_ct[ct].schema
+            asch = a_by_ct[ct].schema
+            if bsch is None or asch is None or bsch == asch:
+                continue
+            delta = compare_schema_semver(
+                bsch,
+                asch,
+                treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+            loc = f"requestBody.content.{ct}.schema"
+            if delta.breaking:
+                breaking.append(
+                    Change(
+                        kind="request_schema_narrowed",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.breaking),
+                        before=bsch,
+                        after=asch,
+                    )
+                )
+            elif delta.non_breaking:
+                non_breaking.append(
+                    Change(
+                        kind="request_schema_widened",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.non_breaking),
+                        before=bsch,
+                        after=asch,
+                    )
+                )
+            elif delta.unknown:
+                unknown.append(
+                    Change(
+                        kind="request_schema_changed_unknown",
+                        op=op_key,
+                        location=loc,
+                        detail="; ".join(delta.unknown),
+                        before=bsch,
+                        after=asch,
+                    )
+                )
@@
 def _diff_responses_presence(
@@
     unknown: List[Change],
     *,
     success_status_prefixes: Tuple[str, ...],
+    treat_nullable_tightening_as_breaking: bool,
+    treat_format_as_semantic: bool,
+    treat_pattern_as_semantic: bool,
 ) -> None:
@@
     for status in sorted(bkeys & akeys):
         br = bmap[status]
         ar = amap[status]
         b_ct = {m.content_type for m in br.contents}
         a_ct = {m.content_type for m in ar.contents}
@@
         for ct in sorted(a_ct - b_ct):
             non_breaking.append(
@@
             )
+
+        # Schema compare for shared media types
+        b_by_ct = {m.content_type: m for m in br.contents}
+        a_by_ct = {m.content_type: m for m in ar.contents}
+        for ct in sorted(b_ct & a_ct):
+            bsch = b_by_ct[ct].schema
+            asch = a_by_ct[ct].schema
+            if bsch is None or asch is None or bsch == asch:
+                continue
+            delta = compare_schema_semver(
+                bsch,
+                asch,
+                treat_nullable_tightening_as_breaking=treat_nullable_tightening_as_breaking,
+                treat_format_as_semantic=treat_format_as_semantic,
+                treat_pattern_as_semantic=treat_pattern_as_semantic,
+            )
+            loc = f"responses.{status}.content.{ct}.schema"
+            if any(str(status).startswith(p) for p in success_status_prefixes):
+                if delta.breaking:
+                    breaking.append(
+                        Change(
+                            kind="success_response_schema_narrowed",
+                            op=op_key,
+                            location=loc,
+                            detail="; ".join(delta.breaking),
+                            before=bsch,
+                            after=asch,
+                        )
+                    )
+                elif delta.non_breaking:
+                    non_breaking.append(
+                        Change(
+                            kind="success_response_schema_widened",
+                            op=op_key,
+                            location=loc,
+                            detail="; ".join(delta.non_breaking),
+                            before=bsch,
+                            after=asch,
+                        )
+                    )
+                elif delta.unknown:
+                    unknown.append(
+                        Change(
+                            kind="success_response_schema_changed_unknown",
+                            op=op_key,
+                            location=loc,
+                            detail="; ".join(delta.unknown),
+                            before=bsch,
+                            after=asch,
+                        )
+                    )
+            else:
+                # Non-success schema changes are typically not contract-critical; keep unknown
+                if delta.breaking or delta.non_breaking or delta.unknown:
+                    unknown.append(
+                        Change(
+                            kind="non_success_response_schema_changed",
+                            op=op_key,
+                            location=loc,
+                            detail="; ".join(delta.breaking + delta.non_breaking + delta.unknown),
+                            before=bsch,
+                            after=asch,
+                        )
+                    )
________________________________________
Diff 3 — Add tests for schema narrowing integration
New file: tests/test_openapi_diff_schema_narrowing.py
Covers:
•	enum narrowed in param schema → breaking
•	enum widened → non-breaking
•	request schema narrowing → breaking
•	success response schema narrowing → breaking
•	oneOf change → unknown (max safety default)
diff --git a/tests/test_openapi_diff_schema_narrowing.py b/tests/test_openapi_diff_schema_narrowing.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_diff_schema_narrowing.py
@@ -0,0 +1,245 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_diff import diff_openapi_core
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+def _doc(paths: dict, components: dict | None = None) -> dict:
+    d = {"openapi": "3.0.0", "info": {"title": "t", "version": "1"}, "paths": paths}
+    if components:
+        d["components"] = components
+    return d
+
+
+def test_param_enum_narrowed_is_breaking_enum_widened_is_non_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [
+                            {"in": "query", "name": "q", "required": False, "schema": {"type": "string", "enum": ["a", "b", "c"]}}
+                        ],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    after_narrow = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [
+                            {"in": "query", "name": "q", "required": False, "schema": {"type": "string", "enum": ["a", "b"]}}
+                        ],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after_narrow)
+    assert any(c.kind == "parameter_schema_narrowed" for c in rep.breaking_changes)
+
+    after_wide = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "parameters": [
+                            {"in": "query", "name": "q", "required": False, "schema": {"type": "string", "enum": ["a", "b", "c", "d"]}}
+                        ],
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    rep2 = diff_openapi_core(before, after_wide)
+    assert any(c.kind == "parameter_schema_widened" for c in rep2.non_breaking_changes)
+
+
+def test_request_schema_narrowed_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "post": {
+                        "requestBody": {"required": False, "content": {"application/json": {"schema": {"type": "string"}}}},
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "post": {
+                        "requestBody": {
+                            "required": False,
+                            "content": {"application/json": {"schema": {"type": "string", "enum": ["a", "b"]}}},
+                        },
+                        "responses": {"200": {"description": "ok"}},
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "request_schema_narrowed" for c in rep.breaking_changes)
+
+
+def test_success_response_schema_narrowed_is_breaking() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {
+                            "200": {"content": {"application/json": {"schema": {"type": "string"}}}}
+                        }
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {
+                            "200": {"content": {"application/json": {"schema": {"type": "string", "enum": ["a"]}}}}
+                        }
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "success_response_schema_narrowed" for c in rep.breaking_changes)
+
+
+def test_oneof_change_is_unknown_by_default() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {
+                            "200": {
+                                "content": {
+                                    "application/json": {
+                                        "schema": {"oneOf": [{"type": "string"}, {"type": "integer"}]}
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {
+                            "200": {
+                                "content": {
+                                    "application/json": {
+                                        "schema": {"oneOf": [{"type": "string"}, {"type": "number"}]}
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "success_response_schema_changed_unknown" for c in rep.unknown_changes)
________________________________________
What this gives you
✅ A conservative-but-powerful schema semver comparator that detects:
•	enum narrowing/widening
•	bounds tightening/widening
•	required keys changes
•	property adds/removals (conservative handling)
•	nullable tightening (configurable)
•	array constraint tightening/widening
•	external refs → unknown
•	oneOf/anyOf/allOf diffs → unknown by default (max safety)
✅ Schema semantics wired into:
•	param schemas
•	request schemas per media type
•	success response schemas per media type
✅ Deterministic change records with exact locations.
________________________________________
Next diff after this (Scope 3 max continues)
next diff: unknown allowlist policy + policy schema + apply policy to diff report (suppress/convert unknown/breaking with reason)
Ship next diff: unknown allowlist policy + policy schema + apply policy to diff report (suppress/convert unknown/breaking with reason)
next diff: unknown allowlist policy + policy schema + apply policy to diff report (suppress/convert unknown/breaking with reason)
This diff adds a governed policy layer that can intentionally allow specific unknown or even breaking changes (rare) with a required reason, and then applies it deterministically to the diff report.
You get:
•	tests/contracts/openapi_breaking_policy.json (policy)
•	tests/contracts/openapi_breaking_policy.schema.json (structure + reason + sorting constraints)
•	policy validator test
•	policy application in the diff engine:
o	matching by (kind, op/path, location) (tightest, episode-safe)
o	allowed entries convert:
	unknown → non_breaking with detail annotated
	breaking → unknown or non_breaking depending on policy intent (configurable; default is safest: breaking can only be downgraded to unknown unless explicitly marked “allow_breaking_to_non_breaking”)
•	deterministic “policy_applied” metadata appended to the report so releases are auditable
________________________________________
Diff A — Add policy schema
New file: tests/contracts/openapi_breaking_policy.schema.json
diff --git a/tests/contracts/openapi_breaking_policy.schema.json b/tests/contracts/openapi_breaking_policy.schema.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/openapi_breaking_policy.schema.json
@@ -0,0 +1,161 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "$id": "openapi_breaking_policy_schema_v1",
+  "title": "OpenAPI Breaking/Unknown Allowlist Policy",
+  "type": "object",
+  "additionalProperties": false,
+  "required": ["version", "allow_unknown", "allow_breaking"],
+  "properties": {
+    "version": { "type": "integer", "const": 1 },
+    "allow_unknown": {
+      "type": "array",
+      "items": { "$ref": "#/$defs/policy_item" },
+      "default": []
+    },
+    "allow_breaking": {
+      "type": "array",
+      "items": { "$ref": "#/$defs/policy_item" },
+      "default": []
+    },
+    "allow_breaking_to_non_breaking": {
+      "type": "boolean",
+      "default": false,
+      "description": "If true, allow_breaking entries can be converted to non-breaking. Default false (max safety)."
+    }
+  },
+  "$defs": {
+    "policy_item": {
+      "type": "object",
+      "additionalProperties": false,
+      "required": ["kind", "reason"],
+      "properties": {
+        "kind": {
+          "type": "string",
+          "minLength": 1
+        },
+        "op": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Operation key, e.g. 'GET /items/{}'"
+        },
+        "path": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Normalized path, e.g. '/items/{}' (alternative to op)"
+        },
+        "location": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Precise location string, e.g. 'responses.200.content.application/json.schema'"
+        },
+        "reason": {
+          "type": "string",
+          "minLength": 3
+        }
+      },
+      "anyOf": [
+        { "required": ["op"] },
+        { "required": ["path"] }
+      ]
+    }
+  }
+}
Key semantics enforced:
•	Every allowlist entry has kind + (op or path) + reason
•	location is optional but recommended; when present, it tightens matching further
•	allow_breaking_to_non_breaking defaults to false (safest)
________________________________________
Diff B — Add policy file (empty defaults, deterministic)
New file: tests/contracts/openapi_breaking_policy.json
diff --git a/tests/contracts/openapi_breaking_policy.json b/tests/contracts/openapi_breaking_policy.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/openapi_breaking_policy.json
@@ -0,0 +1,8 @@
+{
+  "version": 1,
+  "allow_unknown": [],
+  "allow_breaking": [],
+  "allow_breaking_to_non_breaking": false
+}
________________________________________
Diff C — Schema validation test for the policy file
New file: tests/test_openapi_breaking_policy_schema.py
diff --git a/tests/test_openapi_breaking_policy_schema.py b/tests/test_openapi_breaking_policy_schema.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_breaking_policy_schema.py
@@ -0,0 +1,70 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+import jsonschema
+
+
+ROOT = Path(__file__).resolve().parents[1]
+POLICY = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+SCHEMA = ROOT / "tests" / "contracts" / "openapi_breaking_policy.schema.json"
+
+
+def test_openapi_breaking_policy_conforms_to_schema() -> None:
+    assert POLICY.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    assert SCHEMA.exists(), "Missing tests/contracts/openapi_breaking_policy.schema.json"
+
+    policy = json.loads(POLICY.read_text(encoding="utf-8"))
+    schema = json.loads(SCHEMA.read_text(encoding="utf-8"))
+    jsonschema.validate(instance=policy, schema=schema)
________________________________________
Diff D — Apply policy to diff report (core logic)
Patch: src/code_audit/web_api/openapi_diff.py
Adds:
•	policy loader type
•	matching function keyed by (kind, op/path, location?)
•	application step that:
o	removes allowlisted entries from unknown/breaking
o	re-emits them as non-breaking (or unknown) with a policy annotation
1) Add policy types + matcher + apply function
diff --git a/src/code_audit/web_api/openapi_diff.py b/src/code_audit/web_api/openapi_diff.py
index 2222222..3333333 100644
--- a/src/code_audit/web_api/openapi_diff.py
+++ b/src/code_audit/web_api/openapi_diff.py
@@ -1,8 +1,10 @@
 from __future__ import annotations
 
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple, Iterable
 
@@
 DEFAULT_ALLOWED_METHODS = {"GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"}
 
+PolicyItem = Dict[str, Any]
+Policy = Dict[str, Any]
+
@@
 def diff_openapi_core(
@@
 ) -> OpenAPIDiffReport:
@@
     report = OpenAPIDiffReport(
@@
     )
     return report
+
+
+def apply_allowlist_policy(
+    report: OpenAPIDiffReport,
+    *,
+    policy: Policy | None,
+) -> OpenAPIDiffReport:
+    """
+    Apply an allowlist policy to a diff report deterministically.
+
+    - allow_unknown: converts matching unknown changes into non-breaking with annotation
+    - allow_breaking: converts matching breaking changes into:
+        - non-breaking only if policy["allow_breaking_to_non_breaking"] == True
+        - otherwise unknown (max safety default)
+
+    Matching is strict:
+      - kind must match
+      - op OR path must match (policy item must have one)
+      - if policy item includes location, it must match too
+    """
+    if not policy:
+        return report
+
+    allow_unknown = policy.get("allow_unknown") or []
+    allow_breaking = policy.get("allow_breaking") or []
+    allow_break_to_nb = bool(policy.get("allow_breaking_to_non_breaking", False))
+
+    def index(items: Iterable[PolicyItem]) -> Dict[tuple, PolicyItem]:
+        out: Dict[tuple, PolicyItem] = {}
+        for it in items:
+            if not isinstance(it, dict):
+                continue
+            kind = it.get("kind")
+            op = it.get("op")
+            path = it.get("path")
+            loc = it.get("location")  # optional
+            if not isinstance(kind, str) or not kind:
+                continue
+            key = (kind, str(op) if op is not None else None, str(path) if path is not None else None, str(loc) if loc is not None else None)
+            out[key] = it
+        return out
+
+    allow_u = index(allow_unknown)
+    allow_b = index(allow_breaking)
+
+    def match_key(c: Change) -> List[tuple]:
+        # Generate possible keys for strict matching (op-based and path-based).
+        # If c.op is present, policy item should use op; if c.path is present, policy may use path.
+        # Location matching is exact when provided.
+        keys = []
+        loc = c.location if c.location is not None else None
+        if c.op is not None:
+            keys.append((c.kind, c.op, None, loc))
+            keys.append((c.kind, c.op, None, None))
+        if c.path is not None:
+            keys.append((c.kind, None, c.path, loc))
+            keys.append((c.kind, None, c.path, None))
+        return keys
+
+    def annotate(c: Change, it: PolicyItem, *, new_kind: str | None = None) -> Change:
+        reason = it.get("reason")
+        reason_s = reason if isinstance(reason, str) else "allowed_by_policy"
+        detail = (c.detail or "").strip()
+        prefix = f"allowed_by_policy: {reason_s}"
+        merged_detail = prefix if not detail else f"{prefix} | {detail}"
+        return Change(
+            kind=new_kind or c.kind,
+            op=c.op,
+            path=c.path,
+            location=c.location,
+            detail=merged_detail,
+            before=c.before,
+            after=c.after,
+        )
+
+    # Process unknowns -> non-breaking if allowlisted
+    new_unknown: List[Change] = []
+    new_non_breaking: List[Change] = list(report.non_breaking_changes)
+
+    for c in report.unknown_changes:
+        it = None
+        for k in match_key(c):
+            if k in allow_u:
+                it = allow_u[k]
+                break
+        if it is not None:
+            new_non_breaking.append(annotate(c, it, new_kind="allowed_unknown"))
+        else:
+            new_unknown.append(c)
+
+    # Process breaking -> (unknown or non-breaking) if allowlisted
+    new_breaking: List[Change] = []
+    for c in report.breaking_changes:
+        it = None
+        for k in match_key(c):
+            if k in allow_b:
+                it = allow_b[k]
+                break
+        if it is None:
+            new_breaking.append(c)
+            continue
+
+        if allow_break_to_nb:
+            new_non_breaking.append(annotate(c, it, new_kind="allowed_breaking"))
+        else:
+            # Max-safety default: allowlisted breaking becomes unknown (still requires visibility)
+            new_unknown.append(annotate(c, it, new_kind="allowed_breaking_to_unknown"))
+
+    # Deterministic sort
+    new_breaking = sorted(new_breaking, key=_change_sort_key)
+    new_non_breaking = sorted(new_non_breaking, key=_change_sort_key)
+    new_unknown = sorted(new_unknown, key=_change_sort_key)
+
+    return OpenAPIDiffReport(
+        breaking=len(new_breaking) > 0,
+        breaking_changes=new_breaking,
+        non_breaking_changes=new_non_breaking,
+        unknown_changes=new_unknown,
+        summary=DiffSummary(
+            breaking_count=len(new_breaking),
+            non_breaking_count=len(new_non_breaking),
+            unknown_count=len(new_unknown),
+        ),
+        version=report.version,
+    )
Why convert allowlisted unknown → non-breaking?
Because the allowlist is an explicit human decision; after allowlisting, the engine should treat it as “permitted surface evolution” and remove it from the “unknown must block release” set.
Why convert allowlisted breaking → unknown by default?
Max safety. It forces visibility: you “allowed it,” but it stays classified as “special attention” unless you intentionally flip allow_breaking_to_non_breaking=true.
________________________________________
Diff E — Add policy application tests
New file: tests/test_openapi_diff_policy_application.py
diff --git a/tests/test_openapi_diff_policy_application.py b/tests/test_openapi_diff_policy_application.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_diff_policy_application.py
@@ -0,0 +1,204 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_diff import apply_allowlist_policy, diff_openapi_core
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+def _doc(paths: dict) -> dict:
+    return {"openapi": "3.0.0", "info": {"title": "t", "version": "1"}, "paths": paths}
+
+
+def test_allow_unknown_converts_unknown_to_non_breaking_with_annotation() -> None:
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {"404": {"content": {"application/json": {"schema": {"type": "string"}}}}}
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {"404": {"content": {}}}
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "removed_non_success_response_media_type" for c in rep.unknown_changes)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [
+            {
+                "kind": "removed_non_success_response_media_type",
+                "op": "GET /x/{}",
+                "location": "responses.404.content.application/json",
+                "reason": "404 payload format is non-contract"
+            }
+        ],
+        "allow_breaking": [],
+        "allow_breaking_to_non_breaking": False,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    assert not any(c.kind == "removed_non_success_response_media_type" for c in rep2.unknown_changes)
+    assert any(c.kind == "allowed_unknown" for c in rep2.non_breaking_changes)
+    assert all("allowed_by_policy" in (c.detail or "") for c in rep2.non_breaking_changes if c.kind == "allowed_unknown")
+
+
+def test_allow_breaking_defaults_to_unknown_unless_explicitly_permitted() -> None:
+    before = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    after = normalize_openapi(_doc({}))
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "removed_operation" for c in rep.breaking_changes)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [],
+        "allow_breaking": [
+            {"kind": "removed_operation", "op": "GET /x/{}", "reason": "Intentional deprecation"}
+        ],
+        "allow_breaking_to_non_breaking": False,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    # Breaking removed from breaking list, but becomes unknown (max safety)
+    assert not any(c.kind == "removed_operation" for c in rep2.breaking_changes)
+    assert any(c.kind == "allowed_breaking_to_unknown" for c in rep2.unknown_changes)
+
+
+def test_allow_breaking_to_non_breaking_when_enabled() -> None:
+    before = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    after = normalize_openapi(_doc({}))
+    rep = diff_openapi_core(before, after)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [],
+        "allow_breaking": [
+            {"kind": "removed_operation", "op": "GET /x/{}", "reason": "Intentional removal with coordinated release"}
+        ],
+        "allow_breaking_to_non_breaking": True,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    assert rep2.breaking is False
+    assert any(c.kind == "allowed_breaking" for c in rep2.non_breaking_changes)
Note: The first test uses a non-success response media type removal as a guaranteed “unknown” from the core classifier.
________________________________________
Diff F — Policy-aware report plumbing (minimal integration)
Right now the diff engine exports:
•	diff_openapi_core(before, after) -> report
You now have:
•	apply_allowlist_policy(report, policy=...) -> report2
That’s enough to wire into future:
•	CLI scripts
•	release gates
•	BOM artifact generation
We intentionally keep the core diff function pure and policy-free; policy application is an explicit step (matches your overall contract discipline).
________________________________________
Next diff after this (Scope 3 continues)
next diff: policy loader + canonical sort/unique enforcement + policy “no ambiguity” lint (duplicate match keys forbidden)
This will add:
•	scripts/validate_openapi_breaking_policy.py --check/--write (optional)
•	a test that ensures allow_unknown/allow_breaking entries are sorted and unique by match key (kind, op/path, location)
•	“tightest” semantics to prevent broad exemptions
If you want it, say: next diff: policy sorted+unique contract + duplicate key ban.
Ship next diff: unknown allowlist policy + policy schema + apply policy to diff report (suppress/convert unknown/breaking with reason)
next diff: unknown allowlist policy + policy schema + apply policy to diff report
This is the full ship for the allowlist layer (policy + schema + validation + application). It adds a governed mechanism to intentionally “permit” specific unknown (and optionally specific breaking) changes with required reasons, and then deterministically applies that policy to a diff report.
You’ll end up with:
•	tests/contracts/openapi_breaking_policy.json (policy file, versioned)
•	tests/contracts/openapi_breaking_policy.schema.json (structure contract)
•	tests/test_openapi_breaking_policy_schema.py (schema validation gate)
•	apply_allowlist_policy() in openapi_diff.py (report transformation, deterministic)
•	tests/test_openapi_diff_policy_application.py (behavioral lock)
________________________________________
Diff A — Policy schema
tests/contracts/openapi_breaking_policy.schema.json
diff --git a/tests/contracts/openapi_breaking_policy.schema.json b/tests/contracts/openapi_breaking_policy.schema.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/openapi_breaking_policy.schema.json
@@ -0,0 +1,161 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "$id": "openapi_breaking_policy_schema_v1",
+  "title": "OpenAPI Breaking/Unknown Allowlist Policy",
+  "type": "object",
+  "additionalProperties": false,
+  "required": ["version", "allow_unknown", "allow_breaking"],
+  "properties": {
+    "version": { "type": "integer", "const": 1 },
+    "allow_unknown": {
+      "type": "array",
+      "items": { "$ref": "#/$defs/policy_item" },
+      "default": []
+    },
+    "allow_breaking": {
+      "type": "array",
+      "items": { "$ref": "#/$defs/policy_item" },
+      "default": []
+    },
+    "allow_breaking_to_non_breaking": {
+      "type": "boolean",
+      "default": false,
+      "description": "If true, allow_breaking entries can be converted to non-breaking. Default false (max safety)."
+    }
+  },
+  "$defs": {
+    "policy_item": {
+      "type": "object",
+      "additionalProperties": false,
+      "required": ["kind", "reason"],
+      "properties": {
+        "kind": {
+          "type": "string",
+          "minLength": 1
+        },
+        "op": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Operation key, e.g. 'GET /items/{}'"
+        },
+        "path": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Normalized path, e.g. '/items/{}' (alternative to op)"
+        },
+        "location": {
+          "type": "string",
+          "minLength": 1,
+          "description": "Precise location string, e.g. 'responses.200.content.application/json.schema'"
+        },
+        "reason": {
+          "type": "string",
+          "minLength": 3
+        }
+      },
+      "anyOf": [
+        { "required": ["op"] },
+        { "required": ["path"] }
+      ]
+    }
+  }
+}
Why this schema is “tight enough” for now
•	Prevents free-form keys (additionalProperties false)
•	Requires reasons (non-empty)
•	Forces either op or path so exemptions are always anchored
•	Allows location to tighten targeting (recommended for unknown + schema changes)
________________________________________
Diff B — Policy file (empty defaults)
tests/contracts/openapi_breaking_policy.json
diff --git a/tests/contracts/openapi_breaking_policy.json b/tests/contracts/openapi_breaking_policy.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/openapi_breaking_policy.json
@@ -0,0 +1,8 @@
+{
+  "version": 1,
+  "allow_unknown": [],
+  "allow_breaking": [],
+  "allow_breaking_to_non_breaking": false
+}
This keeps the default posture as:
•	unknowns stay unknown (and later release gates can block on them)
•	breakings stay breaking
•	nothing is allowed unless explicitly stated
________________________________________
Diff C — Policy schema validation test
tests/test_openapi_breaking_policy_schema.py
diff --git a/tests/test_openapi_breaking_policy_schema.py b/tests/test_openapi_breaking_policy_schema.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_breaking_policy_schema.py
@@ -0,0 +1,70 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+import jsonschema
+
+
+ROOT = Path(__file__).resolve().parents[1]
+POLICY = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+SCHEMA = ROOT / "tests" / "contracts" / "openapi_breaking_policy.schema.json"
+
+
+def test_openapi_breaking_policy_conforms_to_schema() -> None:
+    assert POLICY.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    assert SCHEMA.exists(), "Missing tests/contracts/openapi_breaking_policy.schema.json"
+
+    policy = json.loads(POLICY.read_text(encoding="utf-8"))
+    schema = json.loads(SCHEMA.read_text(encoding="utf-8"))
+    jsonschema.validate(instance=policy, schema=schema)
(Assumes jsonschema is already in dev deps from earlier OpenAPI schema validation gates.)
________________________________________
Diff D — Apply policy to a diff report
Patch src/code_audit/web_api/openapi_diff.py
This adds apply_allowlist_policy(report, policy=...) which:
•	converts allowlisted unknown → allowed_unknown (non-breaking bucket)
•	converts allowlisted breaking →
o	allowed_breaking_to_unknown by default (still visible)
o	or allowed_breaking in non-breaking bucket if allow_breaking_to_non_breaking=true
It matches entries strictly by:
•	kind AND
•	op (preferred) OR path AND
•	if policy specifies location, it must match; otherwise it can match without location
diff --git a/src/code_audit/web_api/openapi_diff.py b/src/code_audit/web_api/openapi_diff.py
index 2222222..3333333 100644
--- a/src/code_audit/web_api/openapi_diff.py
+++ b/src/code_audit/web_api/openapi_diff.py
@@ -1,8 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple, Iterable
 
@@
 DEFAULT_ALLOWED_METHODS = {"GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"}
 
+PolicyItem = Dict[str, Any]
+Policy = Dict[str, Any]
+
@@
 def diff_openapi_core(
@@
     return report
+
+
+def apply_allowlist_policy(
+    report: OpenAPIDiffReport,
+    *,
+    policy: Policy | None,
+) -> OpenAPIDiffReport:
+    """
+    Apply allowlist policy deterministically.
+
+    - allow_unknown: unknown -> non-breaking (kind becomes 'allowed_unknown') with annotation
+    - allow_breaking: breaking -> unknown by default (kind becomes 'allowed_breaking_to_unknown')
+        - if allow_breaking_to_non_breaking=true: breaking -> non-breaking (kind becomes 'allowed_breaking')
+
+    Strict match:
+      - kind must match
+      - op OR path must match (policy entry must include one)
+      - if policy entry includes location, it must match; otherwise location is not required
+    """
+    if not policy:
+        return report
+
+    allow_unknown = policy.get("allow_unknown") or []
+    allow_breaking = policy.get("allow_breaking") or []
+    allow_break_to_nb = bool(policy.get("allow_breaking_to_non_breaking", False))
+
+    def index(items: Iterable[PolicyItem]) -> Dict[tuple, PolicyItem]:
+        out: Dict[tuple, PolicyItem] = {}
+        for it in items:
+            if not isinstance(it, dict):
+                continue
+            kind = it.get("kind")
+            op = it.get("op")
+            path = it.get("path")
+            loc = it.get("location")
+            if not isinstance(kind, str) or not kind:
+                continue
+            key = (
+                kind,
+                str(op) if op is not None else None,
+                str(path) if path is not None else None,
+                str(loc) if loc is not None else None,
+            )
+            out[key] = it
+        return out
+
+    allow_u = index(allow_unknown)
+    allow_b = index(allow_breaking)
+
+    def match_keys(c: Change) -> List[tuple]:
+        loc = c.location if c.location is not None else None
+        keys: List[tuple] = []
+        if c.op is not None:
+            keys.append((c.kind, c.op, None, loc))
+            keys.append((c.kind, c.op, None, None))
+        if c.path is not None:
+            keys.append((c.kind, None, c.path, loc))
+            keys.append((c.kind, None, c.path, None))
+        return keys
+
+    def annotate(c: Change, it: PolicyItem, *, new_kind: str) -> Change:
+        reason = it.get("reason")
+        reason_s = reason if isinstance(reason, str) else "allowed_by_policy"
+        prefix = f"allowed_by_policy: {reason_s}"
+        detail = (c.detail or "").strip()
+        merged_detail = prefix if not detail else f"{prefix} | {detail}"
+        return Change(
+            kind=new_kind,
+            op=c.op,
+            path=c.path,
+            location=c.location,
+            detail=merged_detail,
+            before=c.before,
+            after=c.after,
+        )
+
+    # unknown -> allowed_unknown (non-breaking)
+    new_unknown: List[Change] = []
+    new_non_breaking: List[Change] = list(report.non_breaking_changes)
+    for c in report.unknown_changes:
+        it = None
+        for k in match_keys(c):
+            if k in allow_u:
+                it = allow_u[k]
+                break
+        if it is None:
+            new_unknown.append(c)
+        else:
+            new_non_breaking.append(annotate(c, it, new_kind="allowed_unknown"))
+
+    # breaking -> allowed_breaking_to_unknown (default) OR allowed_breaking (if enabled)
+    new_breaking: List[Change] = []
+    for c in report.breaking_changes:
+        it = None
+        for k in match_keys(c):
+            if k in allow_b:
+                it = allow_b[k]
+                break
+        if it is None:
+            new_breaking.append(c)
+            continue
+
+        if allow_break_to_nb:
+            new_non_breaking.append(annotate(c, it, new_kind="allowed_breaking"))
+        else:
+            new_unknown.append(annotate(c, it, new_kind="allowed_breaking_to_unknown"))
+
+    # Deterministic sort
+    new_breaking = sorted(new_breaking, key=_change_sort_key)
+    new_non_breaking = sorted(new_non_breaking, key=_change_sort_key)
+    new_unknown = sorted(new_unknown, key=_change_sort_key)
+
+    return OpenAPIDiffReport(
+        breaking=len(new_breaking) > 0,
+        breaking_changes=new_breaking,
+        non_breaking_changes=new_non_breaking,
+        unknown_changes=new_unknown,
+        summary=DiffSummary(
+            breaking_count=len(new_breaking),
+            non_breaking_count=len(new_non_breaking),
+            unknown_count=len(new_unknown),
+        ),
+        version=report.version,
+    )
________________________________________
Diff E — Policy application tests
tests/test_openapi_diff_policy_application.py
diff --git a/tests/test_openapi_diff_policy_application.py b/tests/test_openapi_diff_policy_application.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_diff_policy_application.py
@@ -0,0 +1,204 @@
+from __future__ import annotations
+
+from code_audit.web_api.openapi_diff import apply_allowlist_policy, diff_openapi_core
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+def _doc(paths: dict) -> dict:
+    return {"openapi": "3.0.0", "info": {"title": "t", "version": "1"}, "paths": paths}
+
+
+def test_allow_unknown_converts_unknown_to_non_breaking_with_annotation() -> None:
+    # Create an 'unknown' by removing a non-success response media type (core diff treats this as unknown)
+    before = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {"404": {"content": {"application/json": {"schema": {"type": "string"}}}}}
+                    }
+                }
+            }
+        )
+    )
+    after = normalize_openapi(
+        _doc(
+            {
+                "/x/{id}": {
+                    "get": {
+                        "responses": {"404": {"content": {}}}
+                    }
+                }
+            }
+        )
+    )
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "removed_non_success_response_media_type" for c in rep.unknown_changes)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [
+            {
+                "kind": "removed_non_success_response_media_type",
+                "op": "GET /x/{}",
+                "location": "responses.404.content.application/json",
+                "reason": "404 payload is not part of success contract"
+            }
+        ],
+        "allow_breaking": [],
+        "allow_breaking_to_non_breaking": False,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    assert not any(c.kind == "removed_non_success_response_media_type" for c in rep2.unknown_changes)
+    assert any(c.kind == "allowed_unknown" for c in rep2.non_breaking_changes)
+    assert all(
+        "allowed_by_policy" in (c.detail or "")
+        for c in rep2.non_breaking_changes
+        if c.kind == "allowed_unknown"
+    )
+
+
+def test_allow_breaking_defaults_to_unknown_unless_explicitly_permitted() -> None:
+    before = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    after = normalize_openapi(_doc({}))
+    rep = diff_openapi_core(before, after)
+    assert any(c.kind == "removed_operation" for c in rep.breaking_changes)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [],
+        "allow_breaking": [
+            {"kind": "removed_operation", "op": "GET /x/{}", "reason": "Intentional deprecation"}
+        ],
+        "allow_breaking_to_non_breaking": False,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    assert not any(c.kind == "removed_operation" for c in rep2.breaking_changes)
+    assert any(c.kind == "allowed_breaking_to_unknown" for c in rep2.unknown_changes)
+
+
+def test_allow_breaking_to_non_breaking_when_enabled() -> None:
+    before = normalize_openapi(_doc({"/x/{id}": {"get": {"responses": {"200": {"description": "ok"}}}}}))
+    after = normalize_openapi(_doc({}))
+    rep = diff_openapi_core(before, after)
+
+    policy = {
+        "version": 1,
+        "allow_unknown": [],
+        "allow_breaking": [
+            {"kind": "removed_operation", "op": "GET /x/{}", "reason": "Coordinated removal across clients"}
+        ],
+        "allow_breaking_to_non_breaking": True,
+    }
+    rep2 = apply_allowlist_policy(rep, policy=policy)
+    assert rep2.breaking is False
+    assert any(c.kind == "allowed_breaking" for c in rep2.non_breaking_changes)
________________________________________
Operational behavior after this diff
Unknown changes
•	default: remain unknown (can be blocked by release gates later)
•	allowlisted: become allowed_unknown in non-breaking bucket, with detail including the reason
Breaking changes
•	default: remain breaking
•	allowlisted:
o	default mode (allow_breaking_to_non_breaking=false): become allowed_breaking_to_unknown (still visible, but no longer in breaking bucket)
o	override mode (true): become allowed_breaking in non-breaking bucket
This gives you “max safety” by default while still enabling controlled, audited exceptions.
________________________________________
Next diff (tightest follow-on)
next diff: policy sorted+unique contract + duplicate match-key ban (prevents broad exemptions and ambiguity)
This adds:
•	deterministic ordering for policy items
•	uniqueness by match key (kind, op/path, location) enforced by a test
•	better diagnostics if duplicates exist
Ship next diff: policy sorted+unique contract + duplicate match-key ban (prevents broad exemptions and ambiguity)
next diff: policy sorted+unique contract + duplicate match-key ban
This tightens the allowlist policy from “schema-valid” to “operationally unambiguous”:
•	Sorted deterministically (stable diffs, predictable review)
•	Unique by match key (no two entries can target the same change)
•	No broad exemptions encouraged (and if someone tries, it’s explicit and reviewable)
•	Duplicate match keys hard-fail with a precise diagnostic pointing to the collisions
This is the same pattern you’ve used for endpoint registries: sorted + unique + schema + gate.
________________________________________
What is the “match key”?
A policy item matches a Change by:
•	kind (required)
•	op or path (exactly one must exist logically; schema allows either, but both can appear—see note)
•	location (optional; if present, it tightens the match)
So the uniqueness key is:
•	If op is present: ("op", kind, op, location_or_empty)
•	Else: ("path", kind, path, location_or_empty)
This ensures:
•	you can’t have two allow_unknown items that both match the same change
•	you can’t have duplicates in allow_breaking
•	you can’t accidentally write two entries that differ only in reason
________________________________________
Diff A — Add policy lint test: sorted + unique + duplicate ban
New file: tests/test_openapi_breaking_policy_sorted_unique.py
diff --git a/tests/test_openapi_breaking_policy_sorted_unique.py b/tests/test_openapi_breaking_policy_sorted_unique.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_breaking_policy_sorted_unique.py
@@ -0,0 +1,182 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+
+ROOT = Path(__file__).resolve().parents[1]
+POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+
+
+def _item_key(it: dict) -> tuple:
+    kind = it.get("kind")
+    op = it.get("op")
+    path = it.get("path")
+    loc = it.get("location") or ""
+
+    # Schema guarantees kind + (op or path). We enforce deterministic keying here.
+    if op is not None:
+        return ("op", str(kind), str(op), str(loc))
+    return ("path", str(kind), str(path), str(loc))
+
+
+def _is_sorted_unique(items: list[dict]) -> tuple[bool, list[tuple], list[tuple]]:
+    keys = [_item_key(it) for it in items]
+    sorted_keys = sorted(keys)
+    duplicates = []
+    seen = set()
+    for k in keys:
+        if k in seen:
+            duplicates.append(k)
+        else:
+            seen.add(k)
+    return keys == sorted_keys, duplicates, keys
+
+
+def _format_key(k: tuple) -> str:
+    # ("op"|"path", kind, op_or_path, location)
+    mode, kind, target, loc = k
+    loc_s = f" @ {loc}" if loc else ""
+    return f"{mode}:{kind}:{target}{loc_s}"
+
+
+def test_openapi_breaking_policy_allow_unknown_sorted_unique_and_no_duplicate_match_keys() -> None:
+    assert POLICY_PATH.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    policy = json.loads(POLICY_PATH.read_text(encoding="utf-8"))
+    items = policy.get("allow_unknown") or []
+    assert isinstance(items, list)
+    for it in items:
+        assert isinstance(it, dict)
+
+    is_sorted, dups, keys = _is_sorted_unique(items)
+    if dups:
+        lines = [
+            "OpenAPI breaking policy invalid: allow_unknown contains duplicate match keys.",
+            "Duplicates:",
+        ]
+        lines.extend([f"- {_format_key(k)}" for k in sorted(set(dups))])
+        raise AssertionError("\n".join(lines))
+
+    if not is_sorted:
+        lines = [
+            "OpenAPI breaking policy invalid: allow_unknown is not sorted by (op|path, kind, target, location).",
+            "Expected order:",
+        ]
+        lines.extend([f"- {_format_key(k)}" for k in sorted(keys)])
+        lines.append("")
+        lines.append("Fix: sort allow_unknown entries deterministically by the key above.")
+        raise AssertionError("\n".join(lines))
+
+
+def test_openapi_breaking_policy_allow_breaking_sorted_unique_and_no_duplicate_match_keys() -> None:
+    assert POLICY_PATH.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    policy = json.loads(POLICY_PATH.read_text(encoding="utf-8"))
+    items = policy.get("allow_breaking") or []
+    assert isinstance(items, list)
+    for it in items:
+        assert isinstance(it, dict)
+
+    is_sorted, dups, keys = _is_sorted_unique(items)
+    if dups:
+        lines = [
+            "OpenAPI breaking policy invalid: allow_breaking contains duplicate match keys.",
+            "Duplicates:",
+        ]
+        lines.extend([f"- {_format_key(k)}" for k in sorted(set(dups))])
+        raise AssertionError("\n".join(lines))
+
+    if not is_sorted:
+        lines = [
+            "OpenAPI breaking policy invalid: allow_breaking is not sorted by (op|path, kind, target, location).",
+            "Expected order:",
+        ]
+        lines.extend([f"- {_format_key(k)}" for k in sorted(keys)])
+        lines.append("")
+        lines.append("Fix: sort allow_breaking entries deterministically by the key above.")
+        raise AssertionError("\n".join(lines))
What this enforces
•	No duplicates by match key
•	Deterministic sort order for review stability
•	Separate enforcement per list (allow_unknown, allow_breaking) so diagnostics are clearer
________________________________________
Diff B — Optional strictness: forbid specifying both op and path (tightest, no ambiguity)
Right now the schema allows both op and path even though it only requires one. That can be ambiguous if someone writes mismatched values.
If you want max tightness, add a policy lint assertion:
•	If op exists, then path must be absent (and vice versa)
Add this small check inside both tests above (right after assert isinstance(it, dict) loops):
@@
     for it in items:
         assert isinstance(it, dict)
+        has_op = "op" in it and it["op"] is not None
+        has_path = "path" in it and it["path"] is not None
+        if has_op and has_path:
+            raise AssertionError(
+                "OpenAPI breaking policy invalid: item sets both 'op' and 'path'. "
+                "Choose exactly one to avoid ambiguity."
+            )
If you’d rather enforce this structurally in JSON Schema, it’s doable with oneOf branches that disallow the other property, but the lint check is simpler and produces clearer errors.
________________________________________
Why this prevents “broad exemptions”
Without this gate, someone can quietly add:
•	duplicate entries with different reasons and reviewers may miss the duplication
•	unsorted entries, hiding new exemptions in diffs
With this gate:
•	any new exemption is visible (sorted position)
•	duplicates become impossible
To prevent truly broad exemptions (like allowlisting a whole op without location), the next tightening would be to require location for certain kinds (schema changes especially). That’s a separate diff because it’s a policy decision per kind.
________________________________________
Next diff (recommended, still “max scope”)
next diff: require location for schema-related kinds (policy narrowing)
Example rule:
•	if kind ends with _schema_*, require location to be present in policy items
That makes allowlisting schema changes always precise and prevents “allowlist all schema changes for this op” patterns.
Ship next diff: require location for schema-related kinds (policy narrowing)
next diff: require location for schema-related kinds (policy narrowing)
This tightens the allowlist so schema-impacting exemptions can never be broad. If a policy item targets a schema-related change kind, it must specify a precise location (e.g., responses.200.content.application/json.schema). This prevents “allow everything on this op” patterns for schema drift.
We’ll enforce this as a contract test (clear diagnostics, zero ambiguity). Optionally we can also encode it in the JSON Schema, but the test is more readable and easier to evolve as your kind set grows.
________________________________________
What counts as “schema-related kinds”?
By default, anything with _schema_ in the kind string, covering all your current emissions:
•	parameter_schema_narrowed
•	parameter_schema_widened
•	parameter_schema_changed_unknown
•	request_schema_narrowed
•	request_schema_widened
•	request_schema_changed_unknown
•	success_response_schema_narrowed
•	success_response_schema_widened
•	success_response_schema_changed_unknown
•	non_success_response_schema_changed
•	(and future schema kinds automatically)
Rule:
•	If "_schema_" in kind OR kind.endswith("_schema_changed_unknown") (redundant but explicit), then location is required in the policy item.
________________________________________
Diff A — Add policy narrowing lint test
New file: tests/test_openapi_breaking_policy_requires_location_for_schema_kinds.py
diff --git a/tests/test_openapi_breaking_policy_requires_location_for_schema_kinds.py b/tests/test_openapi_breaking_policy_requires_location_for_schema_kinds.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_breaking_policy_requires_location_for_schema_kinds.py
@@ -0,0 +1,125 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+
+ROOT = Path(__file__).resolve().parents[1]
+POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+
+
+def _is_schema_kind(kind: str) -> bool:
+    # Tight, future-proof heuristic: any kind that mentions schema must be precisely located.
+    return "_schema_" in kind
+
+
+def _check(items: list[dict], *, section: str) -> None:
+    bad = []
+    for it in items:
+        kind = it.get("kind")
+        if not isinstance(kind, str) or not kind:
+            continue
+        if not _is_schema_kind(kind):
+            continue
+        loc = it.get("location")
+        if not isinstance(loc, str) or not loc.strip():
+            # Display anchor (op/path) for diagnosis
+            anchor = it.get("op") or it.get("path") or "<missing op/path>"
+            bad.append((kind, str(anchor)))
+
+    if bad:
+        lines = [
+            "OpenAPI breaking policy invalid: schema-related policy items must include a non-empty 'location'.",
+            f"Section: {section}",
+            "",
+            "Missing location for:",
+        ]
+        for kind, anchor in bad:
+            lines.append(f"- kind={kind} anchor={anchor}")
+        lines.append("")
+        lines.append(
+            "Fix: add 'location' to each schema-related allowlist entry, e.g. "
+            "'responses.200.content.application/json.schema' or 'requestBody.content.application/json.schema'."
+        )
+        raise AssertionError("\n".join(lines))
+
+
+def test_policy_requires_location_for_schema_kinds_allow_unknown() -> None:
+    assert POLICY_PATH.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    policy = json.loads(POLICY_PATH.read_text(encoding="utf-8"))
+    items = policy.get("allow_unknown") or []
+    assert isinstance(items, list)
+    for it in items:
+        assert isinstance(it, dict)
+    _check(items, section="allow_unknown")
+
+
+def test_policy_requires_location_for_schema_kinds_allow_breaking() -> None:
+    assert POLICY_PATH.exists(), "Missing tests/contracts/openapi_breaking_policy.json"
+    policy = json.loads(POLICY_PATH.read_text(encoding="utf-8"))
+    items = policy.get("allow_breaking") or []
+    assert isinstance(items, list)
+    for it in items:
+        assert isinstance(it, dict)
+    _check(items, section="allow_breaking")
This is intentionally:
•	strict for schema kinds
•	silent for non-schema kinds (those may be allowed op-wide if you choose)
•	deterministic and review-friendly
________________________________________
Optional Diff B — Encode the same rule into the JSON Schema (heavier)
JSON Schema can express “if kind matches pattern then require location”, but it’s awkward because Draft 2020-12 uses if/then and patterns in if. Example approach:
•	add an allOf with:
o	if: { properties: { kind: { pattern: ".*_schema_.*" } } }
o	then: { required: ["location"] }
If you want that extra structural layer too, say:
“next diff: schema-level location requirement”.
In practice, the test gate is usually enough and clearer.
________________________________________
Next diff after this (Scope 3 continues)
next diff: unknowns block releases unless allowlisted + release gate wires policy into diff report (tagged CI)
That’s the point where:
•	unknown changes become “breaking for release” unless allowlisted
•	SemVer major enforcement can be built on top cleanly
Ship next diff: unknowns block releases unless allowlisted + release gate wires policy into diff report (tagged CI)
Thought for 9s
next diff: unknowns block releases unless allowlisted + release gate wires policy into diff report (tagged CI)
This adds a tag-time hard gate:
•	On release tags (vX.Y.Z), CI computes the OpenAPI diff vs previous tag
•	Applies tests/contracts/openapi_breaking_policy.json
•	Fails the job if any unknown_changes remain after policy application
o	i.e., “unknowns block releases unless allowlisted”
•	Writes an auditable dist/openapi_diff_report.json artifact for the run (useful for release/BOM wiring later)
This diff does not enforce SemVer-major on breaking changes yet (that’s the next release-hardening step). This step is purely: no unreviewed ambiguity ships.
________________________________________
Diff A — Add tag gate script
New file: scripts/check_openapi_release_gate.py
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/check_openapi_release_gate.py
@@ -0,0 +1,251 @@
+from __future__ import annotations
+
+import json
+import os
+import re
+import subprocess
+import sys
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+from code_audit.web_api.openapi_diff import apply_allowlist_policy, diff_openapi_core
+from code_audit.web_api.openapi_normalize import normalize_openapi
+
+
+ROOT = Path(__file__).resolve().parents[1]
+CURRENT_OPENAPI = ROOT / "docs" / "openapi.json"
+POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+OUT_DIR = ROOT / "dist"
+OUT_REPORT = OUT_DIR / "openapi_diff_report.json"
+
+
+TAG_RE = re.compile(r"^v(\d+)\.(\d+)\.(\d+)$")
+
+
+def _run_git(args: list[str]) -> str:
+    p = subprocess.run(
+        ["git", *args],
+        cwd=str(ROOT),
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+        check=False,
+    )
+    if p.returncode != 0:
+        raise RuntimeError(f"git {' '.join(args)} failed: {p.stderr.strip()}")
+    return p.stdout
+
+
+def _current_tag_from_env() -> str:
+    # GitHub Actions: GITHUB_REF_NAME is the tag name for tag triggers
+    tag = os.environ.get("GITHUB_REF_NAME") or ""
+    tag = tag.strip()
+    if not tag:
+        raise RuntimeError("GITHUB_REF_NAME is not set; this gate is intended for tag builds.")
+    return tag
+
+
+def _list_tags_desc() -> list[str]:
+    # Ensure semantic version tags are sorted newest->oldest (git's version sort handles v-prefixed tags).
+    out = _run_git(["tag", "--list", "v*", "--sort=-v:refname"])
+    tags = [t.strip() for t in out.splitlines() if t.strip()]
+    return tags
+
+
+def _find_previous_tag(current: str) -> Optional[str]:
+    tags = _list_tags_desc()
+    if current not in tags:
+        # If current tag isn't present (rare), still attempt to proceed by treating current as newest.
+        # But we cannot reliably choose prev; hard fail for governance.
+        raise RuntimeError(f"Current tag '{current}' not found in git tag list.")
+
+    idx = tags.index(current)
+    if idx == len(tags) - 1:
+        # No previous tag exists
+        return None
+    return tags[idx + 1]
+
+
+def _load_json(path: Path) -> Dict[str, Any]:
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _load_openapi_at_tag(tag: str) -> Dict[str, Any]:
+    # Load docs/openapi.json from a specific tag without checkout.
+    raw = _run_git(["show", f"{tag}:docs/openapi.json"])
+    return json.loads(raw)
+
+
+def _summarize_report(report: Dict[str, Any]) -> str:
+    s = report.get("summary") or {}
+    return (
+        f"breaking={report.get('breaking')} "
+        f"breaking_count={s.get('breaking_count')} "
+        f"unknown_count={s.get('unknown_count')} "
+        f"non_breaking_count={s.get('non_breaking_count')}"
+    )
+
+
+def main() -> int:
+    current_tag = _current_tag_from_env()
+    m = TAG_RE.match(current_tag)
+    if not m:
+        raise RuntimeError(
+            f"Tag '{current_tag}' does not match vX.Y.Z. This gate expects semver tags."
+        )
+
+    if not CURRENT_OPENAPI.exists():
+        raise RuntimeError("Missing docs/openapi.json (current OpenAPI snapshot).")
+
+    if not POLICY_PATH.exists():
+        raise RuntimeError("Missing tests/contracts/openapi_breaking_policy.json (required for release gate).")
+
+    prev = _find_previous_tag(current_tag)
+    if prev is None:
+        # First release: nothing to diff against. Still produce a report for audit.
+        OUT_DIR.mkdir(parents=True, exist_ok=True)
+        report = {
+            "version": 1,
+            "breaking": False,
+            "summary": {"breaking_count": 0, "non_breaking_count": 0, "unknown_count": 0},
+            "breaking_changes": [],
+            "non_breaking_changes": [],
+            "unknown_changes": [],
+            "notes": f"First release tag {current_tag}; no previous tag found.",
+        }
+        OUT_REPORT.write_text(json.dumps(report, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+        print(f"[openapi-release-gate] {current_tag}: first tag, no diff baseline. OK.")
+        return 0
+
+    before_doc = _load_openapi_at_tag(prev)
+    after_doc = _load_json(CURRENT_OPENAPI)
+    policy = _load_json(POLICY_PATH)
+
+    before_norm = normalize_openapi(before_doc)
+    after_norm = normalize_openapi(after_doc)
+    raw_report = diff_openapi_core(before_norm, after_norm)
+    gated_report = apply_allowlist_policy(raw_report, policy=policy)
+
+    OUT_DIR.mkdir(parents=True, exist_ok=True)
+    out = gated_report.to_dict()
+    out["baseline_tag"] = prev
+    out["current_tag"] = current_tag
+    out["policy_path"] = str(POLICY_PATH.relative_to(ROOT))
+    OUT_REPORT.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+
+    print(f"[openapi-release-gate] baseline={prev} current={current_tag} {_summarize_report(out)}")
+
+    # Hard requirement for this diff: unknowns must be empty after allowlist policy.
+    unknown_count = int((out.get("summary") or {}).get("unknown_count") or 0)
+    if unknown_count > 0:
+        print("")
+        print("[openapi-release-gate] FAIL: unknown OpenAPI changes remain after policy application.")
+        print("These must be allowlisted (with reason + location for schema kinds) before a release tag can pass.")
+        print("")
+        # Print the first few unknowns for fast triage (bounded output)
+        unknowns = out.get("unknown_changes") or []
+        for i, ch in enumerate(unknowns[:10]):
+            kind = ch.get("kind")
+            op = ch.get("op")
+            loc = ch.get("location")
+            print(f"  - {kind} :: {op} @ {loc}")
+        if len(unknowns) > 10:
+            print(f"  ... ({len(unknowns) - 10} more)")
+        return 1
+
+    print("[openapi-release-gate] OK: no unknowns remain after policy.")
+    return 0
+
+
+if __name__ == "__main__":
+    try:
+        raise SystemExit(main())
+    except Exception as e:
+        print(f"[openapi-release-gate] ERROR: {e}", file=sys.stderr)
+        raise
Behavior:
•	Always writes dist/openapi_diff_report.json
•	First tag: no previous baseline → writes a trivial “OK” report
•	Otherwise: diffs vs previous tag, applies policy, fails if unknown_count > 0
________________________________________
Diff B — Add tagged CI workflow gate
New file: .github/workflows/openapi-release-gate.yml
diff --git a/.github/workflows/openapi-release-gate.yml b/.github/workflows/openapi-release-gate.yml
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/.github/workflows/openapi-release-gate.yml
@@ -0,0 +1,86 @@
+name: openapi-release-gate
+
+on:
+  push:
+    tags:
+      - "v*"
+  workflow_dispatch: {}
+
+jobs:
+  openapi-release-gate:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout (full history + tags)
+        uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Set up Python
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.13"
+
+      - name: Install package (editable)
+        run: |
+          python -m pip install --upgrade pip
+          python -m pip install -e .
+
+      - name: OpenAPI release gate (unknowns must be allowlisted)
+        env:
+          CI: "true"
+        run: |
+          python scripts/check_openapi_release_gate.py
+
+      - name: Upload diff report artifact
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: openapi-diff-report
+          path: dist/openapi_diff_report.json
Notes:
•	fetch-depth: 0 is essential so the script can git tag and git show <tag>:docs/openapi.json
•	This workflow is hard-fail (blocks the tag pipeline) when unknowns remain
•	Artifact upload is always() so you get the report even on failure
________________________________________
Diff C — Add a small unit test for the gate script output invariants (optional but useful)
If you want a lightweight guard that the script at least imports/runs in non-tag contexts (without actually running git), you can skip this. The workflow is the real enforcement.
I’m leaving it out in this diff to keep it minimal and focused.
________________________________________
What’s enforced now
On any v* tag build:
•	Unknown changes must be empty after allowlist policy application
•	Unknowns can only be cleared by adding policy entries with:
o	kind
o	op or path
o	reason
o	and (for schema kinds) a required location (from your previous policy narrowing gate)
This guarantees:
•	you never ship OpenAPI drift that the classifier couldn’t confidently classify (or that you didn’t explicitly allow)
________________________________________
Next diff after this (natural follow-on)
next diff: release gate also enforces “breaking implies major bump” (SemVer major gating)
…and optionally:
•	attach the diff report to GitHub Releases (release artifact pack)
•	include policy hash + report hash in Release BOM (full provenance)
Ship next diff: release gate also enforces “breaking implies major bump” (SemVer major gating) …and optionally:
next diff: release gate enforces “breaking implies major bump” (SemVer major gating) + optional hooks
This extends the tag-time OpenAPI release gate so that:
•	If the policy-applied diff report still has breaking_changes > 0, then the current tag must be a major bump vs the previous tag.
•	Unknown blocking stays as-is (unknowns must be allowlisted).
•	The gate remains deterministic and fully auditable via the dist/openapi_diff_report.json artifact.
I’m also including two optional, high-value extensions at the end:
1.	allow configuring “success” statuses (2xx only vs 2xx+3xx), and
2.	attach the diff report directly to the GitHub Release (if you’re using a release workflow).
________________________________________
Diff A — Update release gate script to enforce major bump on breaking
Patch: scripts/check_openapi_release_gate.py
Adds:
•	parse previous/current semver from vX.Y.Z
•	if breaking_count > 0 then require current_major > prev_major
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 1111111..2222222 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,6 +1,6 @@
 from __future__ import annotations
 
 import json
 import os
 import re
 import subprocess
 import sys
@@
 TAG_RE = re.compile(r"^v(\d+)\.(\d+)\.(\d+)$")
@@
 def _current_tag_from_env() -> str:
@@
     return tag
+
+
+def _parse_tag(tag: str) -> tuple[int, int, int]:
+    m = TAG_RE.match(tag)
+    if not m:
+        raise RuntimeError(f"Tag '{tag}' does not match vX.Y.Z.")
+    return (int(m.group(1)), int(m.group(2)), int(m.group(3)))
@@
 def main() -> int:
     current_tag = _current_tag_from_env()
-    m = TAG_RE.match(current_tag)
-    if not m:
-        raise RuntimeError(
-            f"Tag '{current_tag}' does not match vX.Y.Z. This gate expects semver tags."
-        )
+    cur_major, cur_minor, cur_patch = _parse_tag(current_tag)
@@
     prev = _find_previous_tag(current_tag)
     if prev is None:
@@
         print(f"[openapi-release-gate] {current_tag}: first tag, no diff baseline. OK.")
         return 0
+
+    prev_major, prev_minor, prev_patch = _parse_tag(prev)
@@
     before_doc = _load_openapi_at_tag(prev)
@@
     raw_report = diff_openapi_core(before_norm, after_norm)
     gated_report = apply_allowlist_policy(raw_report, policy=policy)
@@
     print(f"[openapi-release-gate] baseline={prev} current={current_tag} {_summarize_report(out)}")
@@
     unknown_count = int((out.get("summary") or {}).get("unknown_count") or 0)
     if unknown_count > 0:
@@
         return 1
 
+    # SemVer major gating: breaking changes require major bump.
+    breaking_count = int((out.get("summary") or {}).get("breaking_count") or 0)
+    if breaking_count > 0 and cur_major <= prev_major:
+        print("")
+        print("[openapi-release-gate] FAIL: breaking OpenAPI changes detected but tag is not a major bump.")
+        print(f"Baseline tag: {prev} (major={prev_major})")
+        print(f"Current tag:  {current_tag} (major={cur_major})")
+        print("")
+        print("Rule: if breaking_count > 0 after policy, current_major must be > previous_major.")
+        print("Fix: bump major version (v(N+1).0.0) or allowlist/avoid the breaking changes.")
+        print("")
+        # Print the first few breakings for fast triage (bounded)
+        breaks = out.get('breaking_changes') or []
+        for i, ch in enumerate(breaks[:10]):
+            kind = ch.get("kind")
+            op = ch.get("op")
+            loc = ch.get("location")
+            print(f"  - {kind} :: {op} @ {loc}")
+        if len(breaks) > 10:
+            print(f"  ... ({len(breaks) - 10} more)")
+        return 1
+
     print("[openapi-release-gate] OK: no unknowns remain after policy.")
     return 0
Notes on semantics
•	The check is based on policy-applied breaking_count.
If you allowlist breakings and convert them to unknown/non-breaking, the major gate will follow that (intentionally).
•	Unknowns are still a hard fail before we reach the SemVer check, keeping the contract: no ambiguous drift ships.
________________________________________
Diff B — Optional: allow configuring success status policy for gating
If you want release gating to treat 3xx as success too (some APIs consider redirects part of contract), add an env var:
•	OPENAPI_SUCCESS_PREFIXES="2,3" (default: 2)
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 2222222..3333333 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@
 def main() -> int:
@@
+    prefixes_raw = (os.environ.get("OPENAPI_SUCCESS_PREFIXES") or "2").strip()
+    success_prefixes = tuple([p.strip() for p in prefixes_raw.split(",") if p.strip()])
@@
-    raw_report = diff_openapi_core(before_norm, after_norm)
+    raw_report = diff_openapi_core(before_norm, after_norm, success_status_prefixes=success_prefixes)
This is optional; default stays ("2",).
________________________________________
Diff C — Optional: wire this gate into your existing release workflow
If you already have a release workflow (tag-triggered) and want the check there (instead of or in addition to openapi-release-gate.yml), add a step:
- name: OpenAPI gate (unknowns blocked + breaking requires major)
  env:
    CI: "true"
    OPENAPI_SUCCESS_PREFIXES: "2"
  run: |
    python scripts/check_openapi_release_gate.py
If you’re using “single unified release workflow,” you can delete the separate openapi-release-gate.yml and embed this step there. If you like “belt and suspenders,” keep both.
________________________________________
What’s enforced now (tag builds)
For any vX.Y.Z tag:
1.	Diff current docs/openapi.json vs previous tag’s docs/openapi.json
2.	Apply allowlist policy
3.	Fail if unknown_count > 0
4.	Fail if breaking_count > 0 AND current_major is not greater than previous_major
5.	Upload dist/openapi_diff_report.json for audit/debug
This is the clean “SemVer discipline meets governed contracts” enforcement.
________________________________________
Next diffs you likely want (high value)
If you want the optional “complete operator experience” chain right after this, the next ones are:
•	next diff: attach openapi_diff_report.json to GitHub Release assets (release is self-describing)
•	next diff: BOM includes openapi diff report hash + policy hash (full provenance)
•	next diff: changelog truth enforcement for web_api_openapi_breaking axis (no “lying in metadata”)
Ship next diff: attach openapi_diff_report.json to GitHub Release assets (release is self-describing)
next diff: attach openapi_diff_report.json to GitHub Release assets (release is self-describing)
This diff updates your release workflow so every GitHub Release automatically includes the exact OpenAPI diff report produced by the tag gate:
•	dist/openapi_diff_report.json is generated by scripts/check_openapi_release_gate.py
•	That file is uploaded as a Release asset (not just a CI artifact)
•	Result: anyone can open the release and see the exact contract delta and policy application outcome for that tag
There are two clean ways to do this, depending on how you currently publish releases:
1.	If you already use softprops/action-gh-release → easiest
2.	If you use gh release create/upload → also easy, but more scripting
I’m shipping the softprops version because it’s the most common and least brittle.
________________________________________
Diff A — Update release workflow to publish the diff report as a Release asset
Patch: .github/workflows/release.yml
Assumptions:
•	Your release workflow triggers on push tags v*
•	You are already creating a release (or you want to create one)
•	You can add one extra file under files:
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 1111111..2222222 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -1,6 +1,8 @@
 name: release
 
 on:
   push:
     tags:
       - "v*"
 
 jobs:
   release:
     runs-on: ubuntu-latest
     steps:
       - name: Checkout (full history + tags)
         uses: actions/checkout@v4
         with:
           fetch-depth: 0
 
       - name: Set up Python
         uses: actions/setup-python@v5
         with:
           python-version: "3.13"
 
       - name: Install package (editable)
         run: |
           python -m pip install --upgrade pip
           python -m pip install -e .
 
+      - name: OpenAPI release gate (produces dist/openapi_diff_report.json)
+        env:
+          CI: "true"
+        run: |
+          python scripts/check_openapi_release_gate.py
+
       - name: Build distribution
         run: |
           python -m pip install build
           python -m build
+
+      - name: Verify OpenAPI diff report exists
+        run: |
+          test -f dist/openapi_diff_report.json
+          echo "Found dist/openapi_diff_report.json"
 
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
+            dist/openapi_diff_report.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
Why both dist/* and explicit dist/openapi_diff_report.json?
If your workflow already uploads dist/*, the explicit line is redundant. But I include it because:
•	some repos filter dist artifacts (wheels/sdists only)
•	this makes the intent unmissable in review
If you’d rather keep it clean, remove the explicit line and just ensure the glob includes it.
________________________________________
Diff B — If you don’t have release.yml yet (minimal new workflow)
If you don’t have a release workflow file yet, here’s a minimal one that:
•	runs the OpenAPI tag gate script
•	builds dist
•	publishes a GitHub Release with both the distribution artifacts and the OpenAPI diff report
New file: .github/workflows/release.yml
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/.github/workflows/release.yml
@@ -0,0 +1,75 @@
+name: release
+
+on:
+  push:
+    tags:
+      - "v*"
+
+jobs:
+  release:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout (full history + tags)
+        uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Set up Python
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.13"
+
+      - name: Install package (editable)
+        run: |
+          python -m pip install --upgrade pip
+          python -m pip install -e .
+
+      - name: OpenAPI release gate (unknowns blocked + breaking implies major)
+        env:
+          CI: "true"
+        run: |
+          python scripts/check_openapi_release_gate.py
+
+      - name: Build distribution
+        run: |
+          python -m pip install build
+          python -m build
+
+      - name: Verify OpenAPI diff report exists
+        run: |
+          test -f dist/openapi_diff_report.json
+          echo "Found dist/openapi_diff_report.json"
+
+      - name: Create GitHub Release and upload assets
+        uses: softprops/action-gh-release@v2
+        with:
+          files: |
+            dist/*
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
________________________________________
Notes / gotchas
1) Keep fetch-depth: 0
Your check_openapi_release_gate.py relies on git tag + git show <tag>:docs/openapi.json, so shallow clones will break.
2) Ensure the gate runs before the release is created
That way you don’t publish a Release and then fail afterward.
3) Release assets become part of your immutable audit trail
Anyone can pull:
•	the diff report
•	your wheel/sdist
and reconstruct exactly what contract changes shipped.
________________________________________
Next diff after this (strongly recommended)
next diff: release asset includes policy file + policy hash line inside the diff report
So the release contains:
•	openapi_diff_report.json
•	openapi_breaking_policy.json
and the report includes the policy SHA (proving which policy state was applied when the release was minted).
Ship next diff: release asset includes policy file + policy hash line inside the diff report
next diff: release asset includes policy file + policy hash line inside the diff report
This diff does two things:
1.	Release assets now include openapi_breaking_policy.json (the exact policy used to “greenlight” unknown/breaking changes).
2.	The generated dist/openapi_diff_report.json includes a policy hash (sha256 + short) so the report is self-proving and auditable.
________________________________________
Diff A — Stamp policy hash into openapi_diff_report.json
Patch: scripts/check_openapi_release_gate.py
We compute:
•	policy_sha256 of tests/contracts/openapi_breaking_policy.json
•	write both the full hash and a short prefix into the report
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 3333333..4444444 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
+import hashlib
 import json
 import os
 import re
 import subprocess
 import sys
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
@@
 def _load_json(path: Path) -> Dict[str, Any]:
     return json.loads(path.read_text(encoding="utf-8"))
 
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(1024 * 64), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
@@
 def main() -> int:
@@
     if not POLICY_PATH.exists():
         raise RuntimeError("Missing tests/contracts/openapi_breaking_policy.json (required for release gate).")
@@
     before_doc = _load_openapi_at_tag(prev)
     after_doc = _load_json(CURRENT_OPENAPI)
     policy = _load_json(POLICY_PATH)
+    policy_sha = _sha256_file(POLICY_PATH)
@@
     raw_report = diff_openapi_core(before_norm, after_norm)
     gated_report = apply_allowlist_policy(raw_report, policy=policy)
@@
     out = gated_report.to_dict()
     out["baseline_tag"] = prev
     out["current_tag"] = current_tag
     out["policy_path"] = str(POLICY_PATH.relative_to(ROOT))
+    out["policy_sha256"] = policy_sha
+    out["policy_sha256_short"] = policy_sha[:12]
     OUT_REPORT.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
Why full + short?
•	Full is for cryptographic verification
•	Short is for human scanning in logs/releases/BOMs
________________________________________
Diff B — Upload policy file as a Release asset
Patch: .github/workflows/release.yml
Assuming you’re using softprops/action-gh-release@v2 already.
We add tests/contracts/openapi_breaking_policy.json to uploaded files.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 2222222..3333333 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -1,6 +1,6 @@
 name: release
@@
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
+            tests/contracts/openapi_breaking_policy.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
Optional tightening (recommended): verify the policy file exists before release
Add this step just before action-gh-release:
@@
       - name: Verify OpenAPI policy file exists
         run: |
           test -f tests/contracts/openapi_breaking_policy.json
           echo "Found tests/contracts/openapi_breaking_policy.json"
________________________________________
Resulting release is self-describing
A consumer (or auditor) can now download from the GitHub Release:
•	openapi_diff_report.json (includes policy_sha256)
•	openapi_breaking_policy.json
…and verify the report was produced using that exact policy content.
________________________________________
Next diff (final provenance hardening)
next diff: include openapi snapshot sha(s) + baseline tag snapshot sha inside the diff report
So the report proves:
•	which exact OpenAPI snapshots were compared
•	not just which tags were referenced
Ship next diff: release asset includes policy file + policy hash line inside the diff report
next diff: release asset includes policy file + policy hash line inside the diff report
This ships two hardening moves together:
1.	Stamp a policy hash into dist/openapi_diff_report.json so the report cryptographically identifies the exact allowlist policy that was applied.
2.	Upload tests/contracts/openapi_breaking_policy.json as a GitHub Release asset, alongside your build artifacts and the diff report.
That makes each release self-describing: “Here is the OpenAPI diff; here is the exact policy that permitted it; here is the hash proving they match.”
________________________________________
Diff A — Stamp policy_sha256 into the diff report
Patch: scripts/check_openapi_release_gate.py
Add a small SHA helper, compute the hash of the policy file, and inject it into the report payload.
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 3333333..4444444 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
+import hashlib
 import json
 import os
 import re
 import subprocess
 import sys
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@
 def _load_json(path: Path) -> Dict[str, Any]:
     return json.loads(path.read_text(encoding="utf-8"))
 
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(1024 * 64), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
@@
 def main() -> int:
@@
     if not POLICY_PATH.exists():
         raise RuntimeError("Missing tests/contracts/openapi_breaking_policy.json (required for release gate).")
@@
     before_doc = _load_openapi_at_tag(prev)
     after_doc = _load_json(CURRENT_OPENAPI)
     policy = _load_json(POLICY_PATH)
+    policy_sha = _sha256_file(POLICY_PATH)
@@
     raw_report = diff_openapi_core(before_norm, after_norm)
     gated_report = apply_allowlist_policy(raw_report, policy=policy)
@@
     out = gated_report.to_dict()
     out["baseline_tag"] = prev
     out["current_tag"] = current_tag
     out["policy_path"] = str(POLICY_PATH.relative_to(ROOT))
+    out["policy_sha256"] = policy_sha
+    out["policy_sha256_short"] = policy_sha[:12]
     OUT_REPORT.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
Why include both full + short?
•	policy_sha256 is the canonical verification value.
•	policy_sha256_short makes it easy to eyeball in logs and release notes without losing provenance.
What this enables later
•	BOM enforcement can require the report’s policy_sha256 to match a recorded BOM entry.
•	You can detect policy drift across releases at a glance.
________________________________________
Diff B — Upload the policy file as a Release asset
Patch: .github/workflows/release.yml
Add the policy file to the release upload list.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 2222222..3333333 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -1,6 +1,6 @@
 name: release
@@
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
+            tests/contracts/openapi_breaking_policy.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
Optional: assert presence before publishing (recommended)
This makes failures obvious before the release is minted:
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 3333333..4444444 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Verify OpenAPI diff report exists
         run: |
           test -f dist/openapi_diff_report.json
           echo "Found dist/openapi_diff_report.json"
+
+      - name: Verify OpenAPI policy exists
+        run: |
+          test -f tests/contracts/openapi_breaking_policy.json
+          echo "Found tests/contracts/openapi_breaking_policy.json"
________________________________________
End state after this diff
On every vX.Y.Z tag release:
•	CI generates dist/openapi_diff_report.json
•	The diff report contains:
o	baseline_tag
o	current_tag
o	policy_path
o	policy_sha256 + policy_sha256_short
•	GitHub Release assets include:
o	build artifacts (dist/*)
o	openapi_diff_report.json
o	openapi_breaking_policy.json
Anyone can:
1.	download openapi_breaking_policy.json
2.	SHA-256 it
3.	compare to policy_sha256 in the report
…and prove exactly what policy state governed that release.
________________________________________
Next diff (natural “provenance completes the triangle” step)
next diff: include OpenAPI snapshot hashes in the report (before/after sha256 + file paths)
So the report proves:
•	the exact policy used (done here)
•	the exact OpenAPI snapshots compared (next)
Ship next diff: include OpenAPI snapshot hashes in the report (before/after sha256 + file paths
next diff: include OpenAPI snapshot hashes in the report (before/after sha256 + file paths)
This tightens provenance so dist/openapi_diff_report.json proves exactly which OpenAPI documents were compared, not just which tags were referenced.
You’ll add:
•	before_openapi_path (baseline source path inside repo at tag)
•	after_openapi_path (current repo path)
•	before_openapi_sha256 + _short
•	after_openapi_sha256 + _short
This pairs with the policy hash you already stamped.
________________________________________
Diff A — Stamp before/after OpenAPI hashes into the report
Patch: scripts/check_openapi_release_gate.py
We compute:
•	SHA for after from docs/openapi.json on disk
•	SHA for before from the tag content returned by git show <tag>:docs/openapi.json (hash the raw bytes)
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 4444444..5555555 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import re
 import subprocess
 import sys
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@
 CURRENT_OPENAPI = ROOT / "docs" / "openapi.json"
 POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 OUT_DIR = ROOT / "dist"
 OUT_REPORT = OUT_DIR / "openapi_diff_report.json"
+OPENAPI_REPO_PATH = "docs/openapi.json"
 
@@
 def _sha256_file(path: Path) -> str:
     h = hashlib.sha256()
     with path.open("rb") as f:
         for chunk in iter(lambda: f.read(1024 * 64), b""):
             h.update(chunk)
     return h.hexdigest()
+
+
+def _sha256_bytes(data: bytes) -> str:
+    return hashlib.sha256(data).hexdigest()
 
@@
 def _load_openapi_at_tag(tag: str) -> Dict[str, Any]:
     # Load docs/openapi.json from a specific tag without checkout.
-    raw = _run_git(["show", f"{tag}:docs/openapi.json"])
-    return json.loads(raw)
+    raw = _run_git(["show", f"{tag}:{OPENAPI_REPO_PATH}"])
+    return json.loads(raw)
+
+
+def _load_openapi_raw_at_tag(tag: str) -> bytes:
+    # Return exact bytes as stored in git for hashing/provenance.
+    raw = _run_git(["show", f"{tag}:{OPENAPI_REPO_PATH}"])
+    return raw.encode("utf-8")
 
@@
 def main() -> int:
@@
     if not CURRENT_OPENAPI.exists():
         raise RuntimeError("Missing docs/openapi.json (current OpenAPI snapshot).")
@@
     prev = _find_previous_tag(current_tag)
@@
     before_doc = _load_openapi_at_tag(prev)
     after_doc = _load_json(CURRENT_OPENAPI)
     policy = _load_json(POLICY_PATH)
     policy_sha = _sha256_file(POLICY_PATH)
+
+    # OpenAPI snapshot provenance
+    before_openapi_raw = _load_openapi_raw_at_tag(prev)
+    before_openapi_sha = _sha256_bytes(before_openapi_raw)
+    after_openapi_sha = _sha256_file(CURRENT_OPENAPI)
 
@@
     out = gated_report.to_dict()
     out["baseline_tag"] = prev
     out["current_tag"] = current_tag
     out["policy_path"] = str(POLICY_PATH.relative_to(ROOT))
     out["policy_sha256"] = policy_sha
     out["policy_sha256_short"] = policy_sha[:12]
+
+    out["before_openapi_path"] = f"{prev}:{OPENAPI_REPO_PATH}"
+    out["before_openapi_sha256"] = before_openapi_sha
+    out["before_openapi_sha256_short"] = before_openapi_sha[:12]
+
+    out["after_openapi_path"] = OPENAPI_REPO_PATH
+    out["after_openapi_sha256"] = after_openapi_sha
+    out["after_openapi_sha256_short"] = after_openapi_sha[:12]
+
     OUT_REPORT.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
Why hash “before” from git bytes?
Because it’s the exact baseline snapshot that was used—no checkout, no local formatting influence, no ambiguity.
Why store before_openapi_path as <tag>:docs/openapi.json?
So the provenance points to the immutable git object identity source.
________________________________________
Optional micro-tightening (recommended): include the raw baseline tag in the printed summary
This makes log triage faster (no need to open the artifact):
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 5555555..6666666 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@
-    print(f"[openapi-release-gate] baseline={prev} current={current_tag} {_summarize_report(out)}")
+    print(
+        f"[openapi-release-gate] baseline={prev} ({before_openapi_sha[:12]}) "
+        f"current={current_tag} ({after_openapi_sha[:12]}) {_summarize_report(out)}"
+    )
Not required for correctness—just nicer operator UX.
________________________________________
End state after this diff
Every release now has a diff report that proves:
•	which baseline: baseline_tag + before_openapi_sha256
•	which current snapshot: after_openapi_sha256
•	which policy: policy_sha256
•	plus the actual classified deltas
That’s the full audit triangle: inputs + policy + output.
________________________________________
Next diff (provenance “max”)
next diff: include the classifier code fingerprint (e.g., hash openapi_diff.py + schema_semver.py + openapi_normalize.py under a manifest gate) so the report also proves which diff engine version generated it.
Ship next diff: include the classifier code fingerprint (e.g., hash openapi_diff.py + schema_semver.py + openapi_normalize.py 
next diff: classifier code fingerprint in the OpenAPI diff report
This adds a classifier fingerprint to dist/openapi_diff_report.json so every release artifact can prove which exact diff engine code generated the result.
We’ll compute SHA-256 hashes for the critical modules:
•	src/code_audit/web_api/openapi_normalize.py
•	src/code_audit/web_api/openapi_diff.py
•	src/code_audit/web_api/schema_semver.py
…and write both full + short hashes to the report under a stable key.
This is intentionally orthogonal to signal_logic_version (which is for your analyzer/translator semantics). This fingerprint is a release-time attestation of the OpenAPI classifier surface.
________________________________________
Diff A — Extend release gate script to compute classifier fingerprints
Patch: scripts/check_openapi_release_gate.py
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 5555555..6666666 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import re
 import subprocess
 import sys
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Optional, Tuple, List
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 CURRENT_OPENAPI = ROOT / "docs" / "openapi.json"
 POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 OUT_DIR = ROOT / "dist"
 OUT_REPORT = OUT_DIR / "openapi_diff_report.json"
 OPENAPI_REPO_PATH = "docs/openapi.json"
+
+CLASSIFIER_FILES = [
+    ROOT / "src" / "code_audit" / "web_api" / "openapi_normalize.py",
+    ROOT / "src" / "code_audit" / "web_api" / "openapi_diff.py",
+    ROOT / "src" / "code_audit" / "web_api" / "schema_semver.py",
+]
 
@@
 def _sha256_file(path: Path) -> str:
@@
     return h.hexdigest()
 
@@
 def main() -> int:
@@
     before_doc = _load_openapi_at_tag(prev)
     after_doc = _load_json(CURRENT_OPENAPI)
     policy = _load_json(POLICY_PATH)
     policy_sha = _sha256_file(POLICY_PATH)
@@
     before_openapi_raw = _load_openapi_raw_at_tag(prev)
     before_openapi_sha = _sha256_bytes(before_openapi_raw)
     after_openapi_sha = _sha256_file(CURRENT_OPENAPI)
+
+    # Classifier fingerprint (proves which code generated this report)
+    classifier_fingerprint: Dict[str, Dict[str, str]] = {}
+    missing: List[str] = []
+    for p in CLASSIFIER_FILES:
+        if not p.exists():
+            missing.append(str(p.relative_to(ROOT)))
+            continue
+        h = _sha256_file(p)
+        classifier_fingerprint[str(p.relative_to(ROOT))] = {
+            "sha256": h,
+            "sha256_short": h[:12],
+        }
+    if missing:
+        raise RuntimeError(
+            "Missing classifier fingerprint files (release must be self-describing): "
+            + ", ".join(missing)
+        )
 
@@
     out["policy_path"] = str(POLICY_PATH.relative_to(ROOT))
     out["policy_sha256"] = policy_sha
     out["policy_sha256_short"] = policy_sha[:12]
@@
     out["after_openapi_path"] = OPENAPI_REPO_PATH
     out["after_openapi_sha256"] = after_openapi_sha
     out["after_openapi_sha256_short"] = after_openapi_sha[:12]
+
+    out["classifier_fingerprint"] = {
+        "files": classifier_fingerprint,
+        "file_count": len(classifier_fingerprint),
+    }
 
     OUT_REPORT.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
What gets written into the report
Example shape:
"classifier_fingerprint": {
  "file_count": 3,
  "files": {
    "src/code_audit/web_api/openapi_diff.py": {
      "sha256": "...",
      "sha256_short": "abc123..."
    },
    "src/code_audit/web_api/openapi_normalize.py": {
      "sha256": "...",
      "sha256_short": "def456..."
    },
    "src/code_audit/web_api/schema_semver.py": {
      "sha256": "...",
      "sha256_short": "7890ab..."
    }
  }
}
This is deterministic because:
•	keys are stable relative paths
•	output is JSON dumped with sort_keys=True
Hard-fail on missing files
This guarantees you never ship a release where the provenance section is “partial” or “unknown”.
________________________________________
Optional micro-tightening: include classifier fingerprint short hashes in logs
Nice for operator debugging (not required):
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 6666666..7777777 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@
-    print(f"[openapi-release-gate] baseline={prev} current={current_tag} {_summarize_report(out)}")
+    cf = out.get("classifier_fingerprint", {}).get("files", {})
+    cf_short = ", ".join([f"{k}:{v.get('sha256_short')}" for k, v in sorted(cf.items())])
+    print(f"[openapi-release-gate] baseline={prev} current={current_tag} {_summarize_report(out)}")
+    print(f"[openapi-release-gate] classifier_fingerprint: {cf_short}")
________________________________________
Next diff (if you want the “manifest-guarded engine surface”)
Right now this is an attestation. The next escalation is to contract-gate the classifier surface the same way you do translator policy:
•	Add tests/contracts/openapi_classifier_manifest.json
•	Hash AST-normalized forms of those modules
•	Require version bump (or release-axis bump) when the manifest changes
If you want that, say: next diff: OpenAPI classifier manifest gate.
Ship next diff: OpenAPI classifier manifest gate.
next diff: OpenAPI classifier manifest gate
This adds a contract gate for the OpenAPI semantic diff engine, analogous to your analyzer logic hash gate / translator policy gate:
•	Any semantic change to the OpenAPI classifier surface must be accompanied by:
o	a bump to a dedicated version anchor (recommended: openapi_classifier_version), and
o	a refreshed manifest of AST-normalized hashes
This prevents silent drift in:
•	normalization rules (openapi_normalize.py)
•	diff classification logic (openapi_diff.py)
•	schema semver comparator (schema_semver.py)
•	(optionally) allowlist application semantics if you include it
________________________________________
What this gate enforces
Trigger surface (files)
By default:
•	src/code_audit/web_api/openapi_normalize.py
•	src/code_audit/web_api/openapi_diff.py
•	src/code_audit/web_api/schema_semver.py
Optionally include:
•	src/code_audit/web_api/openapi_policy.py (if you later split policy logic out)
•	src/code_audit/web_api/openapi_scrub.py (if scrub logic is part of classifier semantics)
Contract rule
If the AST-normalized hash of any tracked file changes, then:
•	openapi_classifier_version must bump (or you can choose to reuse signal_logic_version, but I recommend a dedicated anchor since this is Web API contract surface)
________________________________________
Diff A — Add version anchor
Patch: src/code_audit/model/run_result.py
Add a new stable anchor near your existing version keys:
diff --git a/src/code_audit/model/run_result.py b/src/code_audit/model/run_result.py
index 1111111..2222222 100644
--- a/src/code_audit/model/run_result.py
+++ b/src/code_audit/model/run_result.py
@@ -28,8 +28,14 @@
 engine_version = "engine_v1"
 signal_logic_version = "signals_v1"
+
+# OpenAPI classifier contract anchor (web_api semantic diff engine).
+# Bump when OpenAPI normalization/diff/schema semantics change.
+openapi_classifier_version = "openapi_classifier_v1"
If you prefer the version anchor to live closer to the web_api code, place it in:
•	src/code_audit/web_api/__init__.py
or
•	src/code_audit/web_api/versioning.py
But run_result.py already holds your major semantic anchors and is consistently used by gates.
________________________________________
Diff B — Add manifest file
New file: tests/contracts/openapi_classifier_manifest.json
diff --git a/tests/contracts/openapi_classifier_manifest.json b/tests/contracts/openapi_classifier_manifest.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/openapi_classifier_manifest.json
@@ -0,0 +1,24 @@
+{
+  "version": 1,
+  "openapi_classifier_version": "openapi_classifier_v1",
+  "files": {
+    "src/code_audit/web_api/openapi_normalize.py": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    },
+    "src/code_audit/web_api/openapi_diff.py": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    },
+    "src/code_audit/web_api/schema_semver.py": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    }
+  }
+}
This will be populated by a refresh script.
________________________________________
Diff C — Add refresh script (AST-normalized hashing)
New file: scripts/refresh_openapi_classifier_manifest.py
This follows the same principles as your existing manifest refreshers:
•	parse Python AST
•	strip docstrings
•	normalize
•	ignore version literals (openapi_classifier_version itself)
•	hash deterministically
diff --git a/scripts/refresh_openapi_classifier_manifest.py b/scripts/refresh_openapi_classifier_manifest.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/refresh_openapi_classifier_manifest.py
@@ -0,0 +1,220 @@
+from __future__ import annotations
+
+import ast
+import hashlib
+import json
+from pathlib import Path
+from typing import Dict, Any
+
+
+ROOT = Path(__file__).resolve().parents[1]
+MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+
+TARGET_FILES = [
+    "src/code_audit/web_api/openapi_normalize.py",
+    "src/code_audit/web_api/openapi_diff.py",
+    "src/code_audit/web_api/schema_semver.py",
+]
+
+
+def _sha256_text(s: str) -> str:
+    return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+class _StripDocstrings(ast.NodeTransformer):
+    def _strip_body(self, body):
+        if not body:
+            return body
+        # Remove leading docstring Expr nodes
+        if isinstance(body[0], ast.Expr) and isinstance(getattr(body[0], "value", None), ast.Constant):
+            if isinstance(body[0].value.value, str):
+                return body[1:]
+        return body
+
+    def visit_Module(self, node: ast.Module):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_FunctionDef(self, node: ast.FunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_ClassDef(self, node: ast.ClassDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+
+class _NeutralizeVersionLiterals(ast.NodeTransformer):
+    """
+    Replace string constants assigned to openapi_classifier_version with a sentinel,
+    so bumping the version does not itself change the manifest hash.
+    """
+
+    def visit_Assign(self, node: ast.Assign):  # type: ignore[override]
+        # Look for: openapi_classifier_version = "..."
+        for t in node.targets:
+            if isinstance(t, ast.Name) and t.id == "openapi_classifier_version":
+                if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                    node.value = ast.Constant(value="__VERSION__")
+        return self.generic_visit(node)
+
+
+def _ast_normalized_source(path: Path) -> str:
+    src = path.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    tree = _StripDocstrings().visit(tree)  # type: ignore[assignment]
+    tree = _NeutralizeVersionLiterals().visit(tree)  # type: ignore[assignment]
+    ast.fix_missing_locations(tree)
+    # Use ast.dump for stable representation. Include attributes=False to ignore lineno/col offsets.
+    dumped = ast.dump(tree, annotate_fields=True, include_attributes=False)
+    return dumped
+
+
+def _load_openapi_classifier_version() -> str:
+    # Source of truth: src/code_audit/model/run_result.py
+    rr = ROOT / "src" / "code_audit" / "model" / "run_result.py"
+    src = rr.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    for node in tree.body:
+        if isinstance(node, ast.Assign):
+            for t in node.targets:
+                if isinstance(t, ast.Name) and t.id == "openapi_classifier_version":
+                    if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                        return node.value.value
+    raise RuntimeError("openapi_classifier_version not found in src/code_audit/model/run_result.py")
+
+
+def main() -> int:
+    out: Dict[str, Any] = {
+        "version": 1,
+        "openapi_classifier_version": _load_openapi_classifier_version(),
+        "files": {},
+    }
+
+    for rel in TARGET_FILES:
+        p = ROOT / rel
+        if not p.exists():
+            raise RuntimeError(f"Missing target file: {rel}")
+        dumped = _ast_normalized_source(p)
+        h = _sha256_text(dumped)
+        out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
+
+    MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)
+    MANIFEST_PATH.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+    print(f"Wrote {MANIFEST_PATH.relative_to(ROOT)}")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
________________________________________
Diff D — Add contract test that enforces version bump on semantic change
New file: tests/test_openapi_classifier_manifest_gate.py
This is the hard gate:
•	recompute AST hashes
•	compare to committed manifest
•	if changed, require openapi_classifier_version bump and manifest refresh
diff --git a/tests/test_openapi_classifier_manifest_gate.py b/tests/test_openapi_classifier_manifest_gate.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_openapi_classifier_manifest_gate.py
@@ -0,0 +1,178 @@
+from __future__ import annotations
+
+import ast
+import hashlib
+import json
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+
+TARGET_FILES = [
+    "src/code_audit/web_api/openapi_normalize.py",
+    "src/code_audit/web_api/openapi_diff.py",
+    "src/code_audit/web_api/schema_semver.py",
+]
+
+
+def _sha256_text(s: str) -> str:
+    return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+class _StripDocstrings(ast.NodeTransformer):
+    def _strip_body(self, body):
+        if not body:
+            return body
+        if isinstance(body[0], ast.Expr) and isinstance(getattr(body[0], "value", None), ast.Constant):
+            if isinstance(body[0].value.value, str):
+                return body[1:]
+        return body
+
+    def visit_Module(self, node: ast.Module):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_FunctionDef(self, node: ast.FunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_ClassDef(self, node: ast.ClassDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+
+class _NeutralizeVersionLiterals(ast.NodeTransformer):
+    def visit_Assign(self, node: ast.Assign):  # type: ignore[override]
+        for t in node.targets:
+            if isinstance(t, ast.Name) and t.id == "openapi_classifier_version":
+                if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                    node.value = ast.Constant(value="__VERSION__")
+        return self.generic_visit(node)
+
+
+def _ast_normalized_dump(path: Path) -> str:
+    src = path.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    tree = _StripDocstrings().visit(tree)  # type: ignore[assignment]
+    tree = _NeutralizeVersionLiterals().visit(tree)  # type: ignore[assignment]
+    ast.fix_missing_locations(tree)
+    return ast.dump(tree, annotate_fields=True, include_attributes=False)
+
+
+def _load_openapi_classifier_version() -> str:
+    rr = ROOT / "src" / "code_audit" / "model" / "run_result.py"
+    src = rr.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    for node in tree.body:
+        if isinstance(node, ast.Assign):
+            for t in node.targets:
+                if isinstance(t, ast.Name) and t.id == "openapi_classifier_version":
+                    if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                        return node.value.value
+    raise RuntimeError("openapi_classifier_version not found in src/code_audit/model/run_result.py")
+
+
+def test_openapi_classifier_manifest_gate() -> None:
+    assert MANIFEST.exists(), (
+        "Missing tests/contracts/openapi_classifier_manifest.json. "
+        "Run: python scripts/refresh_openapi_classifier_manifest.py"
+    )
+    manifest = json.loads(MANIFEST.read_text(encoding="utf-8"))
+
+    expected_version = manifest.get("openapi_classifier_version")
+    actual_version = _load_openapi_classifier_version()
+
+    if expected_version != actual_version:
+        raise AssertionError(
+            "openapi_classifier_version mismatch vs manifest.\n"
+            f"  manifest: {expected_version}\n"
+            f"  code:     {actual_version}\n"
+            "Fix: run python scripts/refresh_openapi_classifier_manifest.py after bumping the version."
+        )
+
+    files = manifest.get("files") or {}
+    errors = []
+
+    for rel in TARGET_FILES:
+        p = ROOT / rel
+        assert p.exists(), f"Missing target file: {rel}"
+        dumped = _ast_normalized_dump(p)
+        h = _sha256_text(dumped)
+        want = (files.get(rel) or {}).get("ast_sha256")
+        if want != h:
+            errors.append((rel, want, h))
+
+    if errors:
+        lines = [
+            "OpenAPI classifier semantic drift detected (AST hash mismatch).",
+            "This surface is contract-guarded.",
+            "",
+            "Required actions:",
+            "  1) bump openapi_classifier_version in src/code_audit/model/run_result.py",
+            "  2) run: python scripts/refresh_openapi_classifier_manifest.py",
+            "",
+            "Mismatches:",
+        ]
+        for rel, want, got in errors:
+            lines.append(f"- {rel}")
+            lines.append(f"    manifest: {want}")
+            lines.append(f"    current:  {got}")
+        raise AssertionError("\n".join(lines))
________________________________________
Diff E — Document the refresh command (tiny README snippet)
Add to your existing contract/governance doc (pick one: docs/CONTRACT.md or docs/DEPLOYMENT_PLAN.md).
Example patch to docs/CONTRACT.md:
diff --git a/docs/CONTRACT.md b/docs/CONTRACT.md
index 1111111..2222222 100644
--- a/docs/CONTRACT.md
+++ b/docs/CONTRACT.md
@@ -1,6 +1,18 @@
 ## Contract gates
 
 ...
+
+### OpenAPI classifier manifest gate
+
+Scope: OpenAPI semantic diff engine (normalization + diff + schema comparator).
+
+Guarded files:
+- src/code_audit/web_api/openapi_normalize.py
+- src/code_audit/web_api/openapi_diff.py
+- src/code_audit/web_api/schema_semver.py
+
+On semantic change:
+1) bump `openapi_classifier_version` in `src/code_audit/model/run_result.py`
+2) refresh manifest: `python scripts/refresh_openapi_classifier_manifest.py`
________________________________________
Diff F — Add manifest refresh to your “refresh scripts” toolbox (optional)
If you maintain a Makefile target or dev script aggregator, add:
python scripts/refresh_openapi_classifier_manifest.py
Not required, but helpful.
________________________________________
How to use (developer flow)
When you modify the OpenAPI classifier semantics:
1.	bump openapi_classifier_version
2.	run:
python scripts/refresh_openapi_classifier_manifest.py
3.	commit:
o	src/code_audit/model/run_result.py
o	tests/contracts/openapi_classifier_manifest.json
CI will hard-fail if you forget any step.
________________________________________
Next ultra-tightening (if you want “max”)
next diff: dependency-closure hashing for OpenAPI classifier
Same pattern you discussed for confidence: recursively hash internal imports for these modules, AST-normalized, so indirect semantic drift can’t slip through via helper module edits.
Ship next diff: dependency-closure hashing for OpenAPI classifier
next diff: dependency-closure hashing for OpenAPI classifier (AST-normalized, recursive imports)
This upgrades the OpenAPI classifier manifest gate from “hash these 3 entry files” to “hash the entire internal dependency closure those entry files import,” recursively, with AST-normalization.
What it prevents: someone changes a helper module that the classifier imports (directly or indirectly) and slips semantic drift past the gate.
Design goals:
•	Deterministic
•	Only hashes internal project modules (within src/)
•	AST-normalized (docstrings stripped, version literals neutralized)
•	Closure includes:
o	the entrypoint files
o	every internal .py they import (recursively)
•	Manifest records both:
o	the closure file list (sorted)
o	per-file hash
o	a closure graph hash (optional next step; not in this diff)
________________________________________
Diff A — Expand the manifest format (closure aware)
Patch: tests/contracts/openapi_classifier_manifest.json
We change schema shape slightly:
•	entrypoints: list of entry file paths
•	closure_files: sorted list of all hashed files
•	files: mapping for every file in closure
diff --git a/tests/contracts/openapi_classifier_manifest.json b/tests/contracts/openapi_classifier_manifest.json
index 1111111..2222222 100644
--- a/tests/contracts/openapi_classifier_manifest.json
+++ b/tests/contracts/openapi_classifier_manifest.json
@@ -1,24 +1,34 @@
 {
   "version": 1,
   "openapi_classifier_version": "openapi_classifier_v1",
-  "files": {
-    "src/code_audit/web_api/openapi_normalize.py": {
-      "ast_sha256": "REPLACE_ME",
-      "ast_sha256_short": "REPLACE_ME"
-    },
-    "src/code_audit/web_api/openapi_diff.py": {
-      "ast_sha256": "REPLACE_ME",
-      "ast_sha256_short": "REPLACE_ME"
-    },
-    "src/code_audit/web_api/schema_semver.py": {
-      "ast_sha256": "REPLACE_ME",
-      "ast_sha256_short": "REPLACE_ME"
-    }
-  }
+  "entrypoints": [
+    "src/code_audit/web_api/openapi_normalize.py",
+    "src/code_audit/web_api/openapi_diff.py",
+    "src/code_audit/web_api/schema_semver.py"
+  ],
+  "closure_files": [
+    "REPLACE_ME"
+  ],
+  "files": {
+    "REPLACE_ME": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    }
+  }
 }
The refresh script will rewrite closure_files and files deterministically.
________________________________________
Diff B — Add closure resolver + AST hashing in refresh script
Patch: scripts/refresh_openapi_classifier_manifest.py
This replaces the fixed TARGET_FILES hashing with:
1.	discover closure (internal imports only)
2.	hash every file in closure
Key decisions:
•	Only resolve imports that map to files under src/
•	Ignore:
o	stdlib imports
o	third-party imports
o	relative imports that escape package
•	Support:
o	import x.y
o	from x import y
o	from . import foo
o	from ..bar import baz
diff --git a/scripts/refresh_openapi_classifier_manifest.py b/scripts/refresh_openapi_classifier_manifest.py
index 1111111..2222222 100644
--- a/scripts/refresh_openapi_classifier_manifest.py
+++ b/scripts/refresh_openapi_classifier_manifest.py
@@ -1,17 +1,19 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
-from typing import Dict, Any
+from typing import Dict, Any, Iterable, List, Optional, Set, Tuple
 
 
 ROOT = Path(__file__).resolve().parents[1]
 MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 
-TARGET_FILES = [
+ENTRYPOINT_FILES = [
     "src/code_audit/web_api/openapi_normalize.py",
     "src/code_audit/web_api/openapi_diff.py",
     "src/code_audit/web_api/schema_semver.py",
 ]
+
+SRC_ROOT = ROOT / "src"
 
@@
 def _sha256_text(s: str) -> str:
     return hashlib.sha256(s.encode("utf-8")).hexdigest()
 
@@
 def _ast_normalized_source(path: Path) -> str:
@@
     dumped = ast.dump(tree, annotate_fields=True, include_attributes=False)
     return dumped
 
@@
 def _load_openapi_classifier_version() -> str:
@@
     raise RuntimeError("openapi_classifier_version not found in src/code_audit/model/run_result.py")
 
+
+def _module_to_candidate_paths(module: str) -> List[Path]:
+    """
+    Convert a dotted module name to possible file paths under SRC_ROOT:
+      - pkg/mod.py
+      - pkg/mod/__init__.py
+    """
+    parts = module.split(".")
+    p = SRC_ROOT.joinpath(*parts)
+    return [p.with_suffix(".py"), p / "__init__.py"]
+
+
+def _resolve_relative(module: Optional[str], *, level: int, current_file: Path) -> Optional[str]:
+    """
+    Resolve relative import to an absolute dotted module, if possible.
+    current_file is an absolute path under SRC_ROOT.
+    """
+    try:
+        rel = current_file.relative_to(SRC_ROOT)
+    except ValueError:
+        return None
+
+    # current package path: directory containing file (or package if __init__.py)
+    pkg_parts = list(rel.parts)
+    if pkg_parts[-1] == "__init__.py":
+        pkg_parts = pkg_parts[:-1]
+    else:
+        pkg_parts = pkg_parts[:-1]
+
+    if level <= 0:
+        base_parts = pkg_parts
+    else:
+        if level > len(pkg_parts):
+            return None
+        base_parts = pkg_parts[: len(pkg_parts) - level]
+
+    if module:
+        base_parts = base_parts + module.split(".")
+    return ".".join(base_parts)
+
+
+def _collect_import_modules(tree: ast.AST, *, current_file: Path) -> Set[str]:
+    out: Set[str] = set()
+    for node in ast.walk(tree):
+        if isinstance(node, ast.Import):
+            for alias in node.names:
+                if isinstance(alias.name, str) and alias.name:
+                    out.add(alias.name)
+        elif isinstance(node, ast.ImportFrom):
+            # node.module can be None for "from . import x"
+            mod = node.module if isinstance(node.module, str) else None
+            if node.level and node.level > 0:
+                resolved = _resolve_relative(mod, level=node.level, current_file=current_file)
+                if resolved:
+                    out.add(resolved)
+            elif mod:
+                out.add(mod)
+    return out
+
+
+def _resolve_internal_imports(current_file: Path) -> Set[Path]:
+    """
+    Return internal module files under SRC_ROOT imported by current_file.
+    """
+    src = current_file.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    mods = _collect_import_modules(tree, current_file=current_file)
+    resolved: Set[Path] = set()
+    for m in mods:
+        for cand in _module_to_candidate_paths(m):
+            if cand.exists():
+                try:
+                    cand.relative_to(SRC_ROOT)
+                except ValueError:
+                    continue
+                resolved.add(cand)
+                break
+    return resolved
+
+
+def _compute_internal_closure(entry_files: List[Path]) -> List[Path]:
+    """
+    BFS over internal imports to compute closure.
+    Only includes files within SRC_ROOT.
+    Deterministic ordering returned (sorted by relative path).
+    """
+    seen: Set[Path] = set()
+    q: List[Path] = []
+
+    for f in entry_files:
+        if not f.exists():
+            raise RuntimeError(f"Missing entrypoint file: {f}")
+        seen.add(f)
+        q.append(f)
+
+    i = 0
+    while i < len(q):
+        cur = q[i]
+        i += 1
+        for dep in sorted(_resolve_internal_imports(cur), key=lambda p: str(p)):
+            if dep not in seen:
+                seen.add(dep)
+                q.append(dep)
+
+    # Deterministic sort by SRC_ROOT-relative path
+    return sorted(seen, key=lambda p: str(p.relative_to(SRC_ROOT)))
+
 
 def main() -> int:
-    out: Dict[str, Any] = {
-        "version": 1,
-        "openapi_classifier_version": _load_openapi_classifier_version(),
-        "files": {},
-    }
+    entry_abs = [ROOT / rel for rel in ENTRYPOINT_FILES]
+    closure = _compute_internal_closure(entry_abs)
+
+    out: Dict[str, Any] = {
+        "version": 1,
+        "openapi_classifier_version": _load_openapi_classifier_version(),
+        "entrypoints": ENTRYPOINT_FILES,
+        "closure_files": [str(p.relative_to(ROOT)) for p in closure],
+        "files": {},
+    }
 
-    for rel in TARGET_FILES:
-        p = ROOT / rel
-        if not p.exists():
-            raise RuntimeError(f"Missing target file: {rel}")
-        dumped = _ast_normalized_source(p)
+    for p in closure:
+        rel = str(p.relative_to(ROOT))
+        dumped = _ast_normalized_source(p)
         h = _sha256_text(dumped)
-        out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
+        out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
 
     MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)
     MANIFEST_PATH.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
     print(f"Wrote {MANIFEST_PATH.relative_to(ROOT)}")
     return 0
Important note: this computes closure by importing module names to paths by existence under src/. That means it will include:
•	src/code_audit/web_api/*.py
•	plus any internal modules those import (src/code_audit/utils/..., etc.)
It will not include:
•	stdlib (json, re, etc.)
•	third-party (pydantic, fastapi) unless vendored under src/
________________________________________
Diff C — Update the manifest gate test to validate closure + hashes
Patch: tests/test_openapi_classifier_manifest_gate.py
Changes:
•	reads entrypoints + closure_files from manifest
•	recomputes closure deterministically
•	asserts:
o	closure list matches exactly
o	every closure file has a hash entry
o	every hash matches AST-normalized content
•	still requires openapi_classifier_version match
diff --git a/tests/test_openapi_classifier_manifest_gate.py b/tests/test_openapi_classifier_manifest_gate.py
index 1111111..2222222 100644
--- a/tests/test_openapi_classifier_manifest_gate.py
+++ b/tests/test_openapi_classifier_manifest_gate.py
@@ -1,15 +1,16 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
-from typing import Any, Dict
+from typing import Any, Dict, List, Optional, Set
 
 
 ROOT = Path(__file__).resolve().parents[1]
 MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 
-TARGET_FILES = [
-    "src/code_audit/web_api/openapi_normalize.py",
-    "src/code_audit/web_api/openapi_diff.py",
-    "src/code_audit/web_api/schema_semver.py",
-]
+SRC_ROOT = ROOT / "src"
 
@@
 def _ast_normalized_dump(path: Path) -> str:
@@
     return ast.dump(tree, annotate_fields=True, include_attributes=False)
 
@@
 def _load_openapi_classifier_version() -> str:
@@
     raise RuntimeError("openapi_classifier_version not found in src/code_audit/model/run_result.py")
 
+
+def _module_to_candidate_paths(module: str) -> List[Path]:
+    parts = module.split(".")
+    p = SRC_ROOT.joinpath(*parts)
+    return [p.with_suffix(".py"), p / "__init__.py"]
+
+
+def _resolve_relative(module: Optional[str], *, level: int, current_file: Path) -> Optional[str]:
+    try:
+        rel = current_file.relative_to(SRC_ROOT)
+    except ValueError:
+        return None
+    pkg_parts = list(rel.parts)
+    if pkg_parts[-1] == "__init__.py":
+        pkg_parts = pkg_parts[:-1]
+    else:
+        pkg_parts = pkg_parts[:-1]
+    if level > len(pkg_parts):
+        return None
+    base_parts = pkg_parts[: len(pkg_parts) - level] if level > 0 else pkg_parts
+    if module:
+        base_parts = base_parts + module.split(".")
+    return ".".join(base_parts)
+
+
+def _collect_import_modules(tree: ast.AST, *, current_file: Path) -> Set[str]:
+    out: Set[str] = set()
+    for node in ast.walk(tree):
+        if isinstance(node, ast.Import):
+            for alias in node.names:
+                if isinstance(alias.name, str) and alias.name:
+                    out.add(alias.name)
+        elif isinstance(node, ast.ImportFrom):
+            mod = node.module if isinstance(node.module, str) else None
+            if node.level and node.level > 0:
+                resolved = _resolve_relative(mod, level=node.level, current_file=current_file)
+                if resolved:
+                    out.add(resolved)
+            elif mod:
+                out.add(mod)
+    return out
+
+
+def _resolve_internal_imports(current_file: Path) -> Set[Path]:
+    src = current_file.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    mods = _collect_import_modules(tree, current_file=current_file)
+    resolved: Set[Path] = set()
+    for m in mods:
+        for cand in _module_to_candidate_paths(m):
+            if cand.exists():
+                try:
+                    cand.relative_to(SRC_ROOT)
+                except ValueError:
+                    continue
+                resolved.add(cand)
+                break
+    return resolved
+
+
+def _compute_internal_closure(entry_files: List[Path]) -> List[Path]:
+    seen: Set[Path] = set()
+    q: List[Path] = []
+    for f in entry_files:
+        assert f.exists(), f"Missing entrypoint file: {f}"
+        seen.add(f)
+        q.append(f)
+    i = 0
+    while i < len(q):
+        cur = q[i]
+        i += 1
+        for dep in sorted(_resolve_internal_imports(cur), key=lambda p: str(p)):
+            if dep not in seen:
+                seen.add(dep)
+                q.append(dep)
+    return sorted(seen, key=lambda p: str(p.relative_to(SRC_ROOT)))
+
 
 def test_openapi_classifier_manifest_gate() -> None:
     assert MANIFEST.exists(), (
         "Missing tests/contracts/openapi_classifier_manifest.json. "
         "Run: python scripts/refresh_openapi_classifier_manifest.py"
     )
     manifest = json.loads(MANIFEST.read_text(encoding="utf-8"))
 
     expected_version = manifest.get("openapi_classifier_version")
     actual_version = _load_openapi_classifier_version()
@@
-    files = manifest.get("files") or {}
-    errors = []
+    entrypoints = manifest.get("entrypoints") or []
+    assert isinstance(entrypoints, list) and entrypoints, "Manifest missing entrypoints"
+    entry_abs = [ROOT / rel for rel in entrypoints]
+
+    expected_closure = manifest.get("closure_files") or []
+    assert isinstance(expected_closure, list) and expected_closure, "Manifest missing closure_files"
+
+    actual_closure = _compute_internal_closure(entry_abs)
+    actual_closure_rel = [str(p.relative_to(ROOT)) for p in actual_closure]
+
+    if expected_closure != actual_closure_rel:
+        raise AssertionError(
+            "OpenAPI classifier manifest closure mismatch.\n"
+            "Fix: bump openapi_classifier_version and run python scripts/refresh_openapi_classifier_manifest.py\n"
+            f"Expected (manifest) count={len(expected_closure)}\n"
+            f"Actual (computed) count={len(actual_closure_rel)}\n"
+        )
+
+    files = manifest.get("files") or {}
+    errors = []
 
-    for rel in TARGET_FILES:
-        p = ROOT / rel
-        assert p.exists(), f"Missing target file: {rel}"
+    # Ensure every closure file has an entry and matches AST hash.
+    for rel in actual_closure_rel:
+        p = ROOT / rel
+        assert p.exists(), f"Missing closure file: {rel}"
         dumped = _ast_normalized_dump(p)
         h = _sha256_text(dumped)
         want = (files.get(rel) or {}).get("ast_sha256")
         if want != h:
             errors.append((rel, want, h))
 
     if errors:
         lines = [
             "OpenAPI classifier semantic drift detected (AST hash mismatch).",
             "This surface is contract-guarded.",
             "",
             "Required actions:",
             "  1) bump openapi_classifier_version in src/code_audit/model/run_result.py",
             "  2) run: python scripts/refresh_openapi_classifier_manifest.py",
             "",
             "Mismatches:",
         ]
This makes the gate robust against:
•	import path changes that alter closure
•	new internal helper modules used by classifier
•	edits to existing helper modules
________________________________________
Diff D — Update docs to explain closure hashing
Patch: docs/CONTRACT.md (or whichever doc you’ve been using)
diff --git a/docs/CONTRACT.md b/docs/CONTRACT.md
index 2222222..3333333 100644
--- a/docs/CONTRACT.md
+++ b/docs/CONTRACT.md
@@
 ### OpenAPI classifier manifest gate
 
 Scope: OpenAPI semantic diff engine (normalization + diff + schema comparator).
 
 Guarded files:
-- src/code_audit/web_api/openapi_normalize.py
-- src/code_audit/web_api/openapi_diff.py
-- src/code_audit/web_api/schema_semver.py
+- Entrypoints:
+  - src/code_audit/web_api/openapi_normalize.py
+  - src/code_audit/web_api/openapi_diff.py
+  - src/code_audit/web_api/schema_semver.py
+- Plus the full internal dependency closure those files import (recursively), under src/
 
 On semantic change:
 1) bump `openapi_classifier_version` in `src/code_audit/model/run_result.py`
 2) refresh manifest: `python scripts/refresh_openapi_classifier_manifest.py`
________________________________________
Result: what’s locked now
After this diff, any change to:
•	the diff engine
•	the schema semver comparator
•	normalization rules
•	any internal helper module they import
…requires:
•	a bump to openapi_classifier_version
•	a manifest refresh
This is very close to “max governance” for the OpenAPI classifier surface.
________________________________________
Next escalation (rarely needed, but the last gap)
next diff: hash the closure graph edges (import relationships), not just the file set
So if the set of imported files is the same but the import wiring changes, that change is explicitly tracked too.
Ship next diff: hash the closure graph edges (import relationships), not just the file set
next diff: hash the closure graph edges (import relationships), not just the file set
This is the “last mile” tightening for the OpenAPI classifier closure gate:
•	Today (closure hashing) you lock the set of files and their AST hashes.
•	This diff also locks the import graph structure (edges), so a refactor that keeps the same files but rewires dependencies (e.g., moving logic from schema_semver into a helper and importing it differently) is explicitly tracked.
What’s added to the manifest
•	closure_graph:
o	nodes: same as closure_files (sorted)
o	edges: sorted list of directed edges: {"from": "<relpath>", "to": "<relpath>"} where both endpoints are internal files
o	edges_sha256 + _short: hash of a canonical string representation of edges
o	edge_count
What CI enforces
•	manifest closure_files must exactly match computed closure (already enforced)
•	manifest edges must exactly match computed edges
•	edges_sha256 must match (gives a fast summary + BOM-ready fingerprint)
•	any mismatch requires:
1.	bump openapi_classifier_version
2.	refresh manifest
________________________________________
Diff A — Extend manifest format to include graph edges + edge hash
Patch: tests/contracts/openapi_classifier_manifest.json
diff --git a/tests/contracts/openapi_classifier_manifest.json b/tests/contracts/openapi_classifier_manifest.json
index 2222222..3333333 100644
--- a/tests/contracts/openapi_classifier_manifest.json
+++ b/tests/contracts/openapi_classifier_manifest.json
@@ -1,16 +1,33 @@
 {
   "version": 1,
   "openapi_classifier_version": "openapi_classifier_v1",
   "entrypoints": [
     "src/code_audit/web_api/openapi_normalize.py",
     "src/code_audit/web_api/openapi_diff.py",
     "src/code_audit/web_api/schema_semver.py"
   ],
   "closure_files": [
     "REPLACE_ME"
   ],
+  "closure_graph": {
+    "nodes": [
+      "REPLACE_ME"
+    ],
+    "edges": [
+      {
+        "from": "REPLACE_ME",
+        "to": "REPLACE_ME"
+      }
+    ],
+    "edge_count": 0,
+    "edges_sha256": "REPLACE_ME",
+    "edges_sha256_short": "REPLACE_ME"
+  },
   "files": {
     "REPLACE_ME": {
       "ast_sha256": "REPLACE_ME",
       "ast_sha256_short": "REPLACE_ME"
     }
   }
 }
Notes:
•	nodes is redundant with closure_files, but intentional: it makes the graph section self-contained for consumers/BOM tooling.
•	We keep the closure_files top-level list because it’s already in use and is convenient for the gate.
________________________________________
Diff B — Compute edges + edges hash in the refresh script
Patch: scripts/refresh_openapi_classifier_manifest.py
We add:
•	graph_edges: compute internal import edges for every node in the closure
•	canonicalize edges to strings: "<from_rel> -> <to_rel>"
•	hash the joined string with \n separators
diff --git a/scripts/refresh_openapi_classifier_manifest.py b/scripts/refresh_openapi_classifier_manifest.py
index 2222222..3333333 100644
--- a/scripts/refresh_openapi_classifier_manifest.py
+++ b/scripts/refresh_openapi_classifier_manifest.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
-from typing import Dict, Any, Iterable, List, Optional, Set, Tuple
+from typing import Dict, Any, Iterable, List, Optional, Set, Tuple
 
@@
 def _sha256_text(s: str) -> str:
     return hashlib.sha256(s.encode("utf-8")).hexdigest()
 
@@
 def _resolve_internal_imports(current_file: Path) -> Set[Path]:
@@
     return resolved
 
 
 def _compute_internal_closure(entry_files: List[Path]) -> List[Path]:
@@
     return sorted(seen, key=lambda p: str(p.relative_to(SRC_ROOT)))
+
+
+def _compute_internal_edges(nodes: List[Path]) -> List[Dict[str, str]]:
+    """
+    Compute directed import edges among internal nodes.
+    Edge exists from A -> B if A imports B (directly), and B is inside the SRC_ROOT.
+    Only include edges where both endpoints are in the closure node set.
+    Returned deterministically sorted by (from, to).
+    """
+    node_set = set(nodes)
+    edges: List[Dict[str, str]] = []
+    for src in nodes:
+        deps = _resolve_internal_imports(src)
+        for dep in deps:
+            if dep in node_set:
+                edges.append(
+                    {
+                        "from": str(src.relative_to(ROOT)),
+                        "to": str(dep.relative_to(ROOT)),
+                    }
+                )
+    edges.sort(key=lambda e: (e["from"], e["to"]))
+    return edges
+
+
+def _hash_edges(edges: List[Dict[str, str]]) -> str:
+    """
+    Canonical hash of edge list, independent of JSON formatting:
+      "<from> -> <to>\n..."
+    """
+    lines = [f'{e["from"]} -> {e["to"]}' for e in edges]
+    return _sha256_text("\n".join(lines) + ("\n" if lines else ""))
 
@@
 def main() -> int:
     entry_abs = [ROOT / rel for rel in ENTRYPOINT_FILES]
     closure = _compute_internal_closure(entry_abs)
+    edges = _compute_internal_edges(closure)
+    edges_sha = _hash_edges(edges)
 
     out: Dict[str, Any] = {
         "version": 1,
         "openapi_classifier_version": _load_openapi_classifier_version(),
         "entrypoints": ENTRYPOINT_FILES,
         "closure_files": [str(p.relative_to(ROOT)) for p in closure],
+        "closure_graph": {
+            "nodes": [str(p.relative_to(ROOT)) for p in closure],
+            "edges": edges,
+            "edge_count": len(edges),
+            "edges_sha256": edges_sha,
+            "edges_sha256_short": edges_sha[:12],
+        },
         "files": {},
     }
 
     for p in closure:
         rel = str(p.relative_to(ROOT))
         dumped = _ast_normalized_source(p)
         h = _sha256_text(dumped)
         out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
Why the extra newline in _hash_edges?
•	It avoids ambiguity where ["a -> b"] and ["a -> b", ""] could collide in some edge cases. Keeping the canonical form “lines terminated with newline” is a common stable hashing trick.
________________________________________
Diff C — Enforce edge graph equality in the manifest gate test
Patch: tests/test_openapi_classifier_manifest_gate.py
We add:
•	recompute edges from computed closure nodes
•	assert closure_graph.nodes == closure_files and equals computed closure
•	assert edges list equals computed edges list
•	assert edges_sha256 equals computed hash
diff --git a/tests/test_openapi_classifier_manifest_gate.py b/tests/test_openapi_classifier_manifest_gate.py
index 2222222..3333333 100644
--- a/tests/test_openapi_classifier_manifest_gate.py
+++ b/tests/test_openapi_classifier_manifest_gate.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Set
+from typing import Any, Dict, List, Optional, Set
 
@@
 def _sha256_text(s: str) -> str:
     return hashlib.sha256(s.encode("utf-8")).hexdigest()
 
@@
 def _compute_internal_closure(entry_files: List[Path]) -> List[Path]:
@@
     return sorted(seen, key=lambda p: str(p.relative_to(SRC_ROOT)))
+
+
+def _compute_internal_edges(nodes: List[Path]) -> List[Dict[str, str]]:
+    node_set = set(nodes)
+    edges: List[Dict[str, str]] = []
+    for src in nodes:
+        deps = _resolve_internal_imports(src)
+        for dep in deps:
+            if dep in node_set:
+                edges.append(
+                    {
+                        "from": str(src.relative_to(ROOT)),
+                        "to": str(dep.relative_to(ROOT)),
+                    }
+                )
+    edges.sort(key=lambda e: (e["from"], e["to"]))
+    return edges
+
+
+def _hash_edges(edges: List[Dict[str, str]]) -> str:
+    lines = [f'{e["from"]} -> {e["to"]}' for e in edges]
+    return _sha256_text("\n".join(lines) + ("\n" if lines else ""))
 
@@
 def test_openapi_classifier_manifest_gate() -> None:
@@
     expected_closure = manifest.get("closure_files") or []
     assert isinstance(expected_closure, list) and expected_closure, "Manifest missing closure_files"
@@
     if expected_closure != actual_closure_rel:
         raise AssertionError(
             "OpenAPI classifier manifest closure mismatch.\n"
             "Fix: bump openapi_classifier_version and run python scripts/refresh_openapi_classifier_manifest.py\n"
             f"Expected (manifest) count={len(expected_closure)}\n"
             f"Actual (computed) count={len(actual_closure_rel)}\n"
         )
+
+    # Graph checks (edges)
+    cg = manifest.get("closure_graph") or {}
+    nodes = cg.get("nodes") or []
+    edges = cg.get("edges") or []
+    edges_sha = cg.get("edges_sha256")
+
+    assert nodes == expected_closure, (
+        "OpenAPI classifier manifest graph nodes mismatch (must equal closure_files).\n"
+        "Fix: bump openapi_classifier_version and refresh the manifest."
+    )
+    assert isinstance(edges, list), "closure_graph.edges must be a list"
+    computed_edges = _compute_internal_edges(actual_closure)
+
+    if edges != computed_edges:
+        raise AssertionError(
+            "OpenAPI classifier manifest edge graph mismatch.\n"
+            "Fix: bump openapi_classifier_version and run python scripts/refresh_openapi_classifier_manifest.py"
+        )
+
+    computed_edges_sha = _hash_edges(computed_edges)
+    if edges_sha != computed_edges_sha:
+        raise AssertionError(
+            "OpenAPI classifier manifest edges_sha256 mismatch.\n"
+            "Fix: bump openapi_classifier_version and refresh the manifest."
+        )
 
     files = manifest.get("files") or {}
     errors = []
This enforces:
•	closure nodes are exactly what the manifest declares
•	import wiring between internal nodes is exactly what the manifest declares
•	the hash is correct (fast check + BOM friendliness)
________________________________________
How this changes your workflow
When you change classifier code, you now might trigger three kinds of changes:
1.	AST hash changes (file content semantics)
2.	closure file set changes (new/deleted internal modules, or different imports)
3.	edge graph changes (rewires/refactors that keep the file set but change relationships)
All of them require:
•	bump openapi_classifier_version
•	refresh manifest
________________________________________
Next diff (final “max provenance” option)
next diff: include edges_sha256 + file hashes into the Release BOM artifact
So every release can be proven to have used:
•	policy hash
•	before/after OpenAPI hash
•	classifier closure file hashes
•	closure edge hash
Ship next diff: include edges_sha256 + file hashes into the Release BOM artifact
Thought for 6s
next diff: include edges_sha256 + file hashes into the Release BOM artifact
This makes the Release BOM fully self-describing for the OpenAPI classifier surface by embedding:
•	openapi_classifier_manifest hash (the on-disk contract manifest)
•	closure_graph.edges_sha256 (import wiring fingerprint)
•	per-file AST hashes for the entire classifier closure (from the manifest files map)
Outcome
A tagged release will ship a dist/release_bom.json that proves:
•	which OpenAPI snapshots were compared (already in openapi_diff_report.json)
•	which policy allowed unknowns/breakings (already in report + policy asset)
•	which classifier engine semantics were in force:
o	closure file list + per-file AST hashes
o	import edge graph hash
________________________________________
Diff A — New BOM generator script
New file: scripts/generate_release_bom.py
This script writes dist/release_bom.json on tag builds. It consumes:
•	dist/openapi_diff_report.json (already generated by the release gate)
•	tests/contracts/openapi_classifier_manifest.json (contract manifest)
…and emits a single BOM payload suitable for attaching to Releases.
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/generate_release_bom.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+import hashlib
+import json
+import os
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+DIST = ROOT / "dist"
+
+OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
+OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+
+OUT_BOM = DIST / "release_bom.json"
+
+
+def _sha256_bytes(data: bytes) -> str:
+    return hashlib.sha256(data).hexdigest()
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(1024 * 64), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def _load_json(path: Path) -> Dict[str, Any]:
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _require(path: Path, *, why: str) -> None:
+    if not path.exists():
+        raise RuntimeError(f"Missing required file: {path.relative_to(ROOT)} ({why})")
+
+
+def _select_classifier_fingerprint(manifest: Dict[str, Any]) -> Dict[str, Any]:
+    """
+    Extract stable, governance-critical identifiers from openapi_classifier_manifest.json:
+      - openapi_classifier_version
+      - closure_files
+      - per-file ast_sha256
+      - closure_graph.edges_sha256
+    """
+    version = manifest.get("openapi_classifier_version")
+    if not isinstance(version, str) or not version:
+        raise RuntimeError("openapi_classifier_manifest missing openapi_classifier_version")
+
+    closure_files = manifest.get("closure_files")
+    if not isinstance(closure_files, list) or not closure_files:
+        raise RuntimeError("openapi_classifier_manifest missing closure_files")
+    for f in closure_files:
+        if not isinstance(f, str) or not f:
+            raise RuntimeError("openapi_classifier_manifest closure_files contains invalid entry")
+
+    files = manifest.get("files")
+    if not isinstance(files, dict) or not files:
+        raise RuntimeError("openapi_classifier_manifest missing files map")
+
+    # Enforce that every closure file has a hash entry
+    missing = [f for f in closure_files if f not in files]
+    if missing:
+        raise RuntimeError(
+            "openapi_classifier_manifest incomplete: missing file hash entries for: "
+            + ", ".join(missing[:20])
+            + (" ..." if len(missing) > 20 else "")
+        )
+
+    file_hashes: Dict[str, str] = {}
+    for f in closure_files:
+        ent = files.get(f) or {}
+        h = ent.get("ast_sha256")
+        if not isinstance(h, str) or len(h) < 32:
+            raise RuntimeError(f"openapi_classifier_manifest invalid ast_sha256 for {f}")
+        file_hashes[f] = h
+
+    cg = manifest.get("closure_graph") or {}
+    edges_sha = cg.get("edges_sha256")
+    if not isinstance(edges_sha, str) or len(edges_sha) < 32:
+        raise RuntimeError("openapi_classifier_manifest missing/invalid closure_graph.edges_sha256")
+
+    return {
+        "openapi_classifier_version": version,
+        "closure_file_count": len(closure_files),
+        "closure_files": closure_files,  # already sorted by refresh script + gate
+        "closure_graph_edges_sha256": edges_sha,
+        "closure_graph_edges_sha256_short": edges_sha[:12],
+        "files_ast_sha256": file_hashes,  # stable mapping for BOM
+    }
+
+
+def main() -> int:
+    DIST.mkdir(parents=True, exist_ok=True)
+
+    _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
+    _require(OPENAPI_CLASSIFIER_MANIFEST, why="Classifier manifest is part of the release contract surface")
+
+    diff_report = _load_json(OPENAPI_DIFF_REPORT)
+
+    classifier_manifest_bytes = OPENAPI_CLASSIFIER_MANIFEST.read_bytes()
+    classifier_manifest = json.loads(classifier_manifest_bytes.decode("utf-8"))
+
+    bom: Dict[str, Any] = {
+        "version": 1,
+        "release_tag": (os.environ.get("GITHUB_REF_NAME") or "").strip() or None,
+        "artifacts": {
+            "openapi_diff_report": {
+                "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
+                "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
+            },
+            "openapi_classifier_manifest": {
+                "path": str(OPENAPI_CLASSIFIER_MANIFEST.relative_to(ROOT)),
+                "sha256": _sha256_bytes(classifier_manifest_bytes),
+                "sha256_short": _sha256_bytes(classifier_manifest_bytes)[:12],
+                "selected": _select_classifier_fingerprint(classifier_manifest),
+            },
+        },
+        # Copy through the key provenance values from the diff report for convenience.
+        # (These are already part of openapi_diff_report.json; BOM just surfaces them.)
+        "provenance": {
+            "baseline_tag": diff_report.get("baseline_tag"),
+            "current_tag": diff_report.get("current_tag"),
+            "policy_path": diff_report.get("policy_path"),
+            "policy_sha256": diff_report.get("policy_sha256"),
+            "before_openapi_path": diff_report.get("before_openapi_path"),
+            "before_openapi_sha256": diff_report.get("before_openapi_sha256"),
+            "after_openapi_path": diff_report.get("after_openapi_path"),
+            "after_openapi_sha256": diff_report.get("after_openapi_sha256"),
+        },
+    }
+
+    OUT_BOM.write_text(json.dumps(bom, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+    print(f"Wrote {OUT_BOM.relative_to(ROOT)}")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
Why include both “selected fingerprint” and “full manifest sha”?
•	openapi_classifier_manifest.sha256 is the canonical “what file did we ship?”
•	The “selected” section makes the BOM consumable without parsing the manifest:
o	closure_graph_edges_sha256 (wiring)
o	files_ast_sha256 (semantic hashes)
o	closure_files (exact closure membership)
________________________________________
Diff B — Add a minimal BOM validation test (optional but recommended)
This ensures the BOM generator doesn’t drift silently and that required keys exist.
New file: tests/test_release_bom_generator_smoke.py
diff --git a/tests/test_release_bom_generator_smoke.py b/tests/test_release_bom_generator_smoke.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_release_bom_generator_smoke.py
@@ -0,0 +1,63 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+import subprocess
+
+
+ROOT = Path(__file__).resolve().parents[1]
+BOM = ROOT / "dist" / "release_bom.json"
+
+
+def test_generate_release_bom_smoke(tmp_path) -> None:
+    # This is a smoke test: ensures the script runs when prerequisites exist.
+    # In CI/tag workflows, prerequisites are created before calling the script.
+    #
+    # We only assert that the script errors cleanly if prerequisites are missing,
+    # and that it produces the BOM if they exist.
+    #
+    # (We do not attempt to synthesize openapi_diff_report.json here.)
+    p = subprocess.run(
+        ["python", "scripts/generate_release_bom.py"],
+        cwd=str(ROOT),
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+    )
+    # Either it succeeded (if dist/openapi_diff_report.json exists in this environment),
+    # or it failed with a clear message about prerequisites.
+    if p.returncode == 0:
+        assert BOM.exists()
+        data = json.loads(BOM.read_text(encoding="utf-8"))
+        assert data["version"] == 1
+        assert "artifacts" in data
+        assert "openapi_classifier_manifest" in data["artifacts"]
+    else:
+        assert "Missing required file" in (p.stderr + p.stdout)
If you prefer strictly “unit-test only” suites without invoking scripts, skip this. The workflow gate is the real enforcement.
________________________________________
Diff C — Wire BOM generation + upload into Release workflow
Patch: .github/workflows/release.yml
Add:
•	run generate_release_bom.py after the OpenAPI release gate step (so diff report exists)
•	upload dist/release_bom.json as a Release asset (along with openapi_diff_report.json and policy file)
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 3333333..4444444 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -1,6 +1,6 @@
 name: release
 
 on:
   push:
     tags:
       - "v*"
 
 jobs:
   release:
     runs-on: ubuntu-latest
     steps:
       - name: Checkout (full history + tags)
         uses: actions/checkout@v4
         with:
           fetch-depth: 0
 
       - name: Set up Python
         uses: actions/setup-python@v5
         with:
           python-version: "3.13"
 
       - name: Install package (editable)
         run: |
           python -m pip install --upgrade pip
           python -m pip install -e .
 
       - name: OpenAPI release gate (produces dist/openapi_diff_report.json)
         env:
           CI: "true"
         run: |
           python scripts/check_openapi_release_gate.py
+
+      - name: Generate release BOM (includes classifier edges + file hashes)
+        env:
+          CI: "true"
+        run: |
+          python scripts/generate_release_bom.py
+
+      - name: Verify release BOM exists
+        run: |
+          test -f dist/release_bom.json
+          echo "Found dist/release_bom.json"
 
       - name: Build distribution
         run: |
           python -m pip install build
           python -m build
 
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
             tests/contracts/openapi_breaking_policy.json
+            dist/release_bom.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
If dist/* already includes release_bom.json, the explicit line is redundant. Keeping it makes the contract intent obvious.
________________________________________
What releases contain after this diff
Every tag release will include:
•	dist/openapi_diff_report.json
•	tests/contracts/openapi_breaking_policy.json
•	dist/release_bom.json
•	wheels/sdists
And the BOM contains:
•	openapi_classifier_manifest.sha256
•	openapi_classifier_manifest.selected.closure_graph_edges_sha256
•	openapi_classifier_manifest.selected.files_ast_sha256 for the entire closure
This makes classifier semantics fully auditable and independently verifiable.
________________________________________
Next diff (final “release governance closes the loop”)
next diff: release gate asserts BOM consistency
e.g., fail tagged CI if:
•	BOM policy hash doesn’t match diff report policy hash
•	BOM classifier manifest hash doesn’t match the repo file hash
•	BOM missing required provenance fields
Ship next diff: release gate asserts BOM consistency
next diff: release gate asserts BOM consistency
This tightens release-hardening by making the tag gate validate that the generated dist/release_bom.json is:
•	present,
•	internally consistent,
•	consistent with the repo’s contract artifacts,
•	consistent with the OpenAPI diff report it accompanies.
In short: no lying in metadata and no “BOM drift”.
________________________________________
What this enforces (tag builds)
After check_openapi_release_gate.py and generate_release_bom.py run, the release workflow will fail if any of these are true:
Presence + required fields
•	dist/release_bom.json missing
•	dist/openapi_diff_report.json missing
•	tests/contracts/openapi_classifier_manifest.json missing
•	tests/contracts/openapi_breaking_policy.json missing (if you include it in BOM checks)
Hash consistency
•	BOM’s artifacts.openapi_diff_report.sha256 ≠ SHA of the file on disk
•	BOM’s artifacts.openapi_classifier_manifest.sha256 ≠ SHA of repo tests/contracts/openapi_classifier_manifest.json
•	BOM’s artifacts.openapi_classifier_manifest.selected.closure_graph_edges_sha256 ≠ the manifest’s closure_graph.edges_sha256
•	BOM’s artifacts.openapi_classifier_manifest.selected.files_ast_sha256 does not match manifest files[*].ast_sha256 for every closure file
Cross-artifact provenance coherence (BOM ↔ diff report)
•	BOM provenance.policy_sha256 ≠ diff report policy_sha256
•	BOM provenance.before_openapi_sha256 ≠ diff report before_openapi_sha256
•	BOM provenance.after_openapi_sha256 ≠ diff report after_openapi_sha256
•	BOM provenance.baseline_tag/current_tag mismatch diff report baseline/current tags
This makes the BOM a true release attestation, not a best-effort annotation.
________________________________________
Diff A — New BOM consistency checker script
New file: scripts/check_release_bom_consistency.py
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/check_release_bom_consistency.py
@@ -0,0 +1,278 @@
+from __future__ import annotations
+
+import hashlib
+import json
+import sys
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+DIST = ROOT / "dist"
+
+BOM_PATH = DIST / "release_bom.json"
+DIFF_REPORT_PATH = DIST / "openapi_diff_report.json"
+
+CLASSIFIER_MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(1024 * 64), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def _load_json(path: Path) -> Dict[str, Any]:
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _req(cond: bool, msg: str, errors: list[str]) -> None:
+    if not cond:
+        errors.append(msg)
+
+
+def main() -> int:
+    errors: list[str] = []
+
+    _req(BOM_PATH.exists(), f"Missing {BOM_PATH.relative_to(ROOT)}", errors)
+    _req(DIFF_REPORT_PATH.exists(), f"Missing {DIFF_REPORT_PATH.relative_to(ROOT)}", errors)
+    _req(CLASSIFIER_MANIFEST_PATH.exists(), f"Missing {CLASSIFIER_MANIFEST_PATH.relative_to(ROOT)}", errors)
+    _req(POLICY_PATH.exists(), f"Missing {POLICY_PATH.relative_to(ROOT)}", errors)
+
+    if errors:
+        print("[release-bom-check] FAIL: prerequisites missing:", file=sys.stderr)
+        for e in errors:
+            print(f"  - {e}", file=sys.stderr)
+        return 1
+
+    bom = _load_json(BOM_PATH)
+    diff = _load_json(DIFF_REPORT_PATH)
+    manifest = _load_json(CLASSIFIER_MANIFEST_PATH)
+
+    artifacts = bom.get("artifacts") or {}
+    _req(isinstance(artifacts, dict), "BOM: artifacts must be an object", errors)
+
+    od = artifacts.get("openapi_diff_report") or {}
+    _req(isinstance(od, dict), "BOM: artifacts.openapi_diff_report must be an object", errors)
+
+    ocm = artifacts.get("openapi_classifier_manifest") or {}
+    _req(isinstance(ocm, dict), "BOM: artifacts.openapi_classifier_manifest must be an object", errors)
+
+    # --- File hash checks (BOM -> disk) ---
+    expected_diff_sha = od.get("sha256")
+    actual_diff_sha = _sha256_file(DIFF_REPORT_PATH)
+    _req(
+        isinstance(expected_diff_sha, str) and expected_diff_sha == actual_diff_sha,
+        "BOM mismatch: openapi_diff_report.sha256 does not match dist/openapi_diff_report.json",
+        errors,
+    )
+
+    expected_manifest_sha = ocm.get("sha256")
+    actual_manifest_sha = _sha256_file(CLASSIFIER_MANIFEST_PATH)
+    _req(
+        isinstance(expected_manifest_sha, str) and expected_manifest_sha == actual_manifest_sha,
+        "BOM mismatch: openapi_classifier_manifest.sha256 does not match tests/contracts/openapi_classifier_manifest.json",
+        errors,
+    )
+
+    # --- Selected classifier fingerprint checks (BOM -> manifest) ---
+    selected = ocm.get("selected") or {}
+    _req(isinstance(selected, dict), "BOM: openapi_classifier_manifest.selected must be an object", errors)
+
+    sel_edges_sha = selected.get("closure_graph_edges_sha256")
+    man_edges_sha = (manifest.get("closure_graph") or {}).get("edges_sha256")
+    _req(
+        isinstance(sel_edges_sha, str) and isinstance(man_edges_sha, str) and sel_edges_sha == man_edges_sha,
+        "BOM mismatch: selected.closure_graph_edges_sha256 does not match manifest closure_graph.edges_sha256",
+        errors,
+    )
+
+    sel_files_ast = selected.get("files_ast_sha256")
+    _req(isinstance(sel_files_ast, dict), "BOM: selected.files_ast_sha256 must be an object", errors)
+
+    closure_files = manifest.get("closure_files") or []
+    man_files = manifest.get("files") or {}
+    _req(isinstance(closure_files, list) and closure_files, "Manifest invalid: missing closure_files", errors)
+    _req(isinstance(man_files, dict) and man_files, "Manifest invalid: missing files map", errors)
+
+    if isinstance(closure_files, list) and isinstance(man_files, dict) and isinstance(sel_files_ast, dict):
+        # Ensure BOM has hashes for every closure file and they match.
+        for rel in closure_files:
+            ent = man_files.get(rel) or {}
+            man_h = ent.get("ast_sha256")
+            bom_h = sel_files_ast.get(rel)
+            if not isinstance(man_h, str):
+                errors.append(f"Manifest invalid: files[{rel}].ast_sha256 missing")
+                continue
+            if not isinstance(bom_h, str):
+                errors.append(f"BOM mismatch: selected.files_ast_sha256 missing entry for {rel}")
+                continue
+            if bom_h != man_h:
+                errors.append(f"BOM mismatch: ast_sha256 differs for {rel}")
+
+    # --- Provenance coherence checks (BOM <-> diff report) ---
+    prov = bom.get("provenance") or {}
+    _req(isinstance(prov, dict), "BOM: provenance must be an object", errors)
+
+    def _eq(field: str) -> None:
+        bom_v = prov.get(field)
+        diff_v = diff.get(field)
+        if bom_v != diff_v:
+            errors.append(f"Provenance mismatch: {field} (bom={bom_v!r}, diff_report={diff_v!r})")
+
+    # Keep this list tight and contract-relevant.
+    for k in [
+        "baseline_tag",
+        "current_tag",
+        "policy_path",
+        "policy_sha256",
+        "before_openapi_path",
+        "before_openapi_sha256",
+        "after_openapi_path",
+        "after_openapi_sha256",
+    ]:
+        _eq(k)
+
+    if errors:
+        print("[release-bom-check] FAIL: release BOM consistency violations:", file=sys.stderr)
+        for e in errors:
+            print(f"  - {e}", file=sys.stderr)
+        print("", file=sys.stderr)
+        print("Fix:", file=sys.stderr)
+        print("  - re-run: python scripts/check_openapi_release_gate.py", file=sys.stderr)
+        print("  - re-run: python scripts/generate_release_bom.py", file=sys.stderr)
+        print("  - ensure tests/contracts/openapi_classifier_manifest.json is refreshed & committed", file=sys.stderr)
+        return 1
+
+    print("[release-bom-check] OK: release_bom.json is consistent with diff report and classifier manifest.")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
This script is deterministic, low-noise, and gives precise reasons on failure.
________________________________________
Diff B — Wire BOM consistency check into tag release workflow
Patch: .github/workflows/release.yml
Insert this after BOM generation and before release publishing.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 4444444..5555555 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Generate release BOM (includes classifier edges + file hashes)
         env:
           CI: "true"
         run: |
           python scripts/generate_release_bom.py
+
+      - name: Release BOM consistency check (hard gate)
+        env:
+          CI: "true"
+        run: |
+          python scripts/check_release_bom_consistency.py
Now the release job will hard-fail if:
•	the BOM is stale,
•	the diff report is stale,
•	the manifest is stale,
•	or anything was tampered with.
________________________________________
Optional tightening (recommended): also run in the standalone OpenAPI tag gate workflow
If you still have .github/workflows/openapi-release-gate.yml (separate from release.yml), you can add:
- name: Generate release BOM
  env:
    CI: "true"
  run: python scripts/generate_release_bom.py

- name: Release BOM consistency check
  env:
    CI: "true"
  run: python scripts/check_release_bom_consistency.py
This gives earlier failure before the release workflow runs (if you keep both workflows).
________________________________________
Next diff (final polish)
next diff: BOM contract manifest gate
Hash-guard scripts/generate_release_bom.py + scripts/check_release_bom_consistency.py + the BOM schema (if you add one) so changes to BOM semantics require a version bump (same discipline as the rest of your system).
Ship next diff: BOM contract manifest gate
next diff: BOM contract manifest gate
This adds a first-class contract gate for the Release BOM system so semantic drift in BOM generation/validation is never silent.
It mirrors your existing governance style:
•	Manifest of AST-normalized hashes for BOM-critical code + schema surfaces
•	Version anchor that must bump when the guarded surface changes
•	Refresh script to regenerate the manifest deterministically
•	Hard CI test that fails if:
o	code changed without version bump + manifest refresh
o	manifest version doesn’t match code anchor
o	manifest files missing / wrong
This closes the loop: the release BOM becomes a governed contract surface, not “just a script.”
________________________________________
What’s in the guarded surface
Files to hash (minimum)
•	scripts/generate_release_bom.py (BOM semantics)
•	scripts/check_release_bom_consistency.py (BOM truth enforcement semantics)
Optional but recommended
•	schemas/release_bom.schema.json (if you add a BOM schema; see below)
•	Any future “BOM policy constants” module(s)
Because you already have a pattern of freezing schema IDs and gating schema changes, adding an explicit BOM schema is a strong move. I include it here as optional but scaffolded.
________________________________________
Diff A — Add BOM version anchor
Patch: src/code_audit/model/run_result.py
Add a new anchor alongside your other governance anchors:
diff --git a/src/code_audit/model/run_result.py b/src/code_audit/model/run_result.py
index 2222222..3333333 100644
--- a/src/code_audit/model/run_result.py
+++ b/src/code_audit/model/run_result.py
@@ -28,10 +28,16 @@
 engine_version = "engine_v1"
 signal_logic_version = "signals_v1"
 openapi_classifier_version = "openapi_classifier_v1"
+
+# Release BOM contract anchor.
+# Bump when release_bom generation/validation semantics change.
+bom_logic_version = "bom_v1"
If you prefer BOM versioning to live elsewhere, the same works in a dedicated src/code_audit/contracts/versioning.py, but keeping anchors centralized is consistent with your repo.
________________________________________
Diff B — Add BOM manifest file
New file: tests/contracts/bom_manifest.json
diff --git a/tests/contracts/bom_manifest.json b/tests/contracts/bom_manifest.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/contracts/bom_manifest.json
@@ -0,0 +1,28 @@
+{
+  "version": 1,
+  "bom_logic_version": "bom_v1",
+  "files": {
+    "scripts/generate_release_bom.py": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    },
+    "scripts/check_release_bom_consistency.py": {
+      "ast_sha256": "REPLACE_ME",
+      "ast_sha256_short": "REPLACE_ME"
+    }
+  }
+}
If/when you add a BOM JSON schema file, you can add:
•	schemas/release_bom.schema.json hashed either as:
o	raw sha256 (schema is JSON, not Python), or
o	canonical JSON hash (sorted keys) to ignore formatting
I include an optional extension below.
________________________________________
Diff C — Add manifest refresh script (AST-normalized)
New file: scripts/refresh_bom_manifest.py
This follows your established AST hashing approach:
•	strips docstrings
•	ignores the bom_logic_version literal so bumping version doesn’t itself change the manifest hash
•	uses ast.dump(..., include_attributes=False) for stable semantics
diff --git a/scripts/refresh_bom_manifest.py b/scripts/refresh_bom_manifest.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/refresh_bom_manifest.py
@@ -0,0 +1,205 @@
+from __future__ import annotations
+
+import ast
+import hashlib
+import json
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+MANIFEST_PATH = ROOT / "tests" / "contracts" / "bom_manifest.json"
+
+TARGET_FILES = [
+    "scripts/generate_release_bom.py",
+    "scripts/check_release_bom_consistency.py",
+]
+
+
+def _sha256_text(s: str) -> str:
+    return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+class _StripDocstrings(ast.NodeTransformer):
+    def _strip_body(self, body):
+        if not body:
+            return body
+        if isinstance(body[0], ast.Expr) and isinstance(getattr(body[0], "value", None), ast.Constant):
+            if isinstance(body[0].value.value, str):
+                return body[1:]
+        return body
+
+    def visit_Module(self, node: ast.Module):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_FunctionDef(self, node: ast.FunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_ClassDef(self, node: ast.ClassDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+
+class _NeutralizeVersionLiterals(ast.NodeTransformer):
+    """
+    Replace string constants assigned to bom_logic_version with a sentinel,
+    so bumping the version does not itself change the manifest hash.
+    """
+
+    def visit_Assign(self, node: ast.Assign):  # type: ignore[override]
+        for t in node.targets:
+            if isinstance(t, ast.Name) and t.id == "bom_logic_version":
+                if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                    node.value = ast.Constant(value="__VERSION__")
+        return self.generic_visit(node)
+
+
+def _ast_normalized_dump(path: Path) -> str:
+    src = path.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    tree = _StripDocstrings().visit(tree)  # type: ignore[assignment]
+    tree = _NeutralizeVersionLiterals().visit(tree)  # type: ignore[assignment]
+    ast.fix_missing_locations(tree)
+    return ast.dump(tree, annotate_fields=True, include_attributes=False)
+
+
+def _load_bom_logic_version() -> str:
+    rr = ROOT / "src" / "code_audit" / "model" / "run_result.py"
+    src = rr.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    for node in tree.body:
+        if isinstance(node, ast.Assign):
+            for t in node.targets:
+                if isinstance(t, ast.Name) and t.id == "bom_logic_version":
+                    if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                        return node.value.value
+    raise RuntimeError("bom_logic_version not found in src/code_audit/model/run_result.py")
+
+
+def main() -> int:
+    out: Dict[str, Any] = {
+        "version": 1,
+        "bom_logic_version": _load_bom_logic_version(),
+        "files": {},
+    }
+
+    for rel in TARGET_FILES:
+        p = ROOT / rel
+        if not p.exists():
+            raise RuntimeError(f"Missing target file: {rel}")
+        dumped = _ast_normalized_dump(p)
+        h = _sha256_text(dumped)
+        out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
+
+    MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)
+    MANIFEST_PATH.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+    print(f"Wrote {MANIFEST_PATH.relative_to(ROOT)}")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
________________________________________
Diff D — Add hard gate test: semantic change requires version bump + manifest refresh
New file: tests/test_bom_manifest_gate.py
diff --git a/tests/test_bom_manifest_gate.py b/tests/test_bom_manifest_gate.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_bom_manifest_gate.py
@@ -0,0 +1,178 @@
+from __future__ import annotations
+
+import ast
+import hashlib
+import json
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+MANIFEST_PATH = ROOT / "tests" / "contracts" / "bom_manifest.json"
+
+TARGET_FILES = [
+    "scripts/generate_release_bom.py",
+    "scripts/check_release_bom_consistency.py",
+]
+
+
+def _sha256_text(s: str) -> str:
+    return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+class _StripDocstrings(ast.NodeTransformer):
+    def _strip_body(self, body):
+        if not body:
+            return body
+        if isinstance(body[0], ast.Expr) and isinstance(getattr(body[0], "value", None), ast.Constant):
+            if isinstance(body[0].value.value, str):
+                return body[1:]
+        return body
+
+    def visit_Module(self, node: ast.Module):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_FunctionDef(self, node: ast.FunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+    def visit_ClassDef(self, node: ast.ClassDef):  # type: ignore[override]
+        node.body = self._strip_body(node.body)
+        self.generic_visit(node)
+        return node
+
+
+class _NeutralizeVersionLiterals(ast.NodeTransformer):
+    def visit_Assign(self, node: ast.Assign):  # type: ignore[override]
+        for t in node.targets:
+            if isinstance(t, ast.Name) and t.id == "bom_logic_version":
+                if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                    node.value = ast.Constant(value="__VERSION__")
+        return self.generic_visit(node)
+
+
+def _ast_normalized_dump(path: Path) -> str:
+    src = path.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    tree = _StripDocstrings().visit(tree)  # type: ignore[assignment]
+    tree = _NeutralizeVersionLiterals().visit(tree)  # type: ignore[assignment]
+    ast.fix_missing_locations(tree)
+    return ast.dump(tree, annotate_fields=True, include_attributes=False)
+
+
+def _load_bom_logic_version() -> str:
+    rr = ROOT / "src" / "code_audit" / "model" / "run_result.py"
+    src = rr.read_text(encoding="utf-8")
+    tree = ast.parse(src)
+    for node in tree.body:
+        if isinstance(node, ast.Assign):
+            for t in node.targets:
+                if isinstance(t, ast.Name) and t.id == "bom_logic_version":
+                    if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
+                        return node.value.value
+    raise RuntimeError("bom_logic_version not found in src/code_audit/model/run_result.py")
+
+
+def test_bom_manifest_gate() -> None:
+    assert MANIFEST_PATH.exists(), (
+        "Missing tests/contracts/bom_manifest.json. "
+        "Run: python scripts/refresh_bom_manifest.py"
+    )
+    manifest = json.loads(MANIFEST_PATH.read_text(encoding="utf-8"))
+
+    expected_version = manifest.get("bom_logic_version")
+    actual_version = _load_bom_logic_version()
+    if expected_version != actual_version:
+        raise AssertionError(
+            "bom_logic_version mismatch vs manifest.\n"
+            f"  manifest: {expected_version}\n"
+            f"  code:     {actual_version}\n"
+            "Fix: after bumping bom_logic_version, run python scripts/refresh_bom_manifest.py"
+        )
+
+    files = manifest.get("files") or {}
+    errors = []
+    for rel in TARGET_FILES:
+        p = ROOT / rel
+        assert p.exists(), f"Missing target file: {rel}"
+        dumped = _ast_normalized_dump(p)
+        h = _sha256_text(dumped)
+        want = (files.get(rel) or {}).get("ast_sha256")
+        if want != h:
+            errors.append((rel, want, h))
+
+    if errors:
+        lines = [
+            "Release BOM semantic drift detected (AST hash mismatch).",
+            "This surface is contract-guarded.",
+            "",
+            "Required actions:",
+            "  1) bump bom_logic_version in src/code_audit/model/run_result.py",
+            "  2) run: python scripts/refresh_bom_manifest.py",
+            "",
+            "Mismatches:",
+        ]
+        for rel, want, got in errors:
+            lines.append(f"- {rel}")
+            lines.append(f"    manifest: {want}")
+            lines.append(f"    current:  {got}")
+        raise AssertionError("\n".join(lines))
________________________________________
Diff E — Add docs snippet (refresh instructions)
Patch: docs/CONTRACT.md
diff --git a/docs/CONTRACT.md b/docs/CONTRACT.md
index 3333333..4444444 100644
--- a/docs/CONTRACT.md
+++ b/docs/CONTRACT.md
@@
 ### Release BOM contract gate
+
+Scope: release_bom generation + consistency validation.
+
+Guarded files:
+- scripts/generate_release_bom.py
+- scripts/check_release_bom_consistency.py
+
+On semantic change:
+1) bump `bom_logic_version` in `src/code_audit/model/run_result.py`
+2) refresh manifest:
+   `python scripts/refresh_bom_manifest.py`
________________________________________
Optional tightening (recommended): include BOM schema in the gate
If you add schemas/release_bom.schema.json, you can extend the manifest to include a raw SHA:
•	add to tests/contracts/bom_manifest.json under json_files (or reuse files with sha256)
•	update refresh_bom_manifest.py to hash it
That locks BOM schema drift under the same discipline as generator drift.
If you want that as the immediate follow-on, say:
next diff: BOM schema + schema hash inclusion in bom_manifest.
________________________________________
How to run (developer flow)
When you change BOM semantics:
1.	bump:
o	bom_logic_version = "bom_v2" (or similar)
2.	run:
python scripts/refresh_bom_manifest.py
3.	commit:
o	src/code_audit/model/run_result.py
o	tests/contracts/bom_manifest.json
CI will fail otherwise.
________________________________________
Next diff (natural continuation)
next diff: add schemas/release_bom.schema.json + validate BOM against schema in release workflow
Ship next diff: add schemas/release_bom.schema.json + validate BOM against schema in release workflow
next diff: add schemas/release_bom.schema.json + validate BOM against schema in release workflow
This makes the Release BOM a first-class schema-governed artifact (like run_result_v1), and adds a tag-time hard gate that validates the generated dist/release_bom.json against that schema before publishing the GitHub Release.
You’ll get:
•	A stable $id + schema_version naming contract
•	A minimal, strict JSON Schema that matches the BOM fields you already emit
•	A validator script that fails with actionable errors
•	A release workflow step that enforces schema compliance (so malformed BOMs never ship)
________________________________________
Diff A — Add BOM JSON Schema
New file: schemas/release_bom.schema.json
This schema is intentionally strict where it matters (presence + types + required provenance/artifact hash fields), while still allowing controlled forward evolution by:
•	permitting additional keys inside provenance (optional), but
•	keeping artifacts shape and required subfields locked.
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/schemas/release_bom.schema.json
@@ -0,0 +1,222 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "$id": "release_bom_schema_v1",
+  "schema_version": "release_bom_schema_v1",
+  "title": "Release BOM Schema",
+  "type": "object",
+  "additionalProperties": false,
+  "required": ["version", "release_tag", "artifacts", "provenance"],
+  "properties": {
+    "version": {
+      "type": "integer",
+      "const": 1
+    },
+    "release_tag": {
+      "type": ["string", "null"],
+      "pattern": "^v\\d+\\.\\d+\\.\\d+$"
+    },
+    "artifacts": {
+      "type": "object",
+      "additionalProperties": false,
+      "required": ["openapi_diff_report", "openapi_classifier_manifest"],
+      "properties": {
+        "openapi_diff_report": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "sha256"],
+          "properties": {
+            "path": { "type": "string", "minLength": 1 },
+            "sha256": {
+              "type": "string",
+              "pattern": "^[a-f0-9]{64}$"
+            }
+          }
+        },
+        "openapi_classifier_manifest": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "sha256", "sha256_short", "selected"],
+          "properties": {
+            "path": { "type": "string", "minLength": 1 },
+            "sha256": {
+              "type": "string",
+              "pattern": "^[a-f0-9]{64}$"
+            },
+            "sha256_short": {
+              "type": "string",
+              "pattern": "^[a-f0-9]{12}$"
+            },
+            "selected": {
+              "type": "object",
+              "additionalProperties": false,
+              "required": [
+                "openapi_classifier_version",
+                "closure_file_count",
+                "closure_files",
+                "closure_graph_edges_sha256",
+                "closure_graph_edges_sha256_short",
+                "files_ast_sha256"
+              ],
+              "properties": {
+                "openapi_classifier_version": { "type": "string", "minLength": 1 },
+                "closure_file_count": { "type": "integer", "minimum": 1 },
+                "closure_files": {
+                  "type": "array",
+                  "minItems": 1,
+                  "items": { "type": "string", "minLength": 1 }
+                },
+                "closure_graph_edges_sha256": {
+                  "type": "string",
+                  "pattern": "^[a-f0-9]{64}$"
+                },
+                "closure_graph_edges_sha256_short": {
+                  "type": "string",
+                  "pattern": "^[a-f0-9]{12}$"
+                },
+                "files_ast_sha256": {
+                  "type": "object",
+                  "minProperties": 1,
+                  "additionalProperties": {
+                    "type": "string",
+                    "pattern": "^[a-f0-9]{64}$"
+                  }
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "provenance": {
+      "type": "object",
+      "additionalProperties": false,
+      "required": [
+        "baseline_tag",
+        "current_tag",
+        "policy_path",
+        "policy_sha256",
+        "before_openapi_path",
+        "before_openapi_sha256",
+        "after_openapi_path",
+        "after_openapi_sha256"
+      ],
+      "properties": {
+        "baseline_tag": { "type": ["string", "null"] },
+        "current_tag": { "type": ["string", "null"] },
+        "policy_path": { "type": ["string", "null"] },
+        "policy_sha256": {
+          "type": ["string", "null"],
+          "pattern": "^[a-f0-9]{64}$"
+        },
+        "before_openapi_path": { "type": ["string", "null"] },
+        "before_openapi_sha256": {
+          "type": ["string", "null"],
+          "pattern": "^[a-f0-9]{64}$"
+        },
+        "after_openapi_path": { "type": ["string", "null"] },
+        "after_openapi_sha256": {
+          "type": ["string", "null"],
+          "pattern": "^[a-f0-9]{64}$"
+        }
+      }
+    }
+  }
+}
Notes
•	additionalProperties: false at top-level means BOM stays clean and reviewable.
•	The schema assumes your BOM version is 1. If you bump, you’ll produce release_bom_schema_v2 and adjust const.
•	This schema doesn’t validate that closure_file_count == len(closure_files) (JSON Schema can’t easily do that); that’s already enforced by your BOM consistency checker.
________________________________________
Diff B — Add a BOM schema validator script
New file: scripts/validate_release_bom.py
Uses jsonschema (commonly present; if not, add dependency in your tooling extras). It validates BOM against schema and prints readable errors.
diff --git a/scripts/validate_release_bom.py b/scripts/validate_release_bom.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/validate_release_bom.py
@@ -0,0 +1,128 @@
+from __future__ import annotations
+
+import json
+import sys
+from pathlib import Path
+
+try:
+    import jsonschema
+except Exception as e:  # pragma: no cover
+    raise RuntimeError(
+        "jsonschema is required to validate release_bom.json. "
+        "Add it to dev dependencies or install it in CI."
+    ) from e
+
+
+ROOT = Path(__file__).resolve().parents[1]
+SCHEMA_PATH = ROOT / "schemas" / "release_bom.schema.json"
+
+
+def _load_json(path: Path):
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def main(argv: list[str]) -> int:
+    if len(argv) != 2:
+        print("Usage: python scripts/validate_release_bom.py dist/release_bom.json", file=sys.stderr)
+        return 2
+
+    bom_path = ROOT / argv[1]
+    if not SCHEMA_PATH.exists():
+        print(f"Missing schema: {SCHEMA_PATH.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+    if not bom_path.exists():
+        print(f"Missing BOM: {bom_path.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+
+    schema = _load_json(SCHEMA_PATH)
+    bom = _load_json(bom_path)
+
+    validator = jsonschema.Draft202012Validator(schema)
+    errors = sorted(validator.iter_errors(bom), key=lambda e: list(e.path))
+    if errors:
+        print("[release-bom-schema] FAIL: release_bom.json does not match schema.", file=sys.stderr)
+        for e in errors[:50]:
+            path = ".".join([str(p) for p in e.path]) or "<root>"
+            print(f"  - {path}: {e.message}", file=sys.stderr)
+        if len(errors) > 50:
+            print(f"  ... ({len(errors) - 50} more)", file=sys.stderr)
+        return 1
+
+    print("[release-bom-schema] OK: release_bom.json matches schemas/release_bom.schema.json")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main(sys.argv))
________________________________________
Diff C — Add schema validation step to the release workflow (hard gate)
Patch: .github/workflows/release.yml
Run after BOM generation and before publishing.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 5555555..6666666 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Generate release BOM (includes classifier edges + file hashes)
         env:
           CI: "true"
         run: |
           python scripts/generate_release_bom.py
 
+      - name: Validate release BOM against schema (hard gate)
+        env:
+          CI: "true"
+        run: |
+          python scripts/validate_release_bom.py dist/release_bom.json
+
       - name: Release BOM consistency check (hard gate)
         env:
           CI: "true"
         run: |
           python scripts/check_release_bom_consistency.py
Dependency note
If jsonschema isn’t already available in your CI environment, add an install line in the workflow “Install package” step:
python -m pip install jsonschema
Or add it to a dev dependency group and install that group in CI.
________________________________________
Optional tightening: attach the BOM schema as a release asset too
So every release includes both artifact + schema:
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 6666666..7777777 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
             tests/contracts/openapi_breaking_policy.json
             dist/release_bom.json
+            schemas/release_bom.schema.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
This makes releases fully self-describing even for third-party auditors.
________________________________________
Next diff (ties back into your “schema version governance” pattern)
next diff: BOM schema_version naming contract + $id embedding check
Enforce:
•	schema_version matches ^release_bom_schema_v\\d+$
•	$id includes schema_version (or vice versa)
•	changes to schemas/release_bom.schema.json require bom_logic_version bump + bom_manifest refresh
Ship next diff: BOM schema_version naming contract + $id embedding check
next diff: BOM schema_version naming contract + $id embedding check
This adds a hard contract test for your BOM schema file so it can’t drift into ambiguous identity:
•	schema_version must match: ^release_bom_schema_v\d+$
•	$id must either equal schema_version or include it as a substring (so consumers can reliably correlate)
•	Optional: assert $schema is Draft 2020-12 (since you’re using it)
This keeps schema identity stable and avoids the “$id says one thing, schema_version says another” trap.
________________________________________
Diff A — Add schema identity contract test
New file: tests/test_release_bom_schema_identity_contract.py
diff --git a/tests/test_release_bom_schema_identity_contract.py b/tests/test_release_bom_schema_identity_contract.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_release_bom_schema_identity_contract.py
@@ -0,0 +1,120 @@
+from __future__ import annotations
+
+import json
+import re
+from pathlib import Path
+
+
+ROOT = Path(__file__).resolve().parents[1]
+SCHEMA_PATH = ROOT / "schemas" / "release_bom.schema.json"
+
+SCHEMA_VERSION_RE = re.compile(r"^release_bom_schema_v\d+$")
+
+
+def test_release_bom_schema_version_naming_contract() -> None:
+    assert SCHEMA_PATH.exists(), "Missing schemas/release_bom.schema.json"
+    data = json.loads(SCHEMA_PATH.read_text(encoding="utf-8"))
+
+    schema_version = data.get("schema_version")
+    schema_id = data.get("$id")
+    schema_decl = data.get("$schema")
+
+    assert isinstance(schema_version, str) and schema_version, "schema_version must be a non-empty string"
+    assert SCHEMA_VERSION_RE.match(schema_version), (
+        f"schema_version must match {SCHEMA_VERSION_RE.pattern}. "
+        f"Got: {schema_version!r}"
+    )
+
+    assert isinstance(schema_id, str) and schema_id, "$id must be a non-empty string"
+
+    # Identity coherence rule:
+    # Either $id == schema_version (tightest), or $id contains schema_version (still unambiguous).
+    if schema_id != schema_version and schema_version not in schema_id:
+        raise AssertionError(
+            "BOM schema identity mismatch.\n"
+            f"  schema_version: {schema_version}\n"
+            f"  $id:           {schema_id}\n"
+            "Rule: $id must equal schema_version OR include schema_version as a substring."
+        )
+
+    # Optional: enforce the schema draft to avoid accidental downgrade.
+    assert schema_decl == "https://json-schema.org/draft/2020-12/schema", (
+        "Unexpected $schema value. Expected Draft 2020-12:\n"
+        "  https://json-schema.org/draft/2020-12/schema"
+    )
Why a test (vs schema self-constraints)?
•	JSON Schema can’t easily self-validate these relationships.
•	This is a governance contract: tests are the right tool.
________________________________________
Diff B — Align the BOM schema file to the contract (if needed)
If your current schema already has:
•	$id: "release_bom_schema_v1"
•	schema_version: "release_bom_schema_v1"
…then you’re already compliant.
If not, update schemas/release_bom.schema.json like this:
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 1111111..2222222 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@ -1,7 +1,7 @@
 {
   "$schema": "https://json-schema.org/draft/2020-12/schema",
-  "$id": "release_bom_schema_v1",
-  "schema_version": "release_bom_schema_v1",
+  "$id": "release_bom_schema_v1",
+  "schema_version": "release_bom_schema_v1",
   "title": "Release BOM Schema",
   "type": "object",
   "additionalProperties": false,
   "required": ["version", "release_tag", "artifacts", "provenance"],
   "properties": {
(That diff shows no change if you’re already correct—just illustrating the expected state.)
________________________________________
Optional tightening (recommended): include this in your “schema manifest gate”
If you already have a schema version enforcement script (ci/enforce_schema_version_bump.sh) for other schemas, you can add release_bom.schema.json to that policy. But even without that, the identity contract test prevents the most dangerous ambiguity.
________________________________________
Next diff (natural follow-on)
next diff: changes to schemas/release_bom.schema.json require bom_logic_version bump + refresh bom_manifest.json
That’s the “no silent schema drift” guarantee for the BOM schema surface, consistent with your other manifest gates.
Ship next diff: changes to schemas/release_bom.schema.json require bom_logic_version bump + refresh bom_manifest.json
next diff: changes to schemas/release_bom.schema.json require bom_logic_version bump + refresh bom_manifest.json
This extends your BOM contract manifest gate so it also governs the BOM schema. The rule becomes:
•	Any semantic change to:
o	scripts/generate_release_bom.py
o	scripts/check_release_bom_consistency.py
o	schemas/release_bom.schema.json
…requires:
1.	bump bom_logic_version
2.	refresh tests/contracts/bom_manifest.json
To make this formatting-insensitive, we hash the BOM schema as canonical JSON (sorted keys, stable separators), not raw bytes.
________________________________________
Diff A — Extend bom_manifest.json to include schema hash
Patch: tests/contracts/bom_manifest.json
Adds a json_files section (keeps Python AST hashes separate and explicit):
diff --git a/tests/contracts/bom_manifest.json b/tests/contracts/bom_manifest.json
index 1111111..2222222 100644
--- a/tests/contracts/bom_manifest.json
+++ b/tests/contracts/bom_manifest.json
@@ -1,13 +1,22 @@
 {
   "version": 1,
   "bom_logic_version": "bom_v1",
   "files": {
     "scripts/generate_release_bom.py": {
       "ast_sha256": "REPLACE_ME",
       "ast_sha256_short": "REPLACE_ME"
     },
     "scripts/check_release_bom_consistency.py": {
       "ast_sha256": "REPLACE_ME",
       "ast_sha256_short": "REPLACE_ME"
     }
+  },
+  "json_files": {
+    "schemas/release_bom.schema.json": {
+      "canonical_sha256": "REPLACE_ME",
+      "canonical_sha256_short": "REPLACE_ME"
+    }
   }
 }
________________________________________
Diff B — Update refresh script to hash the schema canonically
Patch: scripts/refresh_bom_manifest.py
Adds:
•	JSON_TARGET_FILES = ["schemas/release_bom.schema.json"]
•	canonical JSON hashing: json.dumps(obj, sort_keys=True, separators=(",", ":"))
diff --git a/scripts/refresh_bom_manifest.py b/scripts/refresh_bom_manifest.py
index 1111111..2222222 100644
--- a/scripts/refresh_bom_manifest.py
+++ b/scripts/refresh_bom_manifest.py
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
 from typing import Any, Dict
 
@@
 TARGET_FILES = [
     "scripts/generate_release_bom.py",
     "scripts/check_release_bom_consistency.py",
 ]
+
+JSON_TARGET_FILES = [
+    "schemas/release_bom.schema.json",
+]
@@
 def _sha256_text(s: str) -> str:
     return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+def _canonical_json_sha256(path: Path) -> str:
+    obj = json.loads(path.read_text(encoding="utf-8"))
+    canonical = json.dumps(obj, sort_keys=True, separators=(",", ":"))
+    return _sha256_text(canonical)
@@
 def main() -> int:
     out: Dict[str, Any] = {
         "version": 1,
         "bom_logic_version": _load_bom_logic_version(),
         "files": {},
+        "json_files": {},
     }
@@
     for rel in TARGET_FILES:
         p = ROOT / rel
         if not p.exists():
             raise RuntimeError(f"Missing target file: {rel}")
         dumped = _ast_normalized_dump(p)
         h = _sha256_text(dumped)
         out["files"][rel] = {"ast_sha256": h, "ast_sha256_short": h[:12]}
+
+    for rel in JSON_TARGET_FILES:
+        p = ROOT / rel
+        if not p.exists():
+            raise RuntimeError(f"Missing json target file: {rel}")
+        h = _canonical_json_sha256(p)
+        out["json_files"][rel] = {"canonical_sha256": h, "canonical_sha256_short": h[:12]}
 
     MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)
     MANIFEST_PATH.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
     print(f"Wrote {MANIFEST_PATH.relative_to(ROOT)}")
     return 0
________________________________________
Diff C — Update BOM manifest gate test to enforce schema hash too
Patch: tests/test_bom_manifest_gate.py
Adds canonical hashing and asserts the schema hash matches manifest.
diff --git a/tests/test_bom_manifest_gate.py b/tests/test_bom_manifest_gate.py
index 1111111..2222222 100644
--- a/tests/test_bom_manifest_gate.py
+++ b/tests/test_bom_manifest_gate.py
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import ast
 import hashlib
 import json
 from pathlib import Path
 from typing import Any, Dict
 
@@
 TARGET_FILES = [
     "scripts/generate_release_bom.py",
     "scripts/check_release_bom_consistency.py",
 ]
+
+JSON_TARGET_FILES = [
+    "schemas/release_bom.schema.json",
+]
@@
 def _sha256_text(s: str) -> str:
     return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+def _canonical_json_sha256(path: Path) -> str:
+    obj = json.loads(path.read_text(encoding="utf-8"))
+    canonical = json.dumps(obj, sort_keys=True, separators=(",", ":"))
+    return _sha256_text(canonical)
@@
 def test_bom_manifest_gate() -> None:
@@
     files = manifest.get("files") or {}
     errors = []
@@
     if errors:
@@
         raise AssertionError("\n".join(lines))
+
+    # JSON schema surface is also governed (formatting-insensitive canonical hash).
+    json_files = manifest.get("json_files") or {}
+    if not isinstance(json_files, dict):
+        raise AssertionError("bom_manifest.json_files must be an object")
+
+    json_errors = []
+    for rel in JSON_TARGET_FILES:
+        p = ROOT / rel
+        assert p.exists(), f"Missing json target file: {rel}"
+        want = (json_files.get(rel) or {}).get("canonical_sha256")
+        got = _canonical_json_sha256(p)
+        if want != got:
+            json_errors.append((rel, want, got))
+
+    if json_errors:
+        lines = [
+            "Release BOM schema drift detected (canonical JSON hash mismatch).",
+            "This surface is contract-guarded.",
+            "",
+            "Required actions:",
+            "  1) bump bom_logic_version in src/code_audit/model/run_result.py",
+            "  2) run: python scripts/refresh_bom_manifest.py",
+            "",
+            "Mismatches:",
+        ]
+        for rel, want, got in json_errors:
+            lines.append(f"- {rel}")
+            lines.append(f"    manifest: {want}")
+            lines.append(f"    current:  {got}")
+        raise AssertionError("\n".join(lines))
This makes schema edits impossible to sneak in via formatting or whitespace changes: only actual semantic JSON changes (key/value differences) affect the canonical hash.
________________________________________
Diff D — Update docs to include schema in the BOM gate surface
Patch: docs/CONTRACT.md
diff --git a/docs/CONTRACT.md b/docs/CONTRACT.md
index 4444444..5555555 100644
--- a/docs/CONTRACT.md
+++ b/docs/CONTRACT.md
@@
 ### Release BOM contract gate
 
 Scope: release_bom generation + consistency validation.
 
 Guarded files:
 - scripts/generate_release_bom.py
 - scripts/check_release_bom_consistency.py
+- schemas/release_bom.schema.json (canonical JSON hash)
 
 On semantic change:
 1) bump `bom_logic_version` in `src/code_audit/model/run_result.py`
 2) refresh manifest:
    `python scripts/refresh_bom_manifest.py`
________________________________________
Developer workflow recap
When you change BOM schema or BOM semantics:
1.	bump:
o	bom_logic_version = "bom_v2" (or next)
2.	refresh manifest:
python scripts/refresh_bom_manifest.py
3.	commit:
o	src/code_audit/model/run_result.py
o	tests/contracts/bom_manifest.json
o	and your changed file(s)
CI will block merges if you forget any step.
________________________________________
Next diff (optional “max governance” polish)
next diff: enforce schemas/release_bom.schema.json also appears in the Release BOM assets + BOM includes its sha
So the release itself carries:
•	the BOM
•	the BOM schema
•	and the BOM points to the schema hash (full self-description).
Ship next diff: enforce schemas/release_bom.schema.json also appears in the Release BOM assets + BOM includes its sha
next diff: enforce schemas/release_bom.schema.json is shipped as a Release asset + BOM includes its sha
This hardens the “release is self-describing” story for the BOM itself:
1.	Release assets must include schemas/release_bom.schema.json (so consumers can validate the BOM out-of-band).
2.	dist/release_bom.json must contain the BOM schema’s hash + identity (schema_version + $id), and the release gate must fail if it’s missing or mismatched.
________________________________________
Diff A — Extend BOM generator to include BOM schema provenance + sha
Patch: scripts/generate_release_bom.py
Add:
•	load schemas/release_bom.schema.json
•	compute canonical sha256 (sorting keys, stable separators)
•	add a new artifact entry: release_bom_schema
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
index 1111111..2222222 100644
--- a/scripts/generate_release_bom.py
+++ b/scripts/generate_release_bom.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 from pathlib import Path
 from typing import Any, Dict
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
 
 OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
 OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+RELEASE_BOM_SCHEMA = ROOT / "schemas" / "release_bom.schema.json"
 
 OUT_BOM = DIST / "release_bom.json"
 
@@
 def _load_json(path: Path) -> Dict[str, Any]:
     return json.loads(path.read_text(encoding="utf-8"))
 
+def _canonical_json_bytes(obj: Any) -> bytes:
+    # Canonical JSON for stable hashing (formatting-insensitive).
+    return json.dumps(obj, sort_keys=True, separators=(",", ":")).encode("utf-8")
+
+def _sha256_canonical_json_file(path: Path) -> str:
+    obj = _load_json(path)
+    return _sha256_bytes(_canonical_json_bytes(obj))
@@
 def main() -> int:
     DIST.mkdir(parents=True, exist_ok=True)
 
     _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
     _require(OPENAPI_CLASSIFIER_MANIFEST, why="Classifier manifest is part of the release contract surface")
+    _require(RELEASE_BOM_SCHEMA, why="Release BOM schema must be present for self-describing releases")
 
     diff_report = _load_json(OPENAPI_DIFF_REPORT)
 
     classifier_manifest_bytes = OPENAPI_CLASSIFIER_MANIFEST.read_bytes()
     classifier_manifest = json.loads(classifier_manifest_bytes.decode("utf-8"))
+    bom_schema = _load_json(RELEASE_BOM_SCHEMA)
+    bom_schema_sha = _sha256_canonical_json_file(RELEASE_BOM_SCHEMA)
+    bom_schema_id = bom_schema.get("$id")
+    bom_schema_version = bom_schema.get("schema_version")
+    if not isinstance(bom_schema_id, str) or not bom_schema_id:
+        raise RuntimeError("schemas/release_bom.schema.json missing $id")
+    if not isinstance(bom_schema_version, str) or not bom_schema_version:
+        raise RuntimeError("schemas/release_bom.schema.json missing schema_version")
 
     bom: Dict[str, Any] = {
         "version": 1,
         "release_tag": (os.environ.get("GITHUB_REF_NAME") or "").strip() or None,
         "artifacts": {
+            "release_bom_schema": {
+                "path": str(RELEASE_BOM_SCHEMA.relative_to(ROOT)),
+                "schema_version": bom_schema_version,
+                "$id": bom_schema_id,
+                "canonical_sha256": bom_schema_sha,
+                "canonical_sha256_short": bom_schema_sha[:12],
+            },
             "openapi_diff_report": {
                 "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
                 "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
             },
             "openapi_classifier_manifest": {
                 "path": str(OPENAPI_CLASSIFIER_MANIFEST.relative_to(ROOT)),
                 "sha256": _sha256_bytes(classifier_manifest_bytes),
                 "sha256_short": _sha256_bytes(classifier_manifest_bytes)[:12],
                 "selected": _select_classifier_fingerprint(classifier_manifest),
             },
         },
         "provenance": {
@@
         },
     }
 
     OUT_BOM.write_text(json.dumps(bom, indent=2, sort_keys=True) + "\n", encoding="utf-8")
     print(f"Wrote {OUT_BOM.relative_to(ROOT)}")
     return 0
Why “canonical_sha256” for the schema?
So whitespace/formatting changes do not cause false churn in the BOM identity fields. Only real JSON changes do.
________________________________________
Diff B — Tighten BOM schema JSON Schema to include the new artifact entry
Patch: schemas/release_bom.schema.json
Add release_bom_schema under artifacts.required and define its fields.
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 1111111..2222222 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@ -1,6 +1,6 @@
 {
   "$schema": "https://json-schema.org/draft/2020-12/schema",
   "$id": "release_bom_schema_v1",
   "schema_version": "release_bom_schema_v1",
   "title": "Release BOM Schema",
@@
     "artifacts": {
       "type": "object",
       "additionalProperties": false,
-      "required": ["openapi_diff_report", "openapi_classifier_manifest"],
+      "required": ["release_bom_schema", "openapi_diff_report", "openapi_classifier_manifest"],
       "properties": {
+        "release_bom_schema": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "schema_version", "$id", "canonical_sha256", "canonical_sha256_short"],
+          "properties": {
+            "path": { "type": "string", "minLength": 1 },
+            "schema_version": { "type": "string", "minLength": 1 },
+            "$id": { "type": "string", "minLength": 1 },
+            "canonical_sha256": {
+              "type": "string",
+              "pattern": "^[a-f0-9]{64}$"
+            },
+            "canonical_sha256_short": {
+              "type": "string",
+              "pattern": "^[a-f0-9]{12}$"
+            }
+          }
+        },
         "openapi_diff_report": {
           "type": "object",
           "additionalProperties": false,
           "required": ["path", "sha256"],
           "properties": {
             "path": { "type": "string", "minLength": 1 },
             "sha256": {
               "type": "string",
               "pattern": "^[a-f0-9]{64}$"
             }
           }
         },
________________________________________
Diff C — Enforce schema is uploaded as a Release asset (and fail if missing)
Patch: .github/workflows/release.yml
Add the schema file explicitly to release assets and include a preflight assert so a workflow edit can’t silently stop shipping it.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 7777777..8888888 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Validate release BOM against schema (hard gate)
         env:
           CI: "true"
         run: |
           python scripts/validate_release_bom.py dist/release_bom.json
 
       - name: Release BOM consistency check (hard gate)
         env:
           CI: "true"
         run: |
           python scripts/check_release_bom_consistency.py
+
+      - name: Verify BOM schema file exists (must be shipped)
+        run: |
+          test -f schemas/release_bom.schema.json
+          echo "Found schemas/release_bom.schema.json"
 
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
             tests/contracts/openapi_breaking_policy.json
             dist/release_bom.json
-            schemas/release_bom.schema.json
+            schemas/release_bom.schema.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
(If you already added it earlier, keep it; the important part is the explicit presence check + schema-in-BOM enforcement below.)
________________________________________
Diff D — Extend BOM consistency checker to enforce schema sha + identity
Patch: scripts/check_release_bom_consistency.py
Add:
•	verify BOM contains artifacts.release_bom_schema
•	compute canonical schema sha and compare to BOM
•	optionally also verify $id and schema_version fields match the schema file
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 1111111..2222222 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import sys
 from pathlib import Path
 from typing import Any, Dict
 
@@
 CLASSIFIER_MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
+RELEASE_BOM_SCHEMA_PATH = ROOT / "schemas" / "release_bom.schema.json"
 
@@
 def _load_json(path: Path) -> Dict[str, Any]:
     return json.loads(path.read_text(encoding="utf-8"))
 
+def _canonical_json_bytes(obj: Any) -> bytes:
+    return json.dumps(obj, sort_keys=True, separators=(",", ":")).encode("utf-8")
+
+def _sha256_canonical_json_file(path: Path) -> str:
+    obj = _load_json(path)
+    return hashlib.sha256(_canonical_json_bytes(obj)).hexdigest()
@@
 def main() -> int:
     errors: list[str] = []
@@
     _req(CLASSIFIER_MANIFEST_PATH.exists(), f"Missing {CLASSIFIER_MANIFEST_PATH.relative_to(ROOT)}", errors)
     _req(POLICY_PATH.exists(), f"Missing {POLICY_PATH.relative_to(ROOT)}", errors)
+    _req(RELEASE_BOM_SCHEMA_PATH.exists(), f"Missing {RELEASE_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors)
@@
     bom = _load_json(BOM_PATH)
     diff = _load_json(DIFF_REPORT_PATH)
     manifest = _load_json(CLASSIFIER_MANIFEST_PATH)
+    schema = _load_json(RELEASE_BOM_SCHEMA_PATH)
@@
     artifacts = bom.get("artifacts") or {}
@@
     ocm = artifacts.get("openapi_classifier_manifest") or {}
     _req(isinstance(ocm, dict), "BOM: artifacts.openapi_classifier_manifest must be an object", errors)
+
+    # --- BOM schema artifact must be present and correct ---
+    rbs = artifacts.get("release_bom_schema") or {}
+    _req(isinstance(rbs, dict), "BOM: artifacts.release_bom_schema must be an object", errors)
+    schema_sha = _sha256_canonical_json_file(RELEASE_BOM_SCHEMA_PATH)
+    expected_schema_sha = rbs.get("canonical_sha256")
+    if expected_schema_sha != schema_sha:
+        errors.append("BOM mismatch: release_bom_schema.canonical_sha256 does not match schemas/release_bom.schema.json")
+
+    # Identity fields must match schema file
+    if rbs.get("$id") != schema.get("$id"):
+        errors.append("BOM mismatch: release_bom_schema.$id does not match schema $id")
+    if rbs.get("schema_version") != schema.get("schema_version"):
+        errors.append("BOM mismatch: release_bom_schema.schema_version does not match schema_version")
This makes the release gate fail if:
•	someone regenerates BOM but forgets to ship schema
•	someone ships schema but forgets to regenerate BOM
•	schema changed but BOM didn’t capture it
________________________________________
What’s enforced now
On tag builds, before publishing a release:
•	dist/release_bom.json must:
o	validate against schemas/release_bom.schema.json
o	be consistent with diff report + classifier manifest
o	include BOM schema identity + canonical sha
•	release assets must include:
o	dist/release_bom.json
o	schemas/release_bom.schema.json
So any consumer can download the release, validate the BOM using the included schema, and verify hashes against the BOM’s own embedded provenance.
________________________________________
Next diff (if you want the last “belt-and-suspenders” step)
next diff: BOM generator copies the schema into dist/ and release uploads only dist/*
That eliminates the “upload from repo root” escape hatch and makes the release payload a single directory snapshot.
Ship next diff: BOM generator copies the schema into dist/ and release uploads only dist/*
next diff: BOM generator copies the schema into dist/ and release uploads only dist/*
This eliminates “release uploads from repo root” entirely. The release payload becomes a single canonical directory snapshot (dist/) that contains everything needed to audit the release:
•	build artifacts (wheel/sdist)
•	openapi_diff_report.json
•	release_bom.json
•	openapi_breaking_policy.json (copied into dist/)
•	release_bom.schema.json (copied into dist/)
•	(optional) openapi_classifier_manifest.json (copied into dist/)
Then the release workflow uploads only dist/*.
________________________________________
Diff A — BOM generator copies BOM schema (and optionally policy) into dist/
Patch: scripts/generate_release_bom.py
We add a small helper to copy files into dist/ with deterministic names, then point BOM artifact paths to the dist copies.
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
index 2222222..3333333 100644
--- a/scripts/generate_release_bom.py
+++ b/scripts/generate_release_bom.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
+import shutil
 from pathlib import Path
 from typing import Any, Dict
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
 
 OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
 OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 RELEASE_BOM_SCHEMA = ROOT / "schemas" / "release_bom.schema.json"
+OPENAPI_POLICY = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 
 OUT_BOM = DIST / "release_bom.json"
 
@@
 def _require(path: Path, *, why: str) -> None:
@@
     if not path.exists():
         raise RuntimeError(f"Missing required file: {path.relative_to(ROOT)} ({why})")
 
+def _copy_into_dist(src: Path, dest_name: str) -> Path:
+    DIST.mkdir(parents=True, exist_ok=True)
+    dest = DIST / dest_name
+    shutil.copyfile(src, dest)
+    return dest
+
@@
 def main() -> int:
     DIST.mkdir(parents=True, exist_ok=True)
 
     _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
     _require(OPENAPI_CLASSIFIER_MANIFEST, why="Classifier manifest is part of the release contract surface")
     _require(RELEASE_BOM_SCHEMA, why="Release BOM schema must be present for self-describing releases")
+    _require(OPENAPI_POLICY, why="OpenAPI breaking policy must be present for self-describing releases")
 
     diff_report = _load_json(OPENAPI_DIFF_REPORT)
 
     classifier_manifest_bytes = OPENAPI_CLASSIFIER_MANIFEST.read_bytes()
     classifier_manifest = json.loads(classifier_manifest_bytes.decode("utf-8"))
+
+    # Copy schema + policy into dist/ so release uploads only dist/*
+    dist_schema = _copy_into_dist(RELEASE_BOM_SCHEMA, "release_bom.schema.json")
+    dist_policy = _copy_into_dist(OPENAPI_POLICY, "openapi_breaking_policy.json")
 
-    bom_schema = _load_json(RELEASE_BOM_SCHEMA)
-    bom_schema_sha = _sha256_canonical_json_file(RELEASE_BOM_SCHEMA)
+    bom_schema = _load_json(dist_schema)
+    bom_schema_sha = _sha256_canonical_json_file(dist_schema)
     bom_schema_id = bom_schema.get("$id")
     bom_schema_version = bom_schema.get("schema_version")
@@
     bom: Dict[str, Any] = {
         "version": 1,
         "release_tag": (os.environ.get("GITHUB_REF_NAME") or "").strip() or None,
         "artifacts": {
             "release_bom_schema": {
-                "path": str(RELEASE_BOM_SCHEMA.relative_to(ROOT)),
+                "path": str(dist_schema.relative_to(ROOT)),
                 "schema_version": bom_schema_version,
                 "$id": bom_schema_id,
                 "canonical_sha256": bom_schema_sha,
                 "canonical_sha256_short": bom_schema_sha[:12],
             },
+            "openapi_breaking_policy": {
+                "path": str(dist_policy.relative_to(ROOT)),
+                "sha256": _sha256_file(dist_policy),
+                "sha256_short": _sha256_file(dist_policy)[:12],
+            },
             "openapi_diff_report": {
                 "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
                 "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
             },
             "openapi_classifier_manifest": {
                 "path": str(OPENAPI_CLASSIFIER_MANIFEST.relative_to(ROOT)),
                 "sha256": _sha256_bytes(classifier_manifest_bytes),
                 "sha256_short": _sha256_bytes(classifier_manifest_bytes)[:12],
                 "selected": _select_classifier_fingerprint(classifier_manifest),
             },
         },
         "provenance": {
@@
         },
     }
Why copy policy too?
So the release is self-contained even if you later decide to upload only dist/*. This also makes your “policy hash in diff report” trivially auditable from release assets.
Optional (if you want everything in dist)
Also copy the classifier manifest into dist/:
+    dist_classifier_manifest = _copy_into_dist(OPENAPI_CLASSIFIER_MANIFEST, "openapi_classifier_manifest.json")
…but note: your BOM currently references the repo path tests/contracts/openapi_classifier_manifest.json. If you do this, you should either:
•	keep referencing the repo file (fine, release still self-describing via BOM selected fingerprint + manifest sha), or
•	switch to referencing the dist copy and use that hash (max portability)
If you want the dist copy variant, say “next diff: dist copy classifier manifest”.
________________________________________
Diff B — Update BOM schema to require policy artifact (since BOM now includes it)
Patch: schemas/release_bom.schema.json
Add openapi_breaking_policy under artifacts.required and define its fields.
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 2222222..3333333 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@
     "artifacts": {
       "type": "object",
       "additionalProperties": false,
-      "required": ["release_bom_schema", "openapi_diff_report", "openapi_classifier_manifest"],
+      "required": ["release_bom_schema", "openapi_breaking_policy", "openapi_diff_report", "openapi_classifier_manifest"],
       "properties": {
         "release_bom_schema": {
@@
         },
+        "openapi_breaking_policy": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "sha256", "sha256_short"],
+          "properties": {
+            "path": { "type": "string", "minLength": 1 },
+            "sha256": { "type": "string", "pattern": "^[a-f0-9]{64}$" },
+            "sha256_short": { "type": "string", "pattern": "^[a-f0-9]{12}$" }
+          }
+        },
         "openapi_diff_report": {
           "type": "object",
           "additionalProperties": false,
           "required": ["path", "sha256"],
           "properties": {
             "path": { "type": "string", "minLength": 1 },
             "sha256": {
               "type": "string",
               "pattern": "^[a-f0-9]{64}$"
             }
           }
         },
________________________________________
Diff C — Update BOM consistency checker to validate the dist-copied schema + policy
Patch: scripts/check_release_bom_consistency.py
We adjust it so:
•	it expects schema path to be dist/release_bom.schema.json
•	it expects policy path to be dist/openapi_breaking_policy.json
•	it verifies their hashes against what BOM claims
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 2222222..3333333 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
-CLASSIFIER_MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
-POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
-RELEASE_BOM_SCHEMA_PATH = ROOT / "schemas" / "release_bom.schema.json"
+CLASSIFIER_MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+DIST_POLICY_PATH = ROOT / "dist" / "openapi_breaking_policy.json"
+DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
@@
-    _req(POLICY_PATH.exists(), f"Missing {POLICY_PATH.relative_to(ROOT)}", errors)
-    _req(RELEASE_BOM_SCHEMA_PATH.exists(), f"Missing {RELEASE_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors)
+    _req(DIST_POLICY_PATH.exists(), f"Missing {DIST_POLICY_PATH.relative_to(ROOT)}", errors)
+    _req(DIST_BOM_SCHEMA_PATH.exists(), f"Missing {DIST_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors)
@@
-    schema = _load_json(RELEASE_BOM_SCHEMA_PATH)
+    schema = _load_json(DIST_BOM_SCHEMA_PATH)
@@
-    schema_sha = _sha256_canonical_json_file(RELEASE_BOM_SCHEMA_PATH)
+    schema_sha = _sha256_canonical_json_file(DIST_BOM_SCHEMA_PATH)
     expected_schema_sha = rbs.get("canonical_sha256")
@@
-    if rbs.get("$id") != schema.get("$id"):
+    if rbs.get("$id") != schema.get("$id"):
         errors.append("BOM mismatch: release_bom_schema.$id does not match schema $id")
     if rbs.get("schema_version") != schema.get("schema_version"):
         errors.append("BOM mismatch: release_bom_schema.schema_version does not match schema_version")
+
+    # Policy artifact checks (BOM -> dist policy file)
+    pol = artifacts.get("openapi_breaking_policy") or {}
+    _req(isinstance(pol, dict), "BOM: artifacts.openapi_breaking_policy must be an object", errors)
+    if isinstance(pol, dict):
+        exp_pol_sha = pol.get("sha256")
+        got_pol_sha = _sha256_file(DIST_POLICY_PATH)
+        if exp_pol_sha != got_pol_sha:
+            errors.append("BOM mismatch: openapi_breaking_policy.sha256 does not match dist/openapi_breaking_policy.json")
________________________________________
Diff D — Update BOM schema validator call to validate against dist schema
Patch: scripts/validate_release_bom.py
Use the schema from dist/ (the release-packaged schema), not repo root.
diff --git a/scripts/validate_release_bom.py b/scripts/validate_release_bom.py
index 1111111..2222222 100644
--- a/scripts/validate_release_bom.py
+++ b/scripts/validate_release_bom.py
@@
 ROOT = Path(__file__).resolve().parents[1]
-SCHEMA_PATH = ROOT / "schemas" / "release_bom.schema.json"
+SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
And update the workflow to ensure the BOM generator ran (it creates the schema copy) before validation (it already does in prior diffs).
________________________________________
Diff E — Release workflow uploads only dist/*
Patch: .github/workflows/release.yml
Remove any non-dist paths.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 8888888..9999999 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
-      - name: Verify BOM schema file exists (must be shipped)
-        run: |
-          test -f schemas/release_bom.schema.json
-          echo "Found schemas/release_bom.schema.json"
+      - name: Verify dist contains required release payload files
+        run: |
+          test -f dist/release_bom.json
+          test -f dist/release_bom.schema.json
+          test -f dist/openapi_diff_report.json
+          test -f dist/openapi_breaking_policy.json
+          echo "dist payload OK"
@@
       - name: Create GitHub Release and upload assets
         uses: softprops/action-gh-release@v2
         with:
           files: |
             dist/*
-            tests/contracts/openapi_breaking_policy.json
-            dist/release_bom.json
-            schemas/release_bom.schema.json
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
Now the release workflow cannot accidentally ship inconsistent root-level artifacts. Everything must flow through dist/.
________________________________________
Result: the release payload becomes a single canonical directory snapshot
dist/
•	wheel/sdist
•	openapi_diff_report.json
•	release_bom.json
•	release_bom.schema.json
•	openapi_breaking_policy.json
•	(optional later) openapi_classifier_manifest.json
Your release job becomes:
1.	generate diff report
2.	generate BOM (and copy schema/policy into dist)
3.	validate BOM vs dist schema
4.	BOM consistency check
5.	build
6.	upload dist/*
________________________________________
Next diff (max portability)
next diff: copy tests/contracts/openapi_classifier_manifest.json into dist and have BOM reference that dist copy
So a release can be audited with only release assets, without any repo checkout.
Ship next diff: copy tests/contracts/openapi_classifier_manifest.json into dist and have BOM reference that dist copy
next diff: copy tests/contracts/openapi_classifier_manifest.json into dist/ and have BOM reference that dist copy
This completes the “release is fully auditable from assets alone” goal:
•	The classifier manifest is no longer a repo-relative dependency.
•	The BOM points to a dist/openapi_classifier_manifest.json copy.
•	All consistency checks validate against the dist copy.
•	Release workflow uploads only dist/* and still remains fully self-describing.
________________________________________
Diff A — BOM generator copies classifier manifest into dist and references it
Patch: scripts/generate_release_bom.py
Changes:
•	copy tests/contracts/openapi_classifier_manifest.json into dist/openapi_classifier_manifest.json
•	compute manifest sha from dist copy bytes
•	set BOM artifact openapi_classifier_manifest.path to the dist file
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
index 3333333..4444444 100644
--- a/scripts/generate_release_bom.py
+++ b/scripts/generate_release_bom.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import shutil
 from pathlib import Path
 from typing import Any, Dict
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
 
 OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
-OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 RELEASE_BOM_SCHEMA = ROOT / "schemas" / "release_bom.schema.json"
 OPENAPI_POLICY = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 
 OUT_BOM = DIST / "release_bom.json"
@@
 def _copy_into_dist(src: Path, dest_name: str) -> Path:
     DIST.mkdir(parents=True, exist_ok=True)
     dest = DIST / dest_name
     shutil.copyfile(src, dest)
     return dest
 
@@
 def main() -> int:
@@
     _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
     _require(OPENAPI_CLASSIFIER_MANIFEST, why="Classifier manifest is part of the release contract surface")
     _require(RELEASE_BOM_SCHEMA, why="Release BOM schema must be present for self-describing releases")
     _require(OPENAPI_POLICY, why="OpenAPI breaking policy must be present for self-describing releases")
 
     diff_report = _load_json(OPENAPI_DIFF_REPORT)
 
-    classifier_manifest_bytes = OPENAPI_CLASSIFIER_MANIFEST.read_bytes()
-    classifier_manifest = json.loads(classifier_manifest_bytes.decode("utf-8"))
-
     # Copy schema + policy into dist/ so release uploads only dist/*
     dist_schema = _copy_into_dist(RELEASE_BOM_SCHEMA, "release_bom.schema.json")
     dist_policy = _copy_into_dist(OPENAPI_POLICY, "openapi_breaking_policy.json")
+    dist_classifier_manifest = _copy_into_dist(OPENAPI_CLASSIFIER_MANIFEST, "openapi_classifier_manifest.json")
+
+    classifier_manifest_bytes = dist_classifier_manifest.read_bytes()
+    classifier_manifest = json.loads(classifier_manifest_bytes.decode("utf-8"))
 
     bom_schema = _load_json(dist_schema)
     bom_schema_sha = _sha256_canonical_json_file(dist_schema)
@@
     bom: Dict[str, Any] = {
@@
         "artifacts": {
@@
             "openapi_diff_report": {
                 "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
                 "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
             },
             "openapi_classifier_manifest": {
-                "path": str(OPENAPI_CLASSIFIER_MANIFEST.relative_to(ROOT)),
+                "path": str(dist_classifier_manifest.relative_to(ROOT)),
                 "sha256": _sha256_bytes(classifier_manifest_bytes),
                 "sha256_short": _sha256_bytes(classifier_manifest_bytes)[:12],
                 "selected": _select_classifier_fingerprint(classifier_manifest),
             },
         },
         "provenance": {
________________________________________
Diff B — Update BOM schema to reflect dist-based classifier manifest path (no shape change required)
Your BOM schema already validates openapi_classifier_manifest.path as a string, so no schema change is strictly required.
But if you want to explicitly lock the filename to dist, you can tighten via a regex pattern:
Optional patch: schemas/release_bom.schema.json
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 3333333..4444444 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@
         "openapi_classifier_manifest": {
           "type": "object",
           "additionalProperties": false,
           "required": ["path", "sha256", "sha256_short", "selected"],
           "properties": {
-            "path": { "type": "string", "minLength": 1 },
+            "path": { "type": "string", "pattern": "^dist/openapi_classifier_manifest\\.json$" },
             "sha256": {
               "type": "string",
               "pattern": "^[a-f0-9]{64}$"
             },
             "sha256_short": {
               "type": "string",
               "pattern": "^[a-f0-9]{12}$"
             },
Do the same for:
•	release_bom_schema.path → ^dist/release_bom\.schema\.json$
•	openapi_breaking_policy.path → ^dist/openapi_breaking_policy\.json$
•	openapi_diff_report.path → ^dist/openapi_diff_report\.json$
That’s the “no ambiguity” variant.
________________________________________
Diff C — Update BOM consistency checker to validate against the dist classifier manifest
Patch: scripts/check_release_bom_consistency.py
Replace repo-path manifest with dist-path manifest. Also validate that the BOM points to it.
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 3333333..4444444 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
-CLASSIFIER_MANIFEST_PATH = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
+DIST_CLASSIFIER_MANIFEST_PATH = ROOT / "dist" / "openapi_classifier_manifest.json"
 DIST_POLICY_PATH = ROOT / "dist" / "openapi_breaking_policy.json"
 DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
@@
-    _req(CLASSIFIER_MANIFEST_PATH.exists(), f"Missing {CLASSIFIER_MANIFEST_PATH.relative_to(ROOT)}", errors)
+    _req(DIST_CLASSIFIER_MANIFEST_PATH.exists(), f"Missing {DIST_CLASSIFIER_MANIFEST_PATH.relative_to(ROOT)}", errors)
@@
-    manifest = _load_json(CLASSIFIER_MANIFEST_PATH)
+    manifest = _load_json(DIST_CLASSIFIER_MANIFEST_PATH)
@@
-    expected_manifest_sha = ocm.get("sha256")
-    actual_manifest_sha = _sha256_file(CLASSIFIER_MANIFEST_PATH)
+    # Ensure BOM points to dist copy explicitly (prevents mixed-mode).
+    if ocm.get("path") != "dist/openapi_classifier_manifest.json":
+        errors.append("BOM mismatch: openapi_classifier_manifest.path must be dist/openapi_classifier_manifest.json")
+
+    expected_manifest_sha = ocm.get("sha256")
+    actual_manifest_sha = _sha256_file(DIST_CLASSIFIER_MANIFEST_PATH)
     _req(
         isinstance(expected_manifest_sha, str) and expected_manifest_sha == actual_manifest_sha,
-        "BOM mismatch: openapi_classifier_manifest.sha256 does not match tests/contracts/openapi_classifier_manifest.json",
+        "BOM mismatch: openapi_classifier_manifest.sha256 does not match dist/openapi_classifier_manifest.json",
         errors,
     )
All other checks remain the same, but now you’re validating the release payload only.
________________________________________
Diff D — Ensure workflow asserts dist contains the classifier manifest too
Patch: .github/workflows/release.yml
Update the dist preflight list:
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 9999999..aaaaaaa 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Verify dist contains required release payload files
         run: |
           test -f dist/release_bom.json
           test -f dist/release_bom.schema.json
           test -f dist/openapi_diff_report.json
           test -f dist/openapi_breaking_policy.json
+          test -f dist/openapi_classifier_manifest.json
           echo "dist payload OK"
Because the BOM generator creates it, this ensures:
•	nobody can bypass the BOM generator and still publish a release.
________________________________________
Net effect
After this diff:
•	Everything needed to audit OpenAPI breaking semantics lives in dist/
•	Release publishes only dist/*
•	BOM consistency check validates only release assets
•	Consumers can validate release without cloning the repo
________________________________________
Next diff (if you want the absolute last step for OpenAPI release portability)
next diff: include OpenAPI baseline snapshot bytes in dist too (or attach both before/after OpenAPI snapshots)
That would let a third party fully reproduce the diff from release assets alone, without using git tags at all.
Ross next diff: include OpenAPI baseline snapshot bytes in dist too (or attach both before/after OpenAPI snapshots)
next diff: include OpenAPI baseline snapshot bytes in dist/ (attach both before/after snapshots)
This makes releases fully reproducible from assets alone by shipping:
•	dist/openapi_after.json (the current repo OpenAPI at tag)
•	dist/openapi_before.json (the baseline OpenAPI used for comparison)
•	BOM entries for both snapshots with sha256
•	Diff report updated to reference the dist/ snapshot paths (and still include sha256)
•	Release workflow remains upload only dist/*
You get two modes (this diff ships the stronger one):
•	Option 1 (recommended): ship both before and after snapshots in dist/
•	Option 2: ship only the before baseline snapshot (but you already ship after as docs/openapi.json in-repo; once you’re “dist-only,” you want both)
________________________________________
Diff A — Copy baseline + current OpenAPI into dist during the release gate
The most canonical place to do this is the script that already knows the baseline tag and reads both documents: scripts/check_openapi_release_gate.py.
Patch: scripts/check_openapi_release_gate.py
Add two file outputs:
•	dist/openapi_before.json
•	dist/openapi_after.json
…and update the report to point to those dist paths, and compute sha256 over those exact bytes (so the report matches release assets, not repo files).
diff --git a/scripts/check_openapi_release_gate.py b/scripts/check_openapi_release_gate.py
index 6666666..7777777 100644
--- a/scripts/check_openapi_release_gate.py
+++ b/scripts/check_openapi_release_gate.py
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import re
 import subprocess
 import sys
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple, List
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 CURRENT_OPENAPI = ROOT / "docs" / "openapi.json"
 POLICY_PATH = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 OUT_DIR = ROOT / "dist"
 OUT_REPORT = OUT_DIR / "openapi_diff_report.json"
 OPENAPI_REPO_PATH = "docs/openapi.json"
+
+DIST_BEFORE_OPENAPI = OUT_DIR / "openapi_before.json"
+DIST_AFTER_OPENAPI = OUT_DIR / "openapi_after.json"
@@
 def _sha256_file(path: Path) -> str:
@@
     return h.hexdigest()
 
+def _write_json(path: Path, obj: Any) -> None:
+    path.write_text(json.dumps(obj, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+
@@
 def main() -> int:
@@
     before_doc = _load_openapi_at_tag(prev)
     after_doc = _load_json(CURRENT_OPENAPI)
@@
-    before_openapi_raw = _load_openapi_raw_at_tag(prev)
-    before_openapi_sha = _sha256_bytes(before_openapi_raw)
-    after_openapi_sha = _sha256_file(CURRENT_OPENAPI)
+    # Write reproducible snapshots into dist/ (release uploads dist/* only).
+    OUT_DIR.mkdir(parents=True, exist_ok=True)
+    _write_json(DIST_BEFORE_OPENAPI, before_doc)
+    _write_json(DIST_AFTER_OPENAPI, after_doc)
+
+    before_openapi_sha = _sha256_file(DIST_BEFORE_OPENAPI)
+    after_openapi_sha = _sha256_file(DIST_AFTER_OPENAPI)
@@
-    out["before_openapi_path"] = f"{prev}:{OPENAPI_REPO_PATH}"
+    out["before_openapi_path"] = str(DIST_BEFORE_OPENAPI.relative_to(ROOT))
     out["before_openapi_sha256"] = before_openapi_sha
     out["before_openapi_sha256_short"] = before_openapi_sha[:12]
 
-    out["after_openapi_path"] = OPENAPI_REPO_PATH
+    out["after_openapi_path"] = str(DIST_AFTER_OPENAPI.relative_to(ROOT))
     out["after_openapi_sha256"] = after_openapi_sha
     out["after_openapi_sha256_short"] = after_openapi_sha[:12]
Why write before_doc JSON rather than copy raw bytes from git?
•	It makes the snapshot deterministic and human-readable.
•	You already normalize volatility elsewhere; here we want “what classifier actually compared.”
•	If you want byte-identical baseline capture (rarely necessary), we can instead write before_openapi_raw directly to DIST_BEFORE_OPENAPI and compute sha from bytes. But JSON dumps (sort_keys + indent) is typically what you want for auditability.
If you prefer “exact baseline bytes as stored in git,” say so and I’ll adjust to write the raw bytes (that keeps sha identical to the git blob).
________________________________________
Diff B — Include both snapshot artifacts in the Release BOM
Now that the diff report points to dist snapshots, the BOM should explicitly include them too (so you can audit without parsing diff report).
Patch: scripts/generate_release_bom.py
Add:
•	openapi_before_snapshot
•	openapi_after_snapshot
with their sha256 and paths.
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
index 4444444..5555555 100644
--- a/scripts/generate_release_bom.py
+++ b/scripts/generate_release_bom.py
@@
 OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
+DIST_OPENAPI_BEFORE = DIST / "openapi_before.json"
+DIST_OPENAPI_AFTER = DIST / "openapi_after.json"
@@
 def main() -> int:
@@
     _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
+    _require(DIST_OPENAPI_BEFORE, why="OpenAPI before snapshot must exist in dist/ (produced by release gate)")
+    _require(DIST_OPENAPI_AFTER, why="OpenAPI after snapshot must exist in dist/ (produced by release gate)")
@@
     bom: Dict[str, Any] = {
@@
         "artifacts": {
@@
+            "openapi_before_snapshot": {
+                "path": str(DIST_OPENAPI_BEFORE.relative_to(ROOT)),
+                "sha256": _sha256_file(DIST_OPENAPI_BEFORE),
+                "sha256_short": _sha256_file(DIST_OPENAPI_BEFORE)[:12],
+            },
+            "openapi_after_snapshot": {
+                "path": str(DIST_OPENAPI_AFTER.relative_to(ROOT)),
+                "sha256": _sha256_file(DIST_OPENAPI_AFTER),
+                "sha256_short": _sha256_file(DIST_OPENAPI_AFTER)[:12],
+            },
             "openapi_diff_report": {
                 "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
                 "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
             },
________________________________________
Diff C — Update BOM schema to require both OpenAPI snapshot artifacts
Patch: dist/release_bom.schema.json source-of-truth is schemas/release_bom.schema.json
Add to artifacts.required and define the structure (same as other hash artifacts).
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 4444444..5555555 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@
       "additionalProperties": false,
-      "required": ["release_bom_schema", "openapi_breaking_policy", "openapi_diff_report", "openapi_classifier_manifest"],
+      "required": [
+        "release_bom_schema",
+        "openapi_breaking_policy",
+        "openapi_before_snapshot",
+        "openapi_after_snapshot",
+        "openapi_diff_report",
+        "openapi_classifier_manifest"
+      ],
       "properties": {
@@
+        "openapi_before_snapshot": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "sha256", "sha256_short"],
+          "properties": {
+            "path": { "type": "string", "pattern": "^dist/openapi_before\\.json$" },
+            "sha256": { "type": "string", "pattern": "^[a-f0-9]{64}$" },
+            "sha256_short": { "type": "string", "pattern": "^[a-f0-9]{12}$" }
+          }
+        },
+        "openapi_after_snapshot": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "sha256", "sha256_short"],
+          "properties": {
+            "path": { "type": "string", "pattern": "^dist/openapi_after\\.json$" },
+            "sha256": { "type": "string", "pattern": "^[a-f0-9]{64}$" },
+            "sha256_short": { "type": "string", "pattern": "^[a-f0-9]{12}$" }
+          }
+        },
         "openapi_diff_report": {
Because you copy schemas/release_bom.schema.json into dist/ during BOM generation, this remains consistent.
________________________________________
Diff D — Tighten BOM consistency checker to enforce snapshot presence + sha correctness
Patch: scripts/check_release_bom_consistency.py
Add checks:
•	dist snapshots exist
•	BOM claims match file hashes
•	optionally: match diff report sha fields with the actual snapshot sha (strong cross-coherence)
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 4444444..5555555 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
 DIST_CLASSIFIER_MANIFEST_PATH = ROOT / "dist" / "openapi_classifier_manifest.json"
 DIST_POLICY_PATH = ROOT / "dist" / "openapi_breaking_policy.json"
 DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
+DIST_OPENAPI_BEFORE = ROOT / "dist" / "openapi_before.json"
+DIST_OPENAPI_AFTER = ROOT / "dist" / "openapi_after.json"
@@
     _req(DIST_POLICY_PATH.exists(), f"Missing {DIST_POLICY_PATH.relative_to(ROOT)}", errors)
     _req(DIST_BOM_SCHEMA_PATH.exists(), f"Missing {DIST_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors)
+    _req(DIST_OPENAPI_BEFORE.exists(), f"Missing {DIST_OPENAPI_BEFORE.relative_to(ROOT)}", errors)
+    _req(DIST_OPENAPI_AFTER.exists(), f"Missing {DIST_OPENAPI_AFTER.relative_to(ROOT)}", errors)
@@
     artifacts = bom.get("artifacts") or {}
@@
+    # OpenAPI snapshots must be present and match hashes.
+    bef = artifacts.get("openapi_before_snapshot") or {}
+    aft = artifacts.get("openapi_after_snapshot") or {}
+    _req(isinstance(bef, dict), "BOM: artifacts.openapi_before_snapshot must be an object", errors)
+    _req(isinstance(aft, dict), "BOM: artifacts.openapi_after_snapshot must be an object", errors)
+
+    if isinstance(bef, dict):
+        if bef.get("path") != "dist/openapi_before.json":
+            errors.append("BOM mismatch: openapi_before_snapshot.path must be dist/openapi_before.json")
+        if bef.get("sha256") != _sha256_file(DIST_OPENAPI_BEFORE):
+            errors.append("BOM mismatch: openapi_before_snapshot.sha256 does not match dist/openapi_before.json")
+
+    if isinstance(aft, dict):
+        if aft.get("path") != "dist/openapi_after.json":
+            errors.append("BOM mismatch: openapi_after_snapshot.path must be dist/openapi_after.json")
+        if aft.get("sha256") != _sha256_file(DIST_OPENAPI_AFTER):
+            errors.append("BOM mismatch: openapi_after_snapshot.sha256 does not match dist/openapi_after.json")
+
+    # Cross-coherence: BOM provenance should match diff report hashes.
+    # (diff report already points to dist paths after the earlier diff)
+    if (bom.get("provenance") or {}).get("before_openapi_sha256") != diff.get("before_openapi_sha256"):
+        errors.append("Provenance mismatch: before_openapi_sha256 differs between BOM and diff report")
+    if (bom.get("provenance") or {}).get("after_openapi_sha256") != diff.get("after_openapi_sha256"):
+        errors.append("Provenance mismatch: after_openapi_sha256 differs between BOM and diff report")
This ensures no split-brain between:
•	snapshots
•	diff report
•	BOM provenance
________________________________________
Diff E — Release workflow preflight asserts the new dist files exist
Patch: .github/workflows/release.yml
Add:
•	dist/openapi_before.json
•	dist/openapi_after.json
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index aaaaaaa..bbbbbbb 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Verify dist contains required release payload files
         run: |
           test -f dist/release_bom.json
           test -f dist/release_bom.schema.json
           test -f dist/openapi_diff_report.json
           test -f dist/openapi_breaking_policy.json
           test -f dist/openapi_classifier_manifest.json
+          test -f dist/openapi_before.json
+          test -f dist/openapi_after.json
           echo "dist payload OK"
________________________________________
Result
A third party can download the release assets and have everything needed to:
•	validate BOM against shipped BOM schema
•	validate hashes in BOM against shipped files
•	validate OpenAPI diff report provenance
•	reproduce the OpenAPI diff using openapi_before.json + openapi_after.json
•	audit classifier semantics via shipped classifier manifest (plus closure edge hash, file hashes)
All without cloning the repo or fetching tags.
________________________________________
Next diff (if you want the final “provable reproduction” step)
next diff: add a tiny scripts/reproduce_openapi_diff_from_dist.py that:
•	reads dist/openapi_before.json, dist/openapi_after.json, dist/openapi_breaking_policy.json
•	reruns the classifier
•	asserts the regenerated openapi_diff_report.json matches byte-for-byte (golden reproduction check)
That would make releases not just self-describing, but self-verifying.
Ship next diff: add a tiny scripts/reproduce_openapi_diff_from_dist.py that:
next diff: scripts/reproduce_openapi_diff_from_dist.py (self-verifying release asset replay)
This adds a tiny “replay + assert” script that lets anyone (or CI) reproduce the OpenAPI diff report using only the release assets in dist/, and then fails if the regenerated report is not identical to the shipped one.
What it does
•	Loads:
o	dist/openapi_before.json
o	dist/openapi_after.json
o	dist/openapi_breaking_policy.json
o	dist/openapi_classifier_manifest.json (optional: for provenance checks)
o	dist/openapi_diff_report.json (the shipped report)
•	Runs the same classifier pipeline used in your release gate:
o	normalize (if your pipeline normalizes before diff)
o	diff + schema semantics
o	apply unknown allowlist policy
o	produce a report dict
•	Writes a regenerated report to a temp location (or stdout) and compares it to the shipped report after:
o	canonical JSON normalization (sort_keys=True, stable separators)
o	optional scrub of volatile fields (should be none if your report is already volatility-scrubbed; if it isn’t, this script will force you to make it so)
•	Exits nonzero if mismatch.
This gives you a “release is self-verifying” property.
________________________________________
Diff A — New script: scripts/reproduce_openapi_diff_from_dist.py
diff --git a/scripts/reproduce_openapi_diff_from_dist.py b/scripts/reproduce_openapi_diff_from_dist.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/reproduce_openapi_diff_from_dist.py
@@ -0,0 +1,220 @@
+from __future__ import annotations
+
+import json
+import sys
+from pathlib import Path
+from typing import Any, Dict
+
+
+ROOT = Path(__file__).resolve().parents[1]
+DIST = ROOT / "dist"
+
+BEFORE = DIST / "openapi_before.json"
+AFTER = DIST / "openapi_after.json"
+POLICY = DIST / "openapi_breaking_policy.json"
+SHIPPED_REPORT = DIST / "openapi_diff_report.json"
+
+# Optional provenance inputs
+CLASSIFIER_MANIFEST = DIST / "openapi_classifier_manifest.json"
+
+
+def _load_json(path: Path) -> Any:
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _canon(obj: Any) -> str:
+    # Canonical JSON for stable compare (formatting-insensitive).
+    return json.dumps(obj, sort_keys=True, separators=(",", ":"))
+
+
+def _require(path: Path) -> None:
+    if not path.exists():
+        raise RuntimeError(f"Missing required dist file: {path.relative_to(ROOT)}")
+
+
+def _recompute_report(before: Dict[str, Any], after: Dict[str, Any], policy: Dict[str, Any]) -> Dict[str, Any]:
+    """
+    Re-run the OpenAPI breaking-change classifier purely from in-repo code and dist snapshots.
+
+    This function assumes the classifier implementation lives under:
+      src/code_audit/web_api/openapi_normalize.py
+      src/code_audit/web_api/openapi_diff.py
+      src/code_audit/web_api/schema_semver.py
+
+    And that your policy application logic is part of the diff pipeline (or exposed in openapi_diff).
+    Adjust imports to match your actual module paths.
+    """
+    # Local imports to ensure we test the installed code in the repo checkout.
+    from code_audit.web_api.openapi_normalize import openapi_normalize  # type: ignore
+    from code_audit.web_api.openapi_diff import openapi_diff  # type: ignore
+
+    # Normalize both docs (if your pipeline normalizes before diff)
+    before_n = openapi_normalize(before)
+    after_n = openapi_normalize(after)
+
+    # Compute diff report with policy applied.
+    # Expected: openapi_diff(..., policy=policy) returns the full report dict.
+    report = openapi_diff(before_n, after_n, policy=policy)
+    if not isinstance(report, dict):
+        raise RuntimeError("openapi_diff() did not return a dict report")
+    return report
+
+
+def main(argv: list[str]) -> int:
+    # Usage:
+    #   python scripts/reproduce_openapi_diff_from_dist.py
+    # Optional:
+    #   python scripts/reproduce_openapi_diff_from_dist.py --print
+    do_print = "--print" in argv
+
+    _require(BEFORE)
+    _require(AFTER)
+    _require(POLICY)
+    _require(SHIPPED_REPORT)
+
+    before = _load_json(BEFORE)
+    after = _load_json(AFTER)
+    policy = _load_json(POLICY)
+    shipped = _load_json(SHIPPED_REPORT)
+
+    if not isinstance(before, dict) or not isinstance(after, dict) or not isinstance(policy, dict) or not isinstance(shipped, dict):
+        print("[repro-openapi] FAIL: expected JSON objects for inputs/reports", file=sys.stderr)
+        return 1
+
+    recomputed = _recompute_report(before, after, policy)
+
+    # Canonical compare (format-insensitive).
+    shipped_s = _canon(shipped)
+    recomputed_s = _canon(recomputed)
+
+    if shipped_s != recomputed_s:
+        print("[repro-openapi] FAIL: recomputed report does not match shipped dist/openapi_diff_report.json", file=sys.stderr)
+        # Emit a minimal hint without dumping huge content by default.
+        print(f"[repro-openapi] shipped len={len(shipped_s)} recomputed len={len(recomputed_s)}", file=sys.stderr)
+        if do_print:
+            print("---- shipped (canonical) ----", file=sys.stderr)
+            print(shipped_s, file=sys.stderr)
+            print("---- recomputed (canonical) ----", file=sys.stderr)
+            print(recomputed_s, file=sys.stderr)
+        return 1
+
+    # Optional: provenance sanity checks (non-fatal unless you want them fatal)
+    if CLASSIFIER_MANIFEST.exists():
+        # You can extend this to check that shipped report's classifier fingerprint matches manifest.
+        # Keeping this as a hook: the hard provenance checks already exist in BOM consistency gate.
+        pass
+
+    print("[repro-openapi] OK: recomputed report matches shipped dist/openapi_diff_report.json")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main(sys.argv))
Notes / intentional choices
•	Uses canonical JSON compare to avoid meaningless whitespace diffs.
•	Doesn’t write anything to disk by default (keeps dist/ pristine).
•	Has an optional --print for deep debugging.
One integration assumption
This assumes your code exposes:
•	openapi_normalize(before_dict) -> dict
•	openapi_diff(before_dict, after_dict, policy=policy_dict) -> dict
If your function names differ, adjust the imports and call sites—but the contract of this script remains the same.
________________________________________
Diff B — Wire it into the release workflow as a hard gate
Patch: .github/workflows/release.yml
Run after check_openapi_release_gate.py (which generates dist/openapi_before.json, dist/openapi_after.json, and dist/openapi_diff_report.json) and after generate_release_bom.py is fine too, but the earliest is better.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index bbbbbbb..ccccccc 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: OpenAPI release gate (produces dist/openapi_diff_report.json + snapshots)
         env:
           CI: "true"
         run: |
           python scripts/check_openapi_release_gate.py
+
+      - name: Reproduce OpenAPI diff report from dist (self-verify gate)
+        env:
+          CI: "true"
+        run: |
+          python scripts/reproduce_openapi_diff_from_dist.py
Now, if anyone changes:
•	normalization behavior
•	diff semantics
•	policy application
•	report formatting/field set
…without updating the shipped report generation path, this will fail.
________________________________________
Diff C — (Optional) Add a pytest for local enforcement
If you want this gate to run in normal CI too (not only tags), add a test that:
•	generates a tiny synthetic before/after pair and policy in a temp dist-like folder
•	calls _recompute_report and checks stable output
But for “release only” verification, wiring into tag workflow is typically enough.
________________________________________
Next diff (ultra-tier)
next diff: make reproduce_openapi_diff_from_dist.py also assert the BOM’s hashes are correct
So a single script can:
•	validate release_bom.json against dist/release_bom.schema.json
•	validate BOM hashes for all dist artifacts
•	reproduce diff report and compare
That becomes a one-command “audit this release payload” tool.
Ship next diff: make reproduce_openapi_diff_from_dist.py also assert the BOM’s hashes are correct So a single script can:
next diff: make reproduce_openapi_diff_from_dist.py also assert the BOM’s hashes are correct (single-command release audit)
This upgrades the replay script into a one-command release payload auditor:
python scripts/reproduce_openapi_diff_from_dist.py
Now it will:
1.	Validate dist/release_bom.json against the shipped schema dist/release_bom.schema.json
2.	Verify all BOM-declared artifact hashes match the actual dist/* files (including canonical schema hash)
3.	Recompute OpenAPI diff report from dist snapshots + dist policy and assert it matches the shipped dist/openapi_diff_report.json
If any step fails, it exits nonzero with actionable diagnostics.
________________________________________
Diff A — Expand the script: BOM schema validation + BOM hash verification
Patch: scripts/reproduce_openapi_diff_from_dist.py
Key additions:
•	Require: dist/release_bom.json, dist/release_bom.schema.json
•	Validate BOM JSON against schema (Draft 2020-12)
•	Verify hash contracts:
o	openapi_before_snapshot.sha256
o	openapi_after_snapshot.sha256
o	openapi_diff_report.sha256
o	openapi_classifier_manifest.sha256
o	openapi_breaking_policy.sha256
o	release_bom_schema.canonical_sha256 (canonical JSON hashing)
•	Optionally verify “selected classifier fingerprint” matches the dist classifier manifest (strong coherence)
diff --git a/scripts/reproduce_openapi_diff_from_dist.py b/scripts/reproduce_openapi_diff_from_dist.py
index 1111111..2222222 100644
--- a/scripts/reproduce_openapi_diff_from_dist.py
+++ b/scripts/reproduce_openapi_diff_from_dist.py
@@ -1,13 +1,16 @@
 from __future__ import annotations
 
+import hashlib
 import json
 import sys
 from pathlib import Path
 from typing import Any, Dict
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
 
 BEFORE = DIST / "openapi_before.json"
 AFTER = DIST / "openapi_after.json"
 POLICY = DIST / "openapi_breaking_policy.json"
 SHIPPED_REPORT = DIST / "openapi_diff_report.json"
 
-# Optional provenance inputs
 CLASSIFIER_MANIFEST = DIST / "openapi_classifier_manifest.json"
+
+BOM = DIST / "release_bom.json"
+BOM_SCHEMA = DIST / "release_bom.schema.json"
 
@@
 def _load_json(path: Path) -> Any:
     return json.loads(path.read_text(encoding="utf-8"))
 
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(1024 * 64), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def _canonical_json_bytes(obj: Any) -> bytes:
+    return json.dumps(obj, sort_keys=True, separators=(",", ":")).encode("utf-8")
+
+def _sha256_canonical_json_file(path: Path) -> str:
+    obj = _load_json(path)
+    return hashlib.sha256(_canonical_json_bytes(obj)).hexdigest()
+
@@
 def _canon(obj: Any) -> str:
     # Canonical JSON for stable compare (formatting-insensitive).
     return json.dumps(obj, sort_keys=True, separators=(",", ":"))
 
@@
 def _require(path: Path) -> None:
     if not path.exists():
         raise RuntimeError(f"Missing required dist file: {path.relative_to(ROOT)}")
 
+def _validate_bom_schema(bom: Dict[str, Any], schema: Dict[str, Any]) -> list[str]:
+    """
+    Validate BOM against shipped schema.
+    Returns list of error strings (empty means OK).
+    """
+    try:
+        import jsonschema
+    except Exception as e:  # pragma: no cover
+        return [
+            "jsonschema is required to validate release_bom.json against the shipped schema.",
+            "Install jsonschema in the environment used to audit releases.",
+            f"Import error: {e!r}",
+        ]
+
+    v = jsonschema.Draft202012Validator(schema)
+    errs = sorted(v.iter_errors(bom), key=lambda e: list(e.path))
+    out: list[str] = []
+    for e in errs[:50]:
+        path = ".".join([str(p) for p in e.path]) or "<root>"
+        out.append(f"{path}: {e.message}")
+    if len(errs) > 50:
+        out.append(f"... ({len(errs) - 50} more)")
+    return out
+
+def _bom_artifact_path(root: Path, bom_path_str: str) -> Path:
+    # BOM paths are repo-relative, e.g. "dist/openapi_after.json"
+    p = root / bom_path_str
+    return p
+
+def _assert_bom_hashes(bom: Dict[str, Any]) -> list[str]:
+    """
+    Verify BOM-declared hashes against dist payload files.
+    Returns list of error strings.
+    """
+    errors: list[str] = []
+
+    artifacts = bom.get("artifacts") or {}
+    if not isinstance(artifacts, dict):
+        return ["BOM: artifacts must be an object"]
+
+    def _need(name: str) -> Dict[str, Any]:
+        v = artifacts.get(name)
+        if not isinstance(v, dict):
+            errors.append(f"BOM: artifacts.{name} missing or not an object")
+            return {}
+        return v
+
+    # Required artifacts (dist-only release payload)
+    rbs = _need("release_bom_schema")
+    pol = _need("openapi_breaking_policy")
+    bef = _need("openapi_before_snapshot")
+    aft = _need("openapi_after_snapshot")
+    rep = _need("openapi_diff_report")
+    man = _need("openapi_classifier_manifest")
+
+    # Check schema canonical hash + identity
+    if rbs:
+        p = _bom_artifact_path(ROOT, rbs.get("path") or "")
+        if not p.exists():
+            errors.append(f"BOM: release_bom_schema.path not found: {rbs.get('path')!r}")
+        else:
+            got = _sha256_canonical_json_file(p)
+            if rbs.get("canonical_sha256") != got:
+                errors.append("BOM mismatch: release_bom_schema.canonical_sha256 does not match dist schema file")
+            schema_obj = _load_json(p)
+            if rbs.get("$id") != schema_obj.get("$id"):
+                errors.append("BOM mismatch: release_bom_schema.$id does not match schema $id")
+            if rbs.get("schema_version") != schema_obj.get("schema_version"):
+                errors.append("BOM mismatch: release_bom_schema.schema_version does not match schema_version")
+
+    # Check file sha256 artifacts
+    def _check_file_sha(artifact: Dict[str, Any], field: str, label: str) -> None:
+        if not artifact:
+            return
+        path_str = artifact.get("path")
+        if not isinstance(path_str, str) or not path_str:
+            errors.append(f"BOM: {label}.path missing/invalid")
+            return
+        p = _bom_artifact_path(ROOT, path_str)
+        if not p.exists():
+            errors.append(f"BOM: {label}.path not found: {path_str!r}")
+            return
+        got = _sha256_file(p)
+        want = artifact.get(field)
+        if want != got:
+            errors.append(f"BOM mismatch: {label}.{field} does not match file sha256 ({path_str})")
+
+    _check_file_sha(pol, "sha256", "openapi_breaking_policy")
+    _check_file_sha(bef, "sha256", "openapi_before_snapshot")
+    _check_file_sha(aft, "sha256", "openapi_after_snapshot")
+    _check_file_sha(rep, "sha256", "openapi_diff_report")
+    _check_file_sha(man, "sha256", "openapi_classifier_manifest")
+
+    # Optional: verify classifier "selected" fingerprint matches manifest content
+    if man:
+        sel = (man.get("selected") or {})
+        # Note: "selected" lives under openapi_classifier_manifest artifact in BOM, not inside manifest file.
+        # The BOM includes selected fingerprint; the manifest file includes closure_graph + files map.
+        if isinstance(sel, dict):
+            mpath_str = man.get("path")
+            if isinstance(mpath_str, str) and mpath_str:
+                mp = _bom_artifact_path(ROOT, mpath_str)
+                if mp.exists():
+                    mf = _load_json(mp)
+                    edges_sha = (mf.get("closure_graph") or {}).get("edges_sha256")
+                    if sel.get("closure_graph_edges_sha256") and edges_sha and sel.get("closure_graph_edges_sha256") != edges_sha:
+                        errors.append("BOM mismatch: selected.closure_graph_edges_sha256 does not match classifier manifest edges_sha256")
+                    # Verify per-file hashes if present
+                    sel_files = sel.get("files_ast_sha256")
+                    closure = mf.get("closure_files") or []
+                    files_map = mf.get("files") or {}
+                    if isinstance(sel_files, dict) and isinstance(closure, list) and isinstance(files_map, dict):
+                        for rel in closure:
+                            mh = (files_map.get(rel) or {}).get("ast_sha256")
+                            bh = sel_files.get(rel)
+                            if isinstance(mh, str) and isinstance(bh, str) and mh != bh:
+                                errors.append(f"BOM mismatch: selected.files_ast_sha256 differs for {rel}")
+
+    return errors
+
@@
 def _recompute_report(before: Dict[str, Any], after: Dict[str, Any], policy: Dict[str, Any]) -> Dict[str, Any]:
@@
     return report
 
 
 def main(argv: list[str]) -> int:
@@
     do_print = "--print" in argv
 
     _require(BEFORE)
     _require(AFTER)
     _require(POLICY)
     _require(SHIPPED_REPORT)
+    _require(BOM)
+    _require(BOM_SCHEMA)
+    _require(CLASSIFIER_MANIFEST)
 
     before = _load_json(BEFORE)
     after = _load_json(AFTER)
     policy = _load_json(POLICY)
     shipped = _load_json(SHIPPED_REPORT)
+    bom = _load_json(BOM)
+    schema = _load_json(BOM_SCHEMA)
 
     if not isinstance(before, dict) or not isinstance(after, dict) or not isinstance(policy, dict) or not isinstance(shipped, dict):
         print("[repro-openapi] FAIL: expected JSON objects for inputs/reports", file=sys.stderr)
         return 1
+    if not isinstance(bom, dict) or not isinstance(schema, dict):
+        print("[repro-openapi] FAIL: expected JSON objects for BOM/schema", file=sys.stderr)
+        return 1
+
+    # 1) Validate BOM against shipped schema
+    schema_errors = _validate_bom_schema(bom, schema)
+    if schema_errors:
+        print("[repro-openapi] FAIL: release_bom.json does not validate against dist/release_bom.schema.json", file=sys.stderr)
+        for e in schema_errors:
+            print(f"  - {e}", file=sys.stderr)
+        return 1
+
+    # 2) Verify BOM hashes match dist payload
+    bom_hash_errors = _assert_bom_hashes(bom)
+    if bom_hash_errors:
+        print("[repro-openapi] FAIL: BOM hash verification failed", file=sys.stderr)
+        for e in bom_hash_errors:
+            print(f"  - {e}", file=sys.stderr)
+        return 1
 
     recomputed = _recompute_report(before, after, policy)
 
     # Canonical compare (format-insensitive).
     shipped_s = _canon(shipped)
     recomputed_s = _canon(recomputed)
 
     if shipped_s != recomputed_s:
         print("[repro-openapi] FAIL: recomputed report does not match shipped dist/openapi_diff_report.json", file=sys.stderr)
         # Emit a minimal hint without dumping huge content by default.
         print(f"[repro-openapi] shipped len={len(shipped_s)} recomputed len={len(recomputed_s)}", file=sys.stderr)
         if do_print:
             print("---- shipped (canonical) ----", file=sys.stderr)
             print(shipped_s, file=sys.stderr)
             print("---- recomputed (canonical) ----", file=sys.stderr)
             print(recomputed_s, file=sys.stderr)
         return 1
 
-    # Optional: provenance sanity checks (non-fatal unless you want them fatal)
-    if CLASSIFIER_MANIFEST.exists():
-        # You can extend this to check that shipped report's classifier fingerprint matches manifest.
-        # Keeping this as a hook: the hard provenance checks already exist in BOM consistency gate.
-        pass
-
-    print("[repro-openapi] OK: recomputed report matches shipped dist/openapi_diff_report.json")
+    print("[repro-openapi] OK: BOM validates, BOM hashes match dist payload, and recomputed diff matches shipped report")
     return 0
Important note about dependency
This script now imports jsonschema. For tag workflows (and for third-party auditing), that’s reasonable; just ensure your release workflow installs it (either via dev deps or direct pip install).
________________________________________
Diff B — Wire this script as the single “audit dist” hard gate in release workflow
Since the script now subsumes multiple checks, you can either:
Option 1 (cleanest): replace separate steps with the one script
Keep your existing checks if you want redundancy, but the “single command auditor” is the point.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index ccccccc..ddddddd 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: OpenAPI release gate (produces dist/* snapshots + report)
         env:
           CI: "true"
         run: |
           python scripts/check_openapi_release_gate.py
 
       - name: Generate release BOM (copies schema/policy/manifest into dist)
         env:
           CI: "true"
         run: |
           python scripts/generate_release_bom.py
 
-      - name: Validate release BOM against schema (hard gate)
-        env:
-          CI: "true"
-        run: |
-          python scripts/validate_release_bom.py dist/release_bom.json
-
-      - name: Release BOM consistency check (hard gate)
-        env:
-          CI: "true"
-        run: |
-          python scripts/check_release_bom_consistency.py
-
-      - name: Reproduce OpenAPI diff report from dist (self-verify gate)
+      - name: Audit dist payload (BOM schema + hashes + diff reproduction)
         env:
           CI: "true"
         run: |
           python scripts/reproduce_openapi_diff_from_dist.py
This reduces drift risk by centralizing the audit surface into one place.
Option 2: keep existing checks and add this as a final capstone
That’s “defense in depth,” but longer CI.
________________________________________
What the one script guarantees
With this diff, a single command ensures:
✅ BOM is structurally valid (schema)
✅ BOM hashes accurately attest to the exact dist/* payload
✅ OpenAPI diff report is reproducible from shipped inputs (before/after snapshots + policy)
✅ Report is deterministic (canonical JSON compare)
That’s “release is self-verifying” at maximum governance.
________________________________________
Next diff (last polishing move)
next diff: print a minimal structured failure report (e.g., JSON with kind, path, expected, got) so CI logs are machine-parsable, and optionally attach it as dist/audit_failure.json when failing in CI (without polluting green runs).
Ship next diff: print a minimal structured failure report (e.g., JSON with kind, path, expected, got) so CI logs are machine-parsable, and optionally attach it as dist/audit_failure.json when failing in CI (without polluting green runs).
next diff: structured failure report + optional dist/audit_failure.json on failure (no green pollution)
We’ll upgrade scripts/reproduce_openapi_diff_from_dist.py so that any failure produces:
1.	Machine-parsable JSON printed to stderr (single object or NDJSON lines) with fields:
•	kind (e.g., bom_schema_validation, bom_hash_mismatch, diff_report_mismatch)
•	path (artifact or JSON pointer-ish path)
•	expected
•	got
•	details (optional)
•	timestamp_utc (optional; if you want zero volatility, omit)
•	ci boolean (derived from env)
2.	Optional file write: if CI=true, also write dist/audit_failure.json with the same payload.
•	Only written on failure
•	Never written on success
•	Safe for CI artifact upload if desired
This makes failures searchable, dedupe-able, and easy to triage automatically.
________________________________________
Diff A — Add structured error helpers + unify failure exits
Patch: scripts/reproduce_openapi_diff_from_dist.py
Add:
•	_emit_failure(payload, write_path_if_ci=True)
•	_fail(kind, path, expected, got, details=None)
•	convert existing ad-hoc prints into structured events
•	support --no-write if you ever want to suppress writing the file even in CI (optional)
diff --git a/scripts/reproduce_openapi_diff_from_dist.py b/scripts/reproduce_openapi_diff_from_dist.py
index 2222222..3333333 100644
--- a/scripts/reproduce_openapi_diff_from_dist.py
+++ b/scripts/reproduce_openapi_diff_from_dist.py
@@ -1,13 +1,15 @@
 from __future__ import annotations
 
 import hashlib
 import json
+import os
 import sys
 from pathlib import Path
-from typing import Any, Dict
+from typing import Any, Dict, Optional, List
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
@@
 BOM = DIST / "release_bom.json"
 BOM_SCHEMA = DIST / "release_bom.schema.json"
+
+AUDIT_FAILURE = DIST / "audit_failure.json"
@@
 def _load_json(path: Path) -> Any:
     return json.loads(path.read_text(encoding="utf-8"))
@@
 def _require(path: Path) -> None:
     if not path.exists():
-        raise RuntimeError(f"Missing required dist file: {path.relative_to(ROOT)}")
+        raise RuntimeError(f"Missing required dist file: {path.relative_to(ROOT)}")
+
+
+def _is_ci_true() -> bool:
+    return (os.environ.get("CI") or "").strip().lower() == "true"
+
+
+def _emit_failure(payload: Dict[str, Any], *, write_file: bool) -> None:
+    """
+    Emit a single machine-parsable JSON object to stderr.
+    Optionally write dist/audit_failure.json (failure-only).
+    """
+    s = json.dumps(payload, sort_keys=True)
+    print(s, file=sys.stderr)
+    if write_file:
+        DIST.mkdir(parents=True, exist_ok=True)
+        AUDIT_FAILURE.write_text(json.dumps(payload, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+
+
+def _fail(
+    *,
+    kind: str,
+    path: str,
+    expected: Any = None,
+    got: Any = None,
+    details: Optional[Any] = None,
+    exit_code: int = 1,
+    write_file: bool,
+) -> int:
+    payload: Dict[str, Any] = {
+        "kind": kind,
+        "path": path,
+        "expected": expected,
+        "got": got,
+        "details": details,
+        "ci": _is_ci_true(),
+    }
+    _emit_failure(payload, write_file=write_file)
+    return exit_code
@@
 def _validate_bom_schema(bom: Dict[str, Any], schema: Dict[str, Any]) -> list[str]:
@@
     return out
@@
-def _assert_bom_hashes(bom: Dict[str, Any]) -> list[str]:
+def _assert_bom_hashes(bom: Dict[str, Any]) -> list[Dict[str, Any]]:
     """
     Verify BOM-declared hashes against dist payload files.
-    Returns list of error strings.
+    Returns list of structured error objects.
     """
-    errors: list[str] = []
+    errors: list[Dict[str, Any]] = []
@@
     artifacts = bom.get("artifacts") or {}
     if not isinstance(artifacts, dict):
-        return ["BOM: artifacts must be an object"]
+        return [
+            {"kind": "bom_invalid", "path": "release_bom.json:artifacts", "expected": "object", "got": type(artifacts).__name__}
+        ]
@@
     def _need(name: str) -> Dict[str, Any]:
         v = artifacts.get(name)
         if not isinstance(v, dict):
-            errors.append(f"BOM: artifacts.{name} missing or not an object")
+            errors.append({"kind": "bom_missing_artifact", "path": f"release_bom.json:artifacts.{name}", "expected": "object", "got": v})
             return {}
         return v
@@
     if rbs:
         p = _bom_artifact_path(ROOT, rbs.get("path") or "")
         if not p.exists():
-            errors.append(f"BOM: release_bom_schema.path not found: {rbs.get('path')!r}")
+            errors.append({
+                "kind": "bom_path_missing",
+                "path": "release_bom.json:artifacts.release_bom_schema.path",
+                "expected": "existing file",
+                "got": rbs.get("path"),
+            })
         else:
             got = _sha256_canonical_json_file(p)
             if rbs.get("canonical_sha256") != got:
-                errors.append("BOM mismatch: release_bom_schema.canonical_sha256 does not match dist schema file")
+                errors.append({
+                    "kind": "bom_hash_mismatch",
+                    "path": "release_bom.json:artifacts.release_bom_schema.canonical_sha256",
+                    "expected": rbs.get("canonical_sha256"),
+                    "got": got,
+                })
             schema_obj = _load_json(p)
             if rbs.get("$id") != schema_obj.get("$id"):
-                errors.append("BOM mismatch: release_bom_schema.$id does not match schema $id")
+                errors.append({
+                    "kind": "bom_identity_mismatch",
+                    "path": "release_bom.json:artifacts.release_bom_schema.$id",
+                    "expected": rbs.get("$id"),
+                    "got": schema_obj.get("$id"),
+                })
             if rbs.get("schema_version") != schema_obj.get("schema_version"):
-                errors.append("BOM mismatch: release_bom_schema.schema_version does not match schema_version")
+                errors.append({
+                    "kind": "bom_identity_mismatch",
+                    "path": "release_bom.json:artifacts.release_bom_schema.schema_version",
+                    "expected": rbs.get("schema_version"),
+                    "got": schema_obj.get("schema_version"),
+                })
@@
     def _check_file_sha(artifact: Dict[str, Any], field: str, label: str) -> None:
         if not artifact:
             return
         path_str = artifact.get("path")
         if not isinstance(path_str, str) or not path_str:
-            errors.append(f"BOM: {label}.path missing/invalid")
+            errors.append({
+                "kind": "bom_invalid",
+                "path": f"release_bom.json:artifacts.{label}.path",
+                "expected": "non-empty string",
+                "got": path_str,
+            })
             return
         p = _bom_artifact_path(ROOT, path_str)
         if not p.exists():
-            errors.append(f"BOM: {label}.path not found: {path_str!r}")
+            errors.append({
+                "kind": "bom_path_missing",
+                "path": f"release_bom.json:artifacts.{label}.path",
+                "expected": "existing file",
+                "got": path_str,
+            })
             return
         got = _sha256_file(p)
         want = artifact.get(field)
         if want != got:
-            errors.append(f"BOM mismatch: {label}.{field} does not match file sha256 ({path_str})")
+            errors.append({
+                "kind": "bom_hash_mismatch",
+                "path": f"release_bom.json:artifacts.{label}.{field}",
+                "expected": want,
+                "got": got,
+                "details": {"file": path_str},
+            })
@@
                     if sel.get("closure_graph_edges_sha256") and edges_sha and sel.get("closure_graph_edges_sha256") != edges_sha:
-                        errors.append("BOM mismatch: selected.closure_graph_edges_sha256 does not match classifier manifest edges_sha256")
+                        errors.append({
+                            "kind": "bom_selected_mismatch",
+                            "path": "release_bom.json:artifacts.openapi_classifier_manifest.selected.closure_graph_edges_sha256",
+                            "expected": sel.get("closure_graph_edges_sha256"),
+                            "got": edges_sha,
+                        })
@@
                             if isinstance(mh, str) and isinstance(bh, str) and mh != bh:
-                                errors.append(f"BOM mismatch: selected.files_ast_sha256 differs for {rel}")
+                                errors.append({
+                                    "kind": "bom_selected_mismatch",
+                                    "path": f"release_bom.json:artifacts.openapi_classifier_manifest.selected.files_ast_sha256[{rel}]",
+                                    "expected": bh,
+                                    "got": mh,
+                                })
 
     return errors
@@
 def main(argv: list[str]) -> int:
@@
-    do_print = "--print" in argv
+    do_print = "--print" in argv
+    no_write = "--no-write" in argv
+    write_failure_file = _is_ci_true() and (not no_write)
@@
     _require(BEFORE)
@@
     _require(CLASSIFIER_MANIFEST)
@@
     if not isinstance(before, dict) or not isinstance(after, dict) or not isinstance(policy, dict) or not isinstance(shipped, dict):
-        print("[repro-openapi] FAIL: expected JSON objects for inputs/reports", file=sys.stderr)
-        return 1
+        return _fail(
+            kind="input_type_error",
+            path="dist inputs",
+            expected="JSON object for before/after/policy/report",
+            got={"before": type(before).__name__, "after": type(after).__name__, "policy": type(policy).__name__, "report": type(shipped).__name__},
+            write_file=write_failure_file,
+        )
     if not isinstance(bom, dict) or not isinstance(schema, dict):
-        print("[repro-openapi] FAIL: expected JSON objects for BOM/schema", file=sys.stderr)
-        return 1
+        return _fail(
+            kind="input_type_error",
+            path="dist BOM/schema",
+            expected="JSON object for BOM/schema",
+            got={"bom": type(bom).__name__, "schema": type(schema).__name__},
+            write_file=write_failure_file,
+        )
 
     # 1) Validate BOM against shipped schema
     schema_errors = _validate_bom_schema(bom, schema)
     if schema_errors:
-        print("[repro-openapi] FAIL: release_bom.json does not validate against dist/release_bom.schema.json", file=sys.stderr)
-        for e in schema_errors:
-            print(f"  - {e}", file=sys.stderr)
-        return 1
+        return _fail(
+            kind="bom_schema_validation",
+            path="dist/release_bom.json",
+            expected="valid per dist/release_bom.schema.json",
+            got="validation errors",
+            details=schema_errors,
+            write_file=write_failure_file,
+        )
 
     # 2) Verify BOM hashes match dist payload
     bom_hash_errors = _assert_bom_hashes(bom)
     if bom_hash_errors:
-        print("[repro-openapi] FAIL: BOM hash verification failed", file=sys.stderr)
-        for e in bom_hash_errors:
-            print(f"  - {e}", file=sys.stderr)
-        return 1
+        return _fail(
+            kind="bom_hash_verification",
+            path="dist/*",
+            expected="BOM hashes match payload",
+            got="hash mismatches",
+            details=bom_hash_errors,
+            write_file=write_failure_file,
+        )
@@
     if shipped_s != recomputed_s:
-        print("[repro-openapi] FAIL: recomputed report does not match shipped dist/openapi_diff_report.json", file=sys.stderr)
-        # Emit a minimal hint without dumping huge content by default.
-        print(f"[repro-openapi] shipped len={len(shipped_s)} recomputed len={len(recomputed_s)}", file=sys.stderr)
-        if do_print:
-            print("---- shipped (canonical) ----", file=sys.stderr)
-            print(shipped_s, file=sys.stderr)
-            print("---- recomputed (canonical) ----", file=sys.stderr)
-            print(recomputed_s, file=sys.stderr)
-        return 1
+        details: Dict[str, Any] = {
+            "shipped_len": len(shipped_s),
+            "recomputed_len": len(recomputed_s),
+        }
+        if do_print:
+            details["shipped_canonical"] = shipped_s
+            details["recomputed_canonical"] = recomputed_s
+        return _fail(
+            kind="diff_report_mismatch",
+            path="dist/openapi_diff_report.json",
+            expected="recomputed canonical report equals shipped canonical report",
+            got="mismatch",
+            details=details,
+            write_file=write_failure_file,
+        )
 
     print("[repro-openapi] OK: BOM validates, BOM hashes match dist payload, and recomputed diff matches shipped report")
     return 0
What changes in output
Instead of multiple human prints, any failure produces exactly one JSON blob on stderr like:
{
  "ci": true,
  "details": [
    {"kind":"bom_hash_mismatch","path":"release_bom.json:artifacts.openapi_after_snapshot.sha256","expected":"...","got":"...","details":{"file":"dist/openapi_after.json"}}
  ],
  "expected": "BOM hashes match payload",
  "got": "hash mismatches",
  "kind": "bom_hash_verification",
  "path": "dist/*"
}
And (only in CI, only on failure) it writes the same payload to:
•	dist/audit_failure.json
________________________________________
Diff B — Release workflow optionally uploads dist/audit_failure.json on failure
GitHub Actions supports “always upload” artifacts, but you asked not to pollute green runs. The clean pattern:
•	Add an always-run upload step that uploads dist/audit_failure.json only if it exists.
•	That step runs even if earlier steps fail (if: always()), but checks file existence.
Patch: .github/workflows/release.yml
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index ddddddd..eeeeeee 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Audit dist payload (BOM schema + hashes + diff reproduction)
         env:
           CI: "true"
         run: |
           python scripts/reproduce_openapi_diff_from_dist.py
+
+      - name: Upload audit failure report (only if present)
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: release-audit-failure
+          path: dist/audit_failure.json
+          if-no-files-found: ignore
This gives you a clickable artifact when the job fails, without producing anything on green.
________________________________________
Optional micro-tightening (if you want even cleaner machine parsing)
Instead of a single payload, you can emit:
•	one top-level failure payload, plus
•	optionally NDJSON for each sub-error in details.
But the single structured JSON object is usually enough and keeps logs simple.
________________________________________
Next diff (operator-tier polish)
next diff: add stable “kind enum” contract + schema for audit_failure.json
So CI tooling can rely on a fixed set of kind values, and you can validate dist/audit_failure.json structure the same way you validate other artifacts.
Ship next diff: add stable “kind enum” contract + schema for audit_failure.json
next diff: stable kind enum contract + schema for dist/audit_failure.json
This turns the audit failure payload into a governed, machine-consumable contract:
•	Add schemas/release_audit_failure.schema.json
•	Define a stable enum for kind (top-level + nested detail items)
•	Add a validator script scripts/validate_release_audit_failure.py
•	Wire the validator into the workflow (only when the file exists)
•	(Optional but recommended) add a small test that ensures kind values stay in the schema enum
This keeps the CI parsing surface stable over time.
________________________________________
Diff A — Add audit failure schema
New file: schemas/release_audit_failure.schema.json
This schema is strict, but flexible enough to include extra per-kind details in a controlled way.
diff --git a/schemas/release_audit_failure.schema.json b/schemas/release_audit_failure.schema.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/schemas/release_audit_failure.schema.json
@@ -0,0 +1,189 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "$id": "release_audit_failure_schema_v1",
+  "schema_version": "release_audit_failure_schema_v1",
+  "title": "Release Audit Failure Schema",
+  "type": "object",
+  "additionalProperties": false,
+  "required": ["kind", "path", "expected", "got", "ci"],
+  "properties": {
+    "kind": { "$ref": "#/$defs/kind" },
+    "path": { "type": "string", "minLength": 1 },
+    "expected": {},
+    "got": {},
+    "details": {},
+    "ci": { "type": "boolean" }
+  },
+  "$defs": {
+    "kind": {
+      "type": "string",
+      "enum": [
+        "input_type_error",
+        "bom_schema_validation",
+        "bom_hash_verification",
+        "diff_report_mismatch",
+
+        "bom_invalid",
+        "bom_missing_artifact",
+        "bom_path_missing",
+        "bom_hash_mismatch",
+        "bom_identity_mismatch",
+        "bom_selected_mismatch"
+      ]
+    },
+    "detail_item": {
+      "type": "object",
+      "additionalProperties": true,
+      "required": ["kind", "path"],
+      "properties": {
+        "kind": { "$ref": "#/$defs/kind" },
+        "path": { "type": "string", "minLength": 1 },
+        "expected": {},
+        "got": {},
+        "details": {}
+      }
+    },
+    "details_list": {
+      "type": "array",
+      "items": { "$ref": "#/$defs/detail_item" }
+    }
+  }
+}
Notes on schema choices
•	Top-level details is untyped ({}) because:
o	bom_schema_validation uses a list of strings
o	diff_report_mismatch uses a dict of lengths / optional canonical dumps
o	bom_hash_verification uses a list of structured items
•	If you want to strictly type details per kind, you can do it via oneOf keyed by kind, but it gets noisy. This is a good “tight enough” v1.
________________________________________
Diff B — Add a validator script
New file: scripts/validate_release_audit_failure.py
Validates dist/audit_failure.json against the schema. Intended to run only when the file exists.
diff --git a/scripts/validate_release_audit_failure.py b/scripts/validate_release_audit_failure.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/validate_release_audit_failure.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+import json
+import sys
+from pathlib import Path
+
+try:
+    import jsonschema
+except Exception as e:  # pragma: no cover
+    raise RuntimeError(
+        "jsonschema is required to validate audit_failure.json. "
+        "Install jsonschema in CI/dev environment."
+    ) from e
+
+
+ROOT = Path(__file__).resolve().parents[1]
+SCHEMA = ROOT / "schemas" / "release_audit_failure.schema.json"
+
+
+def _load_json(path: Path):
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def main(argv: list[str]) -> int:
+    if len(argv) != 2:
+        print("Usage: python scripts/validate_release_audit_failure.py dist/audit_failure.json", file=sys.stderr)
+        return 2
+
+    target = ROOT / argv[1]
+    if not SCHEMA.exists():
+        print(f"Missing schema: {SCHEMA.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+    if not target.exists():
+        print(f"Missing file: {target.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+
+    schema = _load_json(SCHEMA)
+    obj = _load_json(target)
+
+    v = jsonschema.Draft202012Validator(schema)
+    errs = sorted(v.iter_errors(obj), key=lambda e: list(e.path))
+    if errs:
+        print("[audit-failure-schema] FAIL: audit_failure.json does not match schema.", file=sys.stderr)
+        for e in errs[:50]:
+            path = ".".join([str(p) for p in e.path]) or "<root>"
+            print(f"  - {path}: {e.message}", file=sys.stderr)
+        if len(errs) > 50:
+            print(f"  ... ({len(errs) - 50} more)", file=sys.stderr)
+        return 1
+
+    print("[audit-failure-schema] OK: audit_failure.json matches schema")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main(sys.argv))
________________________________________
Diff C — Validate audit_failure.json in workflow (only when present)
Patch: .github/workflows/release.yml
We’ll validate the file only if it exists, and do it in an always() step so even failures can be checked.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index eeeeeee..fffffff 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Upload audit failure report (only if present)
         if: always()
         uses: actions/upload-artifact@v4
         with:
           name: release-audit-failure
           path: dist/audit_failure.json
           if-no-files-found: ignore
+
+      - name: Validate audit failure payload schema (only if present)
+        if: always()
+        run: |
+          if [ -f dist/audit_failure.json ]; then
+            python -m pip install jsonschema
+            python scripts/validate_release_audit_failure.py dist/audit_failure.json
+          else
+            echo "No dist/audit_failure.json present; skipping."
+          fi
If you already install jsonschema earlier, remove the extra pip install line.
________________________________________
Diff D — (Optional) Add a test to lock the enum list and prevent accidental ad-hoc kinds
This is helpful because someone might add a new kind string in code and forget to update the schema.
New file: tests/test_release_audit_failure_kind_enum_contract.py
diff --git a/tests/test_release_audit_failure_kind_enum_contract.py b/tests/test_release_audit_failure_kind_enum_contract.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_release_audit_failure_kind_enum_contract.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+
+ROOT = Path(__file__).resolve().parents[1]
+SCHEMA = ROOT / "schemas" / "release_audit_failure.schema.json"
+
+
+def test_release_audit_failure_kind_enum_present_and_nonempty() -> None:
+    assert SCHEMA.exists(), "Missing schemas/release_audit_failure.schema.json"
+    data = json.loads(SCHEMA.read_text(encoding="utf-8"))
+
+    defs = data.get("$defs") or {}
+    assert isinstance(defs, dict)
+    kind = defs.get("kind") or {}
+    assert isinstance(kind, dict)
+    enum = kind.get("enum")
+    assert isinstance(enum, list) and enum, "Schema must define a non-empty $defs.kind.enum list"
+    assert all(isinstance(x, str) and x for x in enum), "All enum entries must be non-empty strings"
+    assert len(enum) == len(set(enum)), "Enum must not contain duplicates"
This doesn’t auto-sync code kinds to schema (that’s heavier), but it enforces the existence/quality of the stable enum set.
________________________________________
Next diff (ties it into your broader governance model)
next diff: include schemas/release_audit_failure.schema.json in the BOM manifest gate
So changes to the audit failure schema require bom_logic_version bump + bom_manifest.json refresh (canonical JSON hash), and optionally ship the schema into dist/ alongside the BOM schema as part of the “audit kit.”
Ship next diff: include schemas/release_audit_failure.schema.json in the BOM manifest gate
next diff: include schemas/release_audit_failure.schema.json in the BOM manifest gate
This extends the BOM contract manifest gate (your bom_logic_version + tests/contracts/bom_manifest.json) to also guard the audit failure schema as a canonical JSON surface. Any semantic change to the audit failure schema will now require:
1.	bump bom_logic_version
2.	refresh tests/contracts/bom_manifest.json via python scripts/refresh_bom_manifest.py
Format-only changes (whitespace/ordering) won’t trigger false positives because we hash the canonical JSON.
________________________________________
Diff A — Add audit failure schema to the BOM manifest’s JSON surface
Patch: tests/contracts/bom_manifest.json
Add a second entry under json_files:
diff --git a/tests/contracts/bom_manifest.json b/tests/contracts/bom_manifest.json
index 2222222..3333333 100644
--- a/tests/contracts/bom_manifest.json
+++ b/tests/contracts/bom_manifest.json
@@ -14,10 +14,18 @@
   },
   "json_files": {
     "schemas/release_bom.schema.json": {
       "canonical_sha256": "REPLACE_ME",
       "canonical_sha256_short": "REPLACE_ME"
+    },
+    "schemas/release_audit_failure.schema.json": {
+      "canonical_sha256": "REPLACE_ME",
+      "canonical_sha256_short": "REPLACE_ME"
     }
   }
 }
________________________________________
Diff B — Extend the BOM manifest refresh script to hash the new schema
Patch: scripts/refresh_bom_manifest.py
Add the audit failure schema path to JSON_TARGET_FILES.
diff --git a/scripts/refresh_bom_manifest.py b/scripts/refresh_bom_manifest.py
index 2222222..3333333 100644
--- a/scripts/refresh_bom_manifest.py
+++ b/scripts/refresh_bom_manifest.py
@@ -22,9 +22,10 @@ TARGET_FILES = [
 ]
 
 JSON_TARGET_FILES = [
     "schemas/release_bom.schema.json",
+    "schemas/release_audit_failure.schema.json",
 ]
Everything else stays the same because it already computes canonical JSON hashes and writes them into json_files[...].
________________________________________
Diff C — Extend the BOM manifest gate test to enforce the audit failure schema too
Patch: tests/test_bom_manifest_gate.py
Add the same schema path to JSON_TARGET_FILES in the test (mirrors the refresh script list).
diff --git a/tests/test_bom_manifest_gate.py b/tests/test_bom_manifest_gate.py
index 2222222..3333333 100644
--- a/tests/test_bom_manifest_gate.py
+++ b/tests/test_bom_manifest_gate.py
@@ -19,9 +19,10 @@ TARGET_FILES = [
 ]
 
 JSON_TARGET_FILES = [
     "schemas/release_bom.schema.json",
+    "schemas/release_audit_failure.schema.json",
 ]
Now CI will fail if either schema changes without:
•	bumping bom_logic_version
•	refreshing the manifest
________________________________________
Diff D — Docs: declare audit failure schema is BOM-governed too
Patch: docs/CONTRACT.md
diff --git a/docs/CONTRACT.md b/docs/CONTRACT.md
index 5555555..6666666 100644
--- a/docs/CONTRACT.md
+++ b/docs/CONTRACT.md
@@
 ### Release BOM contract gate
 
 Scope: release_bom generation + consistency validation.
 
 Guarded files:
 - scripts/generate_release_bom.py
 - scripts/check_release_bom_consistency.py
 - schemas/release_bom.schema.json (canonical JSON hash)
+- schemas/release_audit_failure.schema.json (canonical JSON hash)
 
 On semantic change:
 1) bump `bom_logic_version` in `src/code_audit/model/run_result.py`
 2) refresh manifest:
    `python scripts/refresh_bom_manifest.py`
________________________________________
How the governance behaves after this
If you edit schemas/release_audit_failure.schema.json:
•	CI fails with a “schema drift detected” message from test_bom_manifest_gate.py
•	You must:
1.	bump bom_logic_version
2.	run python scripts/refresh_bom_manifest.py
If you only reformat the schema:
•	canonical JSON hash remains stable
•	no version bump needed
________________________________________
Next diff (recommended to close the loop)
next diff: ship release_audit_failure.schema.json into dist/ and reference it from the BOM
So the release “audit kit” is complete:
•	dist/release_bom.schema.json
•	dist/release_audit_failure.schema.json
•	and the BOM carries canonical hashes for both.
Ship next diff: ship release_audit_failure.schema.json into dist/ and reference it from the BOM So the release “audit kit” is complete:
next diff: ship release_audit_failure.schema.json into dist/ and reference it from the BOM (complete “audit kit”)
This makes the release payload fully self-contained for auditing both success artifacts (BOM/diff/snapshots) and failure artifacts (dist/audit_failure.json) with their validation schema.
After this diff, dist/ will include:
•	release_bom.json
•	release_bom.schema.json
•	release_audit_failure.schema.json
•	openapi_before.json
•	openapi_after.json
•	openapi_diff_report.json
•	openapi_breaking_policy.json
•	openapi_classifier_manifest.json
•	(optional, failure-only) audit_failure.json
…and the BOM will attest to the audit failure schema’s identity + canonical hash.
________________________________________
Diff A — Copy release_audit_failure.schema.json into dist during BOM generation and reference it
Patch: scripts/generate_release_bom.py
Add a schema source path, copy it into dist, compute canonical hash, and emit a new BOM artifact entry release_audit_failure_schema.
diff --git a/scripts/generate_release_bom.py b/scripts/generate_release_bom.py
index 5555555..6666666 100644
--- a/scripts/generate_release_bom.py
+++ b/scripts/generate_release_bom.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import shutil
 from pathlib import Path
 from typing import Any, Dict
 
@@
 ROOT = Path(__file__).resolve().parents[1]
 DIST = ROOT / "dist"
 
 OPENAPI_DIFF_REPORT = DIST / "openapi_diff_report.json"
 DIST_OPENAPI_BEFORE = DIST / "openapi_before.json"
 DIST_OPENAPI_AFTER = DIST / "openapi_after.json"
 
 OPENAPI_CLASSIFIER_MANIFEST = ROOT / "tests" / "contracts" / "openapi_classifier_manifest.json"
 RELEASE_BOM_SCHEMA = ROOT / "schemas" / "release_bom.schema.json"
+AUDIT_FAILURE_SCHEMA = ROOT / "schemas" / "release_audit_failure.schema.json"
 OPENAPI_POLICY = ROOT / "tests" / "contracts" / "openapi_breaking_policy.json"
 
 OUT_BOM = DIST / "release_bom.json"
@@
 def main() -> int:
     DIST.mkdir(parents=True, exist_ok=True)
 
     _require(OPENAPI_DIFF_REPORT, why="OpenAPI diff report must be generated before BOM")
     _require(DIST_OPENAPI_BEFORE, why="OpenAPI before snapshot must exist in dist/ (produced by release gate)")
     _require(DIST_OPENAPI_AFTER, why="OpenAPI after snapshot must exist in dist/ (produced by release gate)")
     _require(OPENAPI_CLASSIFIER_MANIFEST, why="Classifier manifest is part of the release contract surface")
     _require(RELEASE_BOM_SCHEMA, why="Release BOM schema must be present for self-describing releases")
+    _require(AUDIT_FAILURE_SCHEMA, why="Audit failure schema must be present for audit kit completeness")
     _require(OPENAPI_POLICY, why="OpenAPI breaking policy must be present for self-describing releases")
@@
     # Copy schema + policy into dist/ so release uploads only dist/*
     dist_schema = _copy_into_dist(RELEASE_BOM_SCHEMA, "release_bom.schema.json")
+    dist_audit_schema = _copy_into_dist(AUDIT_FAILURE_SCHEMA, "release_audit_failure.schema.json")
     dist_policy = _copy_into_dist(OPENAPI_POLICY, "openapi_breaking_policy.json")
     dist_classifier_manifest = _copy_into_dist(OPENAPI_CLASSIFIER_MANIFEST, "openapi_classifier_manifest.json")
@@
     bom_schema = _load_json(dist_schema)
     bom_schema_sha = _sha256_canonical_json_file(dist_schema)
     bom_schema_id = bom_schema.get("$id")
     bom_schema_version = bom_schema.get("schema_version")
@@
+    audit_schema = _load_json(dist_audit_schema)
+    audit_schema_sha = _sha256_canonical_json_file(dist_audit_schema)
+    audit_schema_id = audit_schema.get("$id")
+    audit_schema_version = audit_schema.get("schema_version")
+    if not isinstance(audit_schema_id, str) or not audit_schema_id:
+        raise RuntimeError("schemas/release_audit_failure.schema.json missing $id")
+    if not isinstance(audit_schema_version, str) or not audit_schema_version:
+        raise RuntimeError("schemas/release_audit_failure.schema.json missing schema_version")
+
     bom: Dict[str, Any] = {
         "version": 1,
         "release_tag": (os.environ.get("GITHUB_REF_NAME") or "").strip() or None,
         "artifacts": {
             "release_bom_schema": {
                 "path": str(dist_schema.relative_to(ROOT)),
                 "schema_version": bom_schema_version,
                 "$id": bom_schema_id,
                 "canonical_sha256": bom_schema_sha,
                 "canonical_sha256_short": bom_schema_sha[:12],
             },
+            "release_audit_failure_schema": {
+                "path": str(dist_audit_schema.relative_to(ROOT)),
+                "schema_version": audit_schema_version,
+                "$id": audit_schema_id,
+                "canonical_sha256": audit_schema_sha,
+                "canonical_sha256_short": audit_schema_sha[:12],
+            },
             "openapi_breaking_policy": {
                 "path": str(dist_policy.relative_to(ROOT)),
                 "sha256": _sha256_file(dist_policy),
                 "sha256_short": _sha256_file(dist_policy)[:12],
             },
             "openapi_before_snapshot": {
                 "path": str(DIST_OPENAPI_BEFORE.relative_to(ROOT)),
                 "sha256": _sha256_file(DIST_OPENAPI_BEFORE),
                 "sha256_short": _sha256_file(DIST_OPENAPI_BEFORE)[:12],
             },
             "openapi_after_snapshot": {
                 "path": str(DIST_OPENAPI_AFTER.relative_to(ROOT)),
                 "sha256": _sha256_file(DIST_OPENAPI_AFTER),
                 "sha256_short": _sha256_file(DIST_OPENAPI_AFTER)[:12],
             },
             "openapi_diff_report": {
                 "path": str(OPENAPI_DIFF_REPORT.relative_to(ROOT)),
                 "sha256": _sha256_file(OPENAPI_DIFF_REPORT),
             },
             "openapi_classifier_manifest": {
                 "path": str(dist_classifier_manifest.relative_to(ROOT)),
                 "sha256": _sha256_bytes(classifier_manifest_bytes),
                 "sha256_short": _sha256_bytes(classifier_manifest_bytes)[:12],
                 "selected": _select_classifier_fingerprint(classifier_manifest),
             }
         },
         "provenance": {
@@
         },
     }
________________________________________
Diff B — Extend the BOM schema to require the new audit failure schema artifact
Patch: schemas/release_bom.schema.json
Add release_audit_failure_schema to artifacts.required and define its structure (same shape as release_bom_schema).
diff --git a/schemas/release_bom.schema.json b/schemas/release_bom.schema.json
index 5555555..6666666 100644
--- a/schemas/release_bom.schema.json
+++ b/schemas/release_bom.schema.json
@@
       "type": "object",
       "additionalProperties": false,
       "required": [
         "release_bom_schema",
+        "release_audit_failure_schema",
         "openapi_breaking_policy",
         "openapi_before_snapshot",
         "openapi_after_snapshot",
         "openapi_diff_report",
         "openapi_classifier_manifest"
       ],
       "properties": {
@@
         "release_bom_schema": {
           "type": "object",
           "additionalProperties": false,
           "required": ["path", "schema_version", "$id", "canonical_sha256", "canonical_sha256_short"],
           "properties": {
             "path": { "type": "string", "pattern": "^dist/release_bom\\.schema\\.json$" },
             "schema_version": { "type": "string", "minLength": 1 },
             "$id": { "type": "string", "minLength": 1 },
             "canonical_sha256": { "type": "string", "pattern": "^[a-f0-9]{64}$" },
             "canonical_sha256_short": { "type": "string", "pattern": "^[a-f0-9]{12}$" }
           }
         },
+        "release_audit_failure_schema": {
+          "type": "object",
+          "additionalProperties": false,
+          "required": ["path", "schema_version", "$id", "canonical_sha256", "canonical_sha256_short"],
+          "properties": {
+            "path": { "type": "string", "pattern": "^dist/release_audit_failure\\.schema\\.json$" },
+            "schema_version": { "type": "string", "minLength": 1 },
+            "$id": { "type": "string", "minLength": 1 },
+            "canonical_sha256": { "type": "string", "pattern": "^[a-f0-9]{64}$" },
+            "canonical_sha256_short": { "type": "string", "pattern": "^[a-f0-9]{12}$" }
+          }
+        },
________________________________________
Diff C — Update BOM hash consistency checker to verify the audit schema too
Patch: scripts/check_release_bom_consistency.py
Add checks similar to release_bom_schema, but for release_audit_failure_schema.
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 5555555..6666666 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
 DIST_POLICY_PATH = ROOT / "dist" / "openapi_breaking_policy.json"
 DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
+DIST_AUDIT_SCHEMA_PATH = ROOT / "dist" / "release_audit_failure.schema.json"
@@
     _req(DIST_POLICY_PATH.exists(), f"Missing {DIST_POLICY_PATH.relative_to(ROOT)}", errors)
     _req(DIST_BOM_SCHEMA_PATH.exists(), f"Missing {DIST_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors)
+    _req(DIST_AUDIT_SCHEMA_PATH.exists(), f"Missing {DIST_AUDIT_SCHEMA_PATH.relative_to(ROOT)}", errors)
@@
-    schema = _load_json(DIST_BOM_SCHEMA_PATH)
+    schema = _load_json(DIST_BOM_SCHEMA_PATH)
+    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
@@
     # --- BOM schema artifact must be present and correct ---
     rbs = artifacts.get("release_bom_schema") or {}
@@
     if rbs.get("schema_version") != schema.get("schema_version"):
         errors.append("BOM mismatch: release_bom_schema.schema_version does not match schema_version")
+
+    # --- Audit failure schema artifact must be present and correct ---
+    ras = artifacts.get("release_audit_failure_schema") or {}
+    _req(isinstance(ras, dict), "BOM: artifacts.release_audit_failure_schema must be an object", errors)
+    if isinstance(ras, dict):
+        if ras.get("path") != "dist/release_audit_failure.schema.json":
+            errors.append("BOM mismatch: release_audit_failure_schema.path must be dist/release_audit_failure.schema.json")
+        audit_sha = _sha256_canonical_json_file(DIST_AUDIT_SCHEMA_PATH)
+        if ras.get("canonical_sha256") != audit_sha:
+            errors.append("BOM mismatch: release_audit_failure_schema.canonical_sha256 does not match dist schema file")
+        if ras.get("$id") != audit_schema.get("$id"):
+            errors.append("BOM mismatch: release_audit_failure_schema.$id does not match schema $id")
+        if ras.get("schema_version") != audit_schema.get("schema_version"):
+            errors.append("BOM mismatch: release_audit_failure_schema.schema_version does not match schema_version")
________________________________________
Diff D — Validate audit_failure.json using the shipped schema in dist
Update the validator script to use the dist copy and make it available for external auditors. You already have scripts/validate_release_audit_failure.py; make its schema path point to dist:
Patch: scripts/validate_release_audit_failure.py
diff --git a/scripts/validate_release_audit_failure.py b/scripts/validate_release_audit_failure.py
index 1111111..2222222 100644
--- a/scripts/validate_release_audit_failure.py
+++ b/scripts/validate_release_audit_failure.py
@@
 ROOT = Path(__file__).resolve().parents[1]
-SCHEMA = ROOT / "schemas" / "release_audit_failure.schema.json"
+SCHEMA = ROOT / "dist" / "release_audit_failure.schema.json"
This matches your “dist-only audit kit” direction.
________________________________________
Diff E — Release workflow: assert dist includes the new schema
Patch: .github/workflows/release.yml
Update the dist preflight list:
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index fffffff..0000000 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Verify dist contains required release payload files
         run: |
           test -f dist/release_bom.json
           test -f dist/release_bom.schema.json
+          test -f dist/release_audit_failure.schema.json
           test -f dist/openapi_diff_report.json
           test -f dist/openapi_breaking_policy.json
           test -f dist/openapi_classifier_manifest.json
           test -f dist/openapi_before.json
           test -f dist/openapi_after.json
           echo "dist payload OK"
Because generate_release_bom.py now copies this schema, this also guarantees that BOM generation ran before release.
________________________________________
Result: the release “audit kit” is complete
From release assets alone, an auditor can:
•	Validate the BOM (release_bom.json) with release_bom.schema.json
•	Verify every BOM hash against actual dist/* files
•	Validate any failure artifact (audit_failure.json) with release_audit_failure.schema.json
•	Recompute and verify OpenAPI diff report deterministically from snapshots + policy
All without cloning the repo.
________________________________________
Next diff (final “max governance” polish)
next diff: include release_audit_failure.schema.json in the release BOM consistency schema validation step
You already validate release_bom.json; this would also validate that the shipped audit schema itself satisfies its own identity contract (schema_version naming + $id embedding) using the same pattern you applied to the BOM schema.
Ship next diff: include release_audit_failure.schema.json in the release BOM consistency schema validation step
next diff: validate release_audit_failure.schema.json identity + self-consistency as part of the release “BOM consistency” gate
Goal: when you run the release hard gates (the same stage where you already assert BOM/hash consistency), also enforce that the shipped audit-failure schema in dist/ is:
•	present
•	has coherent identity (schema_version naming + $id embedding)
•	(optionally) usable as a validator schema (Draft 2020-12 shape sanity)
This keeps the “audit kit” from silently degrading (e.g., schema copied but missing $id, or someone changes schema_version naming).
________________________________________
Diff A — Add audit schema identity contract check into scripts/check_release_bom_consistency.py
Patch: scripts/check_release_bom_consistency.py
Add:
•	DIST_AUDIT_SCHEMA_PATH already exists from the prior diff
•	a small helper _check_schema_identity(...)
•	call it for both:
o	dist/release_bom.schema.json
o	dist/release_audit_failure.schema.json
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 6666666..7777777 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
 from __future__ import annotations
 
 import hashlib
 import json
+import re
 import sys
 from pathlib import Path
 from typing import Any, Dict
@@
 DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
 DIST_AUDIT_SCHEMA_PATH = ROOT / "dist" / "release_audit_failure.schema.json"
@@
 def _sha256_canonical_json_file(path: Path) -> str:
     obj = _load_json(path)
     return hashlib.sha256(_canonical_json_bytes(obj)).hexdigest()
 
+def _check_schema_identity(
+    *,
+    schema_obj: Dict[str, Any],
+    schema_path: str,
+    schema_version_re: str,
+    errors: list[str],
+) -> None:
+    """
+    Enforce identity coherence:
+      - schema_version exists and matches naming regex
+      - $id exists
+      - $id equals schema_version OR includes schema_version
+      - $schema is Draft 2020-12 (defense in depth)
+    """
+    sv = schema_obj.get("schema_version")
+    sid = schema_obj.get("$id")
+    sdecl = schema_obj.get("$schema")
+
+    if not isinstance(sv, str) or not sv:
+        errors.append(f"{schema_path}: missing/invalid schema_version")
+        return
+    if not re.match(schema_version_re, sv):
+        errors.append(f"{schema_path}: schema_version must match {schema_version_re} (got {sv!r})")
+
+    if not isinstance(sid, str) or not sid:
+        errors.append(f"{schema_path}: missing/invalid $id")
+    else:
+        if sid != sv and sv not in sid:
+            errors.append(
+                f"{schema_path}: identity mismatch ($id must equal schema_version or include it). "
+                f"schema_version={sv!r} $id={sid!r}"
+            )
+
+    if sdecl != "https://json-schema.org/draft/2020-12/schema":
+        errors.append(
+            f"{schema_path}: unexpected $schema value (expected Draft 2020-12). got={sdecl!r}"
+        )
+
@@
 def main() -> int:
     errors: list[str] = []
@@
-    schema = _load_json(DIST_BOM_SCHEMA_PATH)
-    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
+    schema = _load_json(DIST_BOM_SCHEMA_PATH)
+    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
+
+    # Identity/naming contracts for shipped schemas (release audit kit).
+    _check_schema_identity(
+        schema_obj=schema,
+        schema_path="dist/release_bom.schema.json",
+        schema_version_re=r"^release_bom_schema_v\d+$",
+        errors=errors,
+    )
+    _check_schema_identity(
+        schema_obj=audit_schema,
+        schema_path="dist/release_audit_failure.schema.json",
+        schema_version_re=r"^release_audit_failure_schema_v\d+$",
+        errors=errors,
+    )
@@
     if errors:
         print("[release-bom-consistency] FAIL", file=sys.stderr)
         for e in errors:
             print(f"- {e}", file=sys.stderr)
         return 1
Effect: if the audit schema’s identity drifts (bad schema_version name, missing $id, $id doesn’t embed schema_version, wrong draft), release gating fails before publishing.
________________________________________
Diff B — Ensure the audit failure schema file actually follows the naming regex
If your schemas/release_audit_failure.schema.json currently has:
•	$id: "release_audit_failure_schema_v1"
•	schema_version: "release_audit_failure_schema_v1"
…then you’re already compliant and no schema change is needed.
If not, align it (example):
diff --git a/schemas/release_audit_failure.schema.json b/schemas/release_audit_failure.schema.json
index 1111111..2222222 100644
--- a/schemas/release_audit_failure.schema.json
+++ b/schemas/release_audit_failure.schema.json
@@
-  "$id": "release_audit_failure_v1",
-  "schema_version": "release_audit_failure_v1",
+  "$id": "release_audit_failure_schema_v1",
+  "schema_version": "release_audit_failure_schema_v1",
________________________________________
Diff C — Optional: validate the audit failure schema via jsonschema itself (stronger “schema is a schema” check)
If you want to go beyond identity and also ensure the file is a valid Draft 2020-12 schema structure, you can add a lightweight “meta-validate” step:
•	Use jsonschema.Draft202012Validator.check_schema(schema_obj) inside check_release_bom_consistency.py
•	This requires jsonschema installed in the release workflow environment.
I’m not including it by default because your release gating currently doesn’t require jsonschema in that script, and identity checks catch most governance errors. If you want it, say: “next diff: meta-validate shipped schemas”.
________________________________________
What’s enforced after this diff
On tag release runs, before publishing:
•	The release BOM consistency step now also guarantees:
o	both shipped schemas have correct identity + naming discipline
o	(and still) schema hashes match BOM for both schema artifacts
o	dist payload is coherent
So the “audit kit” can’t silently decay.
________________________________________
Next diff
next diff: meta-validate shipped schemas (optional, stricter)
jsonschema.Draft202012Validator.check_schema(...) for both dist schemas, hard gate on failure.
Ship next diff: meta-validate shipped schemas (optional, stricter) jsonschema.Draft202012Validator.check_schema(...) for both dist schemas, hard gate on failure.
next diff: meta-validate shipped schemas (Draft 2020-12) as a hard gate
This adds the strictest “schema is actually a valid schema” check during the release BOM consistency stage by calling:
•	jsonschema.Draft202012Validator.check_schema(...)
…for both shipped dist schemas:
•	dist/release_bom.schema.json
•	dist/release_audit_failure.schema.json
If either schema is structurally invalid (bad keywords, wrong types, invalid $refs, etc.), the release gate fails before publishing.
________________________________________
Diff A — Add jsonschema meta-validation inside scripts/check_release_bom_consistency.py
Patch: scripts/check_release_bom_consistency.py
Add:
•	optional import of jsonschema
•	helper _meta_validate_schema(...)
•	invoke it after identity checks (so you still get clear “identity mismatch” errors even if jsonschema isn’t installed)
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 7777777..8888888 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
 import hashlib
 import json
 import re
 import sys
 from pathlib import Path
 from typing import Any, Dict
 
@@
 def _check_schema_identity(
@@
     if sdecl != "https://json-schema.org/draft/2020-12/schema":
         errors.append(
             f"{schema_path}: unexpected $schema value (expected Draft 2020-12). got={sdecl!r}"
         )
 
+def _meta_validate_schema(
+    *,
+    schema_obj: Dict[str, Any],
+    schema_path: str,
+    errors: list[str],
+) -> None:
+    """
+    Strict schema validity check using jsonschema's meta-schema validation.
+    This catches structurally invalid schemas before they ship.
+    """
+    try:
+        import jsonschema
+    except Exception as e:
+        errors.append(
+            f"{schema_path}: cannot meta-validate schema (jsonschema not installed). "
+            "Install jsonschema in release workflow environment. "
+            f"Import error: {e!r}"
+        )
+        return
+
+    try:
+        jsonschema.Draft202012Validator.check_schema(schema_obj)
+    except Exception as e:
+        errors.append(f"{schema_path}: meta-schema validation failed: {e!r}")
+
@@
 def main() -> int:
     errors: list[str] = []
@@
     schema = _load_json(DIST_BOM_SCHEMA_PATH)
     audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
 
     # Identity/naming contracts for shipped schemas (release audit kit).
@@
     _check_schema_identity(
         schema_obj=audit_schema,
         schema_path="dist/release_audit_failure.schema.json",
         schema_version_re=r"^release_audit_failure_schema_v\d+$",
         errors=errors,
     )
+
+    # Strict meta-validation: schemas must themselves be valid Draft 2020-12 schemas.
+    _meta_validate_schema(
+        schema_obj=schema,
+        schema_path="dist/release_bom.schema.json",
+        errors=errors,
+    )
+    _meta_validate_schema(
+        schema_obj=audit_schema,
+        schema_path="dist/release_audit_failure.schema.json",
+        errors=errors,
+    )
Behavior
•	If jsonschema is missing: the gate fails with a clear “install jsonschema” message (intentional, since this is release hardening).
•	If schema is invalid: gate fails with the exception details.
________________________________________
Diff B — Ensure release workflow installs jsonschema before running consistency checks
If your release workflow doesn’t already install jsonschema (some earlier diffs installed it in an “always” step), make it explicit before the BOM consistency step.
Patch: .github/workflows/release.yml
Add a small install step (or fold into your existing pip install step):
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 0000000..1111111 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Install release gate deps
         run: |
+          python -m pip install jsonschema
+          python -m pip install -e .
If you already do pip install -e . earlier, you can just add jsonschema there.
________________________________________
Diff C — (Optional) Add a unit test that meta-validates the source schemas too
This catches invalid schemas in normal CI before tags.
New file: tests/test_schema_meta_validation.py
diff --git a/tests/test_schema_meta_validation.py b/tests/test_schema_meta_validation.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_schema_meta_validation.py
@@ -0,0 +1,50 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+import jsonschema
+
+
+ROOT = Path(__file__).resolve().parents[1]
+
+SCHEMAS = [
+    ROOT / "schemas" / "release_bom.schema.json",
+    ROOT / "schemas" / "release_audit_failure.schema.json",
+]
+
+
+def test_release_related_schemas_are_valid_draft202012() -> None:
+    for p in SCHEMAS:
+        assert p.exists(), f"Missing schema: {p}"
+        obj = json.loads(p.read_text(encoding="utf-8"))
+        jsonschema.Draft202012Validator.check_schema(obj)
If you’d rather keep this “release-only,” skip this test and rely on the release workflow. But it’s usually worth it.
________________________________________
Next diff (if you want max determinism in error payloads)
next diff: normalize check_schema exception strings into structured failures
So schema meta-validation errors also emit a machine-parsable JSON failure object (similar to audit_failure.json format), instead of raw exception reprs in logs.
Ship next diff: normalize check_schema exception strings into structured failures
next diff: normalize check_schema errors into structured failures (machine-parsable, no raw repr)
We’ll tighten the meta-validation gate so schema invalidity produces a consistent structured error object (same “kind/path/expected/got/details” discipline you used for audit_failure.json), instead of dumping an exception repr.
Two parts:
1.	Update scripts/check_release_bom_consistency.py so _meta_validate_schema() emits a structured detail record with stable fields.
2.	Ensure the release workflow surfaces those records in logs in a predictable way (the existing “- error line” format is fine, but we’ll embed JSON fragments to preserve machine parsability).
Design choice (keeps it simple + robust)
•	scripts/check_release_bom_consistency.py remains a “human gate” script, but when schema meta-validation fails, it appends a single-line JSON object prefixed with a stable marker so log parsers can find it:
o	SCHEMA_META_ERROR: { ...json... }
•	This avoids needing to refactor the whole script to JSON-only output, while still giving you deterministic machine parsing.
________________________________________
Diff A — Structured meta-validation error payloads in check_release_bom_consistency.py
Patch: scripts/check_release_bom_consistency.py
Add helpers:
•	_schema_meta_error_payload(...) -> dict
•	_append_meta_error(errors, payload) appends a single string with a marker + JSON
Replace the previous errors.append(f"... {e!r}") with the structured payload.
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 8888888..9999999 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@
 import hashlib
 import json
 import re
 import sys
 from pathlib import Path
 from typing import Any, Dict
 
@@
 def _meta_validate_schema(
     *,
     schema_obj: Dict[str, Any],
     schema_path: str,
     errors: list[str],
 ) -> None:
@@
     try:
         import jsonschema
     except Exception as e:
-        errors.append(
-            f"{schema_path}: cannot meta-validate schema (jsonschema not installed). "
-            "Install jsonschema in release workflow environment. "
-            f"Import error: {e!r}"
-        )
+        payload = {
+            "kind": "schema_meta_validation_unavailable",
+            "path": schema_path,
+            "expected": "jsonschema installed (Draft202012Validator.check_schema available)",
+            "got": "import_error",
+            "details": {
+                "exception_type": type(e).__name__,
+                "exception_message": str(e),
+            },
+        }
+        errors.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
         return
 
     try:
         jsonschema.Draft202012Validator.check_schema(schema_obj)
-    except Exception as e:
-        errors.append(f"{schema_path}: meta-schema validation failed: {e!r}")
+    except Exception as e:
+        # Normalize exception into a stable, machine-parsable record.
+        # Avoid repr() to prevent environment-dependent noise.
+        payload = {
+            "kind": "schema_meta_validation_failed",
+            "path": schema_path,
+            "expected": "valid Draft 2020-12 JSON Schema",
+            "got": "invalid_schema",
+            "details": {
+                "exception_type": type(e).__name__,
+                "exception_message": str(e),
+            },
+        }
+        # Some jsonschema exceptions carry additional structured info.
+        # Keep it best-effort and non-fatal if attributes don’t exist.
+        for attr in ("message", "schema_path", "path", "validator", "validator_value", "context"):
+            try:
+                v = getattr(e, attr)
+            except Exception:
+                continue
+            try:
+                json.dumps(v)  # type: ignore[arg-type]
+                payload["details"][attr] = v
+            except Exception:
+                payload["details"][attr] = str(v)
+
+        errors.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
What this gives you
When a schema is invalid, logs now include a line like:
- SCHEMA_META_ERROR: {"details":{"exception_message":"...","exception_type":"SchemaError",...},"expected":"valid Draft 2020-12 JSON Schema","got":"invalid_schema","kind":"schema_meta_validation_failed","path":"dist/release_audit_failure.schema.json"}
A log scraper can reliably:
•	search for SCHEMA_META_ERROR: 
•	parse the trailing JSON
•	act on kind/path/details
No dependence on repr formatting.
________________________________________
Diff B — Optional: enforce “SCHEMA_META_ERROR lines must be valid JSON” with a tiny test
This prevents accidental changes to the marker format that break parsers.
New file: tests/test_schema_meta_error_marker_format.py
diff --git a/tests/test_schema_meta_error_marker_format.py b/tests/test_schema_meta_error_marker_format.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/tests/test_schema_meta_error_marker_format.py
@@ -0,0 +1,56 @@
+from __future__ import annotations
+
+import json
+
+
+def test_schema_meta_error_marker_prefix_is_parseable_json() -> None:
+    # Contract: if a line begins with "SCHEMA_META_ERROR: ",
+    # the remainder must be valid JSON with required keys.
+    sample = (
+        'SCHEMA_META_ERROR: {"kind":"schema_meta_validation_failed","path":"dist/x.json",'
+        '"expected":"valid Draft 2020-12 JSON Schema","got":"invalid_schema","details":{"exception_type":"SchemaError","exception_message":"boom"}}'
+    )
+    assert sample.startswith("SCHEMA_META_ERROR: ")
+    payload = json.loads(sample.split("SCHEMA_META_ERROR: ", 1)[1])
+    for k in ("kind", "path", "expected", "got", "details"):
+        assert k in payload
This is a “format contract” test only.
________________________________________
Diff C — Workflow: nothing required, but here’s an optional extraction step (operator candy)
You already print each error line prefixed with - . If you want to additionally extract these JSON payloads into an artifact when the job fails (without changing semantics), add:
•	a step if: failure() that greps SCHEMA_META_ERROR: lines from logs is hard (GitHub Actions doesn’t expose logs easily).
•	better: keep as-is; your log parser (or future action) can scrape the job output via the Actions API if needed.
So no workflow changes are necessary for this diff.
________________________________________
Next diff (if you want “pure JSON mode”)
next diff: add --json flag to check_release_bom_consistency.py to emit a single JSON failure object (with a details[] list including both human strings and schema meta failures). This is the ultimate “no ad-hoc log parsing” variant.
Ship next diff: add --json flag to check_release_bom_consistency.py to emit a single JSON failure object (with a details[] list including both human strings and schema meta failures). This is the ultimate “no ad-hoc log parsing” variant.
next diff: add --json flag to check_release_bom_consistency.py (single JSON failure object + details[])
This upgrades the release gate script into a dual-mode tool:
•	Default mode (current): human-readable lines
•	--json mode: one machine-parsable JSON object on stderr on failure (and optional JSON on success if you want)
The JSON object will include:
•	kind: "release_bom_consistency_failed"
•	path: "dist/*"
•	expected: "release payload is self-consistent"
•	got: "inconsistencies"
•	details: array of structured items; each item may be:
o	a normalized schema meta failure (already structured)
o	a normalized identity/hash/coherence failure
•	ci: boolean derived from env CI=true
No more scraping arbitrary log text.
________________________________________
Diff A — Add --json flag and structured error aggregation
Patch: scripts/check_release_bom_consistency.py
Key changes:
1.	Parse --json
2.	Maintain errors_text: list[str] and errors_struct: list[dict]
3.	When errors occur:
o	if --json: emit one JSON object (with details[])
o	else: keep current human output (and still include SCHEMA_META_ERROR: lines if present)
4.	Update _meta_validate_schema() to append structured errors directly (no marker needed in JSON mode)
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index 9999999..aaaaaaa 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@ -1,17 +1,19 @@
 from __future__ import annotations
 
 import hashlib
 import json
+import os
 import re
 import sys
 from pathlib import Path
-from typing import Any, Dict
+from typing import Any, Dict, List, Optional
 
@@
 def _check_schema_identity(
     *,
     schema_obj: Dict[str, Any],
     schema_path: str,
     schema_version_re: str,
-    errors: list[str],
+    errors_text: list[str],
+    errors_struct: list[Dict[str, Any]],
 ) -> None:
@@
     sv = schema_obj.get("schema_version")
     sid = schema_obj.get("$id")
     sdecl = schema_obj.get("$schema")
 
     if not isinstance(sv, str) or not sv:
-        errors.append(f"{schema_path}: missing/invalid schema_version")
+        errors_text.append(f"{schema_path}: missing/invalid schema_version")
+        errors_struct.append({
+            "kind": "schema_identity_invalid",
+            "path": schema_path,
+            "expected": "schema_version present and valid",
+            "got": sv,
+        })
         return
     if not re.match(schema_version_re, sv):
-        errors.append(f"{schema_path}: schema_version must match {schema_version_re} (got {sv!r})")
+        errors_text.append(f"{schema_path}: schema_version must match {schema_version_re} (got {sv!r})")
+        errors_struct.append({
+            "kind": "schema_identity_invalid",
+            "path": schema_path,
+            "expected": f"schema_version matches {schema_version_re}",
+            "got": sv,
+        })
 
     if not isinstance(sid, str) or not sid:
-        errors.append(f"{schema_path}: missing/invalid $id")
+        errors_text.append(f"{schema_path}: missing/invalid $id")
+        errors_struct.append({
+            "kind": "schema_identity_invalid",
+            "path": schema_path,
+            "expected": "$id present and valid",
+            "got": sid,
+        })
     else:
         if sid != sv and sv not in sid:
-            errors.append(
+            errors_text.append(
                 f"{schema_path}: identity mismatch ($id must equal schema_version or include it). "
                 f"schema_version={sv!r} $id={sid!r}"
             )
+            errors_struct.append({
+                "kind": "schema_identity_mismatch",
+                "path": schema_path,
+                "expected": "($id == schema_version) OR (schema_version is substring of $id)",
+                "got": {"$id": sid, "schema_version": sv},
+            })
 
     if sdecl != "https://json-schema.org/draft/2020-12/schema":
-        errors.append(
+        errors_text.append(
             f"{schema_path}: unexpected $schema value (expected Draft 2020-12). got={sdecl!r}"
         )
+        errors_struct.append({
+            "kind": "schema_identity_invalid",
+            "path": schema_path,
+            "expected": "https://json-schema.org/draft/2020-12/schema",
+            "got": sdecl,
+        })
 
 def _meta_validate_schema(
     *,
     schema_obj: Dict[str, Any],
     schema_path: str,
-    errors: list[str],
+    errors_text: list[str],
+    errors_struct: list[Dict[str, Any]],
 ) -> None:
@@
     try:
         import jsonschema
     except Exception as e:
-        payload = {
-            "kind": "schema_meta_validation_unavailable",
-            "path": schema_path,
-            "expected": "jsonschema installed (Draft202012Validator.check_schema available)",
-            "got": "import_error",
-            "details": {
-                "exception_type": type(e).__name__,
-                "exception_message": str(e),
-            },
-        }
-        errors.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
+        payload = {
+            "kind": "schema_meta_validation_unavailable",
+            "path": schema_path,
+            "expected": "jsonschema installed (Draft202012Validator.check_schema available)",
+            "got": "import_error",
+            "details": {"exception_type": type(e).__name__, "exception_message": str(e)},
+        }
+        errors_text.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
+        errors_struct.append(payload)
         return
 
     try:
         jsonschema.Draft202012Validator.check_schema(schema_obj)
     except Exception as e:
-        payload = {
+        payload: Dict[str, Any] = {
             "kind": "schema_meta_validation_failed",
             "path": schema_path,
             "expected": "valid Draft 2020-12 JSON Schema",
             "got": "invalid_schema",
             "details": {
                 "exception_type": type(e).__name__,
                 "exception_message": str(e),
             },
         }
@@
-        errors.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
+        errors_text.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
+        errors_struct.append(payload)
 
+def _is_ci_true() -> bool:
+    return (os.environ.get("CI") or "").strip().lower() == "true"
+
+def _emit_failure_json(details: List[Dict[str, Any]]) -> None:
+    payload = {
+        "kind": "release_bom_consistency_failed",
+        "path": "dist/*",
+        "expected": "release payload is self-consistent",
+        "got": "inconsistencies",
+        "ci": _is_ci_true(),
+        "details": details,
+    }
+    print(json.dumps(payload, sort_keys=True), file=sys.stderr)
+
@@
 def main() -> int:
-    errors: list[str] = []
+    json_mode = "--json" in sys.argv[1:]
+    # If json_mode is enabled, we still collect text errors for local debugging,
+    # but the failure output becomes a single JSON object.
+    errors_text: list[str] = []
+    errors_struct: list[Dict[str, Any]] = []
@@
     _req(DIST_POLICY_PATH.exists(), f"Missing {DIST_POLICY_PATH.relative_to(ROOT)}", errors)
@@
-    schema = _load_json(DIST_BOM_SCHEMA_PATH)
-    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
+    schema = _load_json(DIST_BOM_SCHEMA_PATH)
+    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
 
     # Identity/naming contracts for shipped schemas (release audit kit).
     _check_schema_identity(
         schema_obj=schema,
         schema_path="dist/release_bom.schema.json",
         schema_version_re=r"^release_bom_schema_v\d+$",
-        errors=errors,
+        errors_text=errors_text,
+        errors_struct=errors_struct,
     )
     _check_schema_identity(
         schema_obj=audit_schema,
         schema_path="dist/release_audit_failure.schema.json",
         schema_version_re=r"^release_audit_failure_schema_v\d+$",
-        errors=errors,
+        errors_text=errors_text,
+        errors_struct=errors_struct,
     )
 
     # Strict meta-validation: schemas must themselves be valid Draft 2020-12 schemas.
     _meta_validate_schema(
         schema_obj=schema,
         schema_path="dist/release_bom.schema.json",
-        errors=errors,
+        errors_text=errors_text,
+        errors_struct=errors_struct,
     )
     _meta_validate_schema(
         schema_obj=audit_schema,
         schema_path="dist/release_audit_failure.schema.json",
-        errors=errors,
+        errors_text=errors_text,
+        errors_struct=errors_struct,
     )
@@
-    if errors:
-        print("[release-bom-consistency] FAIL", file=sys.stderr)
-        for e in errors:
-            print(f"- {e}", file=sys.stderr)
-        return 1
+    # (Existing checks throughout the script should continue appending human strings to
+    #  errors_text AND structured objects to errors_struct as we migrate them.)
+    #
+    # For now, we keep legacy failures in text form by converting any remaining
+    # plain-text errors into structured records at the end.
+
+    # Convert legacy text errors into structured items if they weren't already added.
+    # This ensures --json always includes every failure.
+    for line in errors_text:
+        # Skip SCHEMA_META_ERROR lines because those already have structured payloads.
+        if line.startswith("SCHEMA_META_ERROR: "):
+            continue
+        errors_struct.append({
+            "kind": "release_bom_consistency_error",
+            "path": "dist/*",
+            "expected": "no errors",
+            "got": line,
+        })
+
+    if errors_text or errors_struct:
+        if json_mode:
+            _emit_failure_json(errors_struct)
+        else:
+            print("[release-bom-consistency] FAIL", file=sys.stderr)
+            for e in errors_text:
+                print(f"- {e}", file=sys.stderr)
+        return 1
 
     print("[release-bom-consistency] OK")
     return 0
One important follow-through
Right now, parts of the script still call _req(..., errors) where errors was the old list. You’ll need to replace those calls to target errors_text (and optionally also add structured entries). For a minimal diff that doesn’t require rewriting every _req call, you can:
•	keep _req writing into errors_text (rename its param)
•	later, as a follow-on, make _req also add structured entries for each failure.
If you already have _req implemented in the file, update its signature like:
-def _req(cond: bool, msg: str, errors: list[str]) -> None:
+def _req(cond: bool, msg: str, errors_text: list[str]) -> None:
     if not cond:
-        errors.append(msg)
+        errors_text.append(msg)
and update callsites from errors → errors_text.
That keeps the refactor small and makes --json mode complete via the “legacy conversion” loop.
________________________________________
Diff B — Update the release workflow to use --json (optional but recommended)
Patch: .github/workflows/release.yml
This ensures tagged CI logs include the single JSON failure object.
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index eeeeeee..fffffff 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Release BOM consistency check (hard gate)
         env:
           CI: "true"
         run: |
-          python scripts/check_release_bom_consistency.py
+          python scripts/check_release_bom_consistency.py --json
If you want to keep human output locally, don’t change anything else—developers can run without --json.
________________________________________
Resulting JSON contract (example)
On failure:
{
  "ci": true,
  "details": [
    {
      "details": {
        "exception_message": "...",
        "exception_type": "SchemaError"
      },
      "expected": "valid Draft 2020-12 JSON Schema",
      "got": "invalid_schema",
      "kind": "schema_meta_validation_failed",
      "path": "dist/release_audit_failure.schema.json"
    },
    {
      "expected": "no errors",
      "got": "Missing dist/openapi_after.json",
      "kind": "release_bom_consistency_error",
      "path": "dist/*"
    }
  ],
  "expected": "release payload is self-consistent",
  "got": "inconsistencies",
  "kind": "release_bom_consistency_failed",
  "path": "dist/*"
}
________________________________________
Next diff (true “all-structured” completion)
next diff: refactor _req() and all consistency checks to emit structured items directly (no legacy conversion), and add a JSON schema for the check_release_bom_consistency --json output so the gate’s own output is contract-governed too.
Ship next diff: refactor _req() and all consistency checks to emit structured items directly (no legacy conversion), and add a JSON schema for the check_release_bom_consistency --json output so the gate’s own output is contract-governed too.
next diff: make _req() + all consistency checks emit structured items only (no legacy conversion) + add a JSON schema for check_release_bom_consistency.py --json output
This finishes the “pure JSON gate” path:
•	scripts/check_release_bom_consistency.py --json emits exactly one JSON object on failure (and optionally one on success if you choose)
•	Every check writes structured detail items (no freeform strings)
•	A new schema schemas/release_bom_consistency_result.schema.json governs the gate output
•	A validator script enforces the output schema (and can be used by CI or auditors)
Below is the full diff set.
________________________________________
Diff A — New schema: schemas/release_bom_consistency_result.schema.json
This is the contract for the gate’s --json output.
diff --git a/schemas/release_bom_consistency_result.schema.json b/schemas/release_bom_consistency_result.schema.json
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/schemas/release_bom_consistency_result.schema.json
@@ -0,0 +1,192 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "$id": "release_bom_consistency_result_schema_v1",
+  "schema_version": "release_bom_consistency_result_schema_v1",
+  "title": "Release BOM Consistency Result Schema",
+  "type": "object",
+  "additionalProperties": false,
+  "required": ["kind", "path", "expected", "got", "ci", "details"],
+  "properties": {
+    "kind": {
+      "type": "string",
+      "const": "release_bom_consistency_failed"
+    },
+    "path": {
+      "type": "string",
+      "const": "dist/*"
+    },
+    "expected": {},
+    "got": {},
+    "ci": { "type": "boolean" },
+    "details": {
+      "type": "array",
+      "minItems": 1,
+      "items": { "$ref": "#/$defs/detail_item" }
+    }
+  },
+  "$defs": {
+    "detail_kind": {
+      "type": "string",
+      "enum": [
+        "missing_file",
+        "invalid_json",
+        "schema_identity_invalid",
+        "schema_identity_mismatch",
+        "schema_meta_validation_unavailable",
+        "schema_meta_validation_failed",
+
+        "bom_missing_artifact",
+        "bom_invalid_artifact",
+        "bom_path_missing",
+        "bom_hash_mismatch",
+        "bom_identity_mismatch",
+        "bom_selected_mismatch",
+
+        "provenance_mismatch"
+      ]
+    },
+    "detail_item": {
+      "type": "object",
+      "additionalProperties": false,
+      "required": ["kind", "path", "expected", "got"],
+      "properties": {
+        "kind": { "$ref": "#/$defs/detail_kind" },
+        "path": { "type": "string", "minLength": 1 },
+        "expected": {},
+        "got": {},
+        "details": {}
+      }
+    }
+  }
+}
Notes:
•	This schema governs the failure payload. If you also want a governed success payload, we can either:
o	emit nothing on success (current behavior), or
o	emit a second schema and a --json-always flag. Not necessary for gating.
________________________________________
Diff B — Validator script: scripts/validate_release_bom_consistency_result.py
diff --git a/scripts/validate_release_bom_consistency_result.py b/scripts/validate_release_bom_consistency_result.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/validate_release_bom_consistency_result.py
@@ -0,0 +1,116 @@
+from __future__ import annotations
+
+import json
+import sys
+from pathlib import Path
+
+try:
+    import jsonschema
+except Exception as e:  # pragma: no cover
+    raise RuntimeError(
+        "jsonschema is required to validate release_bom_consistency_result output."
+    ) from e
+
+
+ROOT = Path(__file__).resolve().parents[1]
+SCHEMA = ROOT / "schemas" / "release_bom_consistency_result.schema.json"
+
+
+def _load_json(path: Path):
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def main(argv: list[str]) -> int:
+    if len(argv) != 2:
+        print("Usage: python scripts/validate_release_bom_consistency_result.py <result.json>", file=sys.stderr)
+        return 2
+
+    target = ROOT / argv[1]
+    if not SCHEMA.exists():
+        print(f"Missing schema: {SCHEMA.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+    if not target.exists():
+        print(f"Missing file: {target.relative_to(ROOT)}", file=sys.stderr)
+        return 1
+
+    schema = _load_json(SCHEMA)
+    obj = _load_json(target)
+
+    v = jsonschema.Draft202012Validator(schema)
+    errs = sorted(v.iter_errors(obj), key=lambda e: list(e.path))
+    if errs:
+        print("[bom-consistency-result-schema] FAIL", file=sys.stderr)
+        for e in errs[:50]:
+            path = ".".join([str(p) for p in e.path]) or "<root>"
+            print(f"  - {path}: {e.message}", file=sys.stderr)
+        if len(errs) > 50:
+            print(f"  ... ({len(errs) - 50} more)", file=sys.stderr)
+        return 1
+
+    print("[bom-consistency-result-schema] OK")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main(sys.argv))
________________________________________
Diff C — Refactor scripts/check_release_bom_consistency.py to emit structured items everywhere (no legacy conversion)
This is the core change. The script will now maintain:
•	issues: list[dict] only
•	helpers:
o	_issue(kind, path, expected, got, details=None)
o	_req_file(path, label)
o	_req(cond, kind, ...) (structured)
o	_load_json_or_issue(...)
When --json is provided:
•	on failure: print exactly one JSON payload (matching schema) and exit 1
•	on success: print [release-bom-consistency] OK and exit 0 (unchanged)
When --json is not provided:
•	keep human output, but derived from structured issues (no ad-hoc strings)
Patch: scripts/check_release_bom_consistency.py
diff --git a/scripts/check_release_bom_consistency.py b/scripts/check_release_bom_consistency.py
index aaaaaaa..bbbbbbb 100644
--- a/scripts/check_release_bom_consistency.py
+++ b/scripts/check_release_bom_consistency.py
@@ -1,200 +1,320 @@
 from __future__ import annotations
 
 import hashlib
 import json
 import os
 import re
 import sys
 from pathlib import Path
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List, Optional, Tuple
 
 ROOT = Path(__file__).resolve().parents[1]
 
 BOM_PATH = ROOT / "dist" / "release_bom.json"
 DIFF_REPORT_PATH = ROOT / "dist" / "openapi_diff_report.json"
 
 DIST_CLASSIFIER_MANIFEST_PATH = ROOT / "dist" / "openapi_classifier_manifest.json"
 DIST_POLICY_PATH = ROOT / "dist" / "openapi_breaking_policy.json"
 DIST_BOM_SCHEMA_PATH = ROOT / "dist" / "release_bom.schema.json"
 DIST_AUDIT_SCHEMA_PATH = ROOT / "dist" / "release_audit_failure.schema.json"
 
 DIST_OPENAPI_BEFORE = ROOT / "dist" / "openapi_before.json"
 DIST_OPENAPI_AFTER = ROOT / "dist" / "openapi_after.json"
 
+def _is_ci_true() -> bool:
+    return (os.environ.get("CI") or "").strip().lower() == "true"
+
 def _load_json(path: Path) -> Dict[str, Any]:
     return json.loads(path.read_text(encoding="utf-8"))
 
 def _canonical_json_bytes(obj: Any) -> bytes:
     return json.dumps(obj, sort_keys=True, separators=(",", ":")).encode("utf-8")
 
 def _sha256_file(path: Path) -> str:
     h = hashlib.sha256()
     with path.open("rb") as f:
         for chunk in iter(lambda: f.read(1024 * 64), b""):
             h.update(chunk)
     return h.hexdigest()
 
 def _sha256_canonical_json_file(path: Path) -> str:
     obj = _load_json(path)
     return hashlib.sha256(_canonical_json_bytes(obj)).hexdigest()
 
-def _req(cond: bool, msg: str, errors_text: list[str]) -> None:
-    if not cond:
-        errors_text.append(msg)
+def _issue(
+    kind: str,
+    path: str,
+    expected: Any,
+    got: Any,
+    details: Optional[Any] = None,
+) -> Dict[str, Any]:
+    d: Dict[str, Any] = {
+        "kind": kind,
+        "path": path,
+        "expected": expected,
+        "got": got,
+    }
+    if details is not None:
+        d["details"] = details
+    return d
+
+def _req_file(p: Path, label: str, issues: List[Dict[str, Any]]) -> bool:
+    if not p.exists():
+        issues.append(_issue("missing_file", str(p.relative_to(ROOT)), "file exists", "missing", {"label": label}))
+        return False
+    return True
+
+def _load_json_or_issue(p: Path, label: str, issues: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
+    try:
+        obj = _load_json(p)
+    except Exception as e:
+        issues.append(_issue(
+            "invalid_json",
+            str(p.relative_to(ROOT)),
+            "valid JSON object",
+            "parse_error",
+            {"label": label, "exception_type": type(e).__name__, "exception_message": str(e)},
+        ))
+        return None
+    if not isinstance(obj, dict):
+        issues.append(_issue("invalid_json", str(p.relative_to(ROOT)), "JSON object", type(obj).__name__, {"label": label}))
+        return None
+    return obj
 
 def _check_schema_identity(
     *,
     schema_obj: Dict[str, Any],
     schema_path: str,
     schema_version_re: str,
-    errors_text: list[str],
-    errors_struct: list[Dict[str, Any]],
+    issues: List[Dict[str, Any]],
 ) -> None:
     """
     Enforce identity coherence:
       - schema_version exists and matches naming regex
       - $id exists
       - $id equals schema_version OR includes schema_version
       - $schema is Draft 2020-12
     """
     sv = schema_obj.get("schema_version")
     sid = schema_obj.get("$id")
     sdecl = schema_obj.get("$schema")
 
     if not isinstance(sv, str) or not sv:
-        errors_text.append(f"{schema_path}: missing/invalid schema_version")
-        errors_struct.append({
-            "kind": "schema_identity_invalid",
-            "path": schema_path,
-            "expected": "schema_version present and valid",
-            "got": sv,
-        })
+        issues.append(_issue("schema_identity_invalid", schema_path, "schema_version present and non-empty string", sv))
         return
     if not re.match(schema_version_re, sv):
-        errors_text.append(f"{schema_path}: schema_version must match {schema_version_re} (got {sv!r})")
-        errors_struct.append({
-            "kind": "schema_identity_invalid",
-            "path": schema_path,
-            "expected": f"schema_version matches {schema_version_re}",
-            "got": sv,
-        })
+        issues.append(_issue("schema_identity_invalid", schema_path, f"schema_version matches {schema_version_re}", sv))
 
     if not isinstance(sid, str) or not sid:
-        errors_text.append(f"{schema_path}: missing/invalid $id")
-        errors_struct.append({
-            "kind": "schema_identity_invalid",
-            "path": schema_path,
-            "expected": "$id present and valid",
-            "got": sid,
-        })
+        issues.append(_issue("schema_identity_invalid", schema_path, "$id present and non-empty string", sid))
     else:
         if sid != sv and sv not in sid:
-            errors_text.append(
-                f"{schema_path}: identity mismatch ($id must equal schema_version or include it). "
-                f"schema_version={sv!r} $id={sid!r}"
-            )
-            errors_struct.append({
-                "kind": "schema_identity_mismatch",
-                "path": schema_path,
-                "expected": "($id == schema_version) OR (schema_version is substring of $id)",
-                "got": {"$id": sid, "schema_version": sv},
-            })
+            issues.append(_issue(
+                "schema_identity_mismatch",
+                schema_path,
+                "($id == schema_version) OR (schema_version is substring of $id)",
+                {"$id": sid, "schema_version": sv},
+            ))
 
     if sdecl != "https://json-schema.org/draft/2020-12/schema":
-        errors_text.append(
-            f"{schema_path}: unexpected $schema value (expected Draft 2020-12). got={sdecl!r}"
-        )
-        errors_struct.append({
-            "kind": "schema_identity_invalid",
-            "path": schema_path,
-            "expected": "https://json-schema.org/draft/2020-12/schema",
-            "got": sdecl,
-        })
+        issues.append(_issue(
+            "schema_identity_invalid",
+            schema_path,
+            "https://json-schema.org/draft/2020-12/schema",
+            sdecl,
+        ))
 
 def _meta_validate_schema(
     *,
     schema_obj: Dict[str, Any],
     schema_path: str,
-    errors_text: list[str],
-    errors_struct: list[Dict[str, Any]],
+    issues: List[Dict[str, Any]],
 ) -> None:
     """
     Strict schema validity check using jsonschema's meta-schema validation.
     """
     try:
         import jsonschema
     except Exception as e:
-        payload = {
-            "kind": "schema_meta_validation_unavailable",
-            "path": schema_path,
-            "expected": "jsonschema installed (Draft202012Validator.check_schema available)",
-            "got": "import_error",
-            "details": {"exception_type": type(e).__name__, "exception_message": str(e)},
-        }
-        errors_text.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
-        errors_struct.append(payload)
+        issues.append(_issue(
+            "schema_meta_validation_unavailable",
+            schema_path,
+            "jsonschema installed (Draft202012Validator.check_schema available)",
+            "import_error",
+            {"exception_type": type(e).__name__, "exception_message": str(e)},
+        ))
         return
 
     try:
         jsonschema.Draft202012Validator.check_schema(schema_obj)
     except Exception as e:
-        payload: Dict[str, Any] = {
-            "kind": "schema_meta_validation_failed",
-            "path": schema_path,
-            "expected": "valid Draft 2020-12 JSON Schema",
-            "got": "invalid_schema",
-            "details": {
-                "exception_type": type(e).__name__,
-                "exception_message": str(e),
-            },
-        }
-        for attr in ("message", "schema_path", "path", "validator", "validator_value", "context"):
-            try:
-                v = getattr(e, attr)
-            except Exception:
-                continue
-            try:
-                json.dumps(v)
-                payload["details"][attr] = v
-            except Exception:
-                payload["details"][attr] = str(v)
-
-        errors_text.append("SCHEMA_META_ERROR: " + json.dumps(payload, sort_keys=True))
-        errors_struct.append(payload)
+        details: Dict[str, Any] = {"exception_type": type(e).__name__, "exception_message": str(e)}
+        for attr in ("message", "schema_path", "path", "validator", "validator_value"):
+            try:
+                v = getattr(e, attr)
+            except Exception:
+                continue
+            try:
+                json.dumps(v)
+                details[attr] = v
+            except Exception:
+                details[attr] = str(v)
+        issues.append(_issue(
+            "schema_meta_validation_failed",
+            schema_path,
+            "valid Draft 2020-12 JSON Schema",
+            "invalid_schema",
+            details,
+        ))
 
+def _emit_failure_json(issues: List[Dict[str, Any]]) -> None:
+    payload = {
+        "kind": "release_bom_consistency_failed",
+        "path": "dist/*",
+        "expected": "release payload is self-consistent",
+        "got": "inconsistencies",
+        "ci": _is_ci_true(),
+        "details": issues,
+    }
+    print(json.dumps(payload, sort_keys=True), file=sys.stderr)
+
+def _emit_failure_human(issues: List[Dict[str, Any]]) -> None:
+    print("[release-bom-consistency] FAIL", file=sys.stderr)
+    for it in issues:
+        # One line per issue; keep it compact and stable.
+        k = it.get("kind")
+        p = it.get("path")
+        print(f"- {k} @ {p}: expected={it.get('expected')!r} got={it.get('got')!r}", file=sys.stderr)
+
+def _check_bom_artifact_file_sha(
+    *,
+    artifacts: Dict[str, Any],
+    name: str,
+    sha_field: str,
+    issues: List[Dict[str, Any]],
+) -> Optional[Path]:
+    a = artifacts.get(name)
+    if not isinstance(a, dict):
+        issues.append(_issue("bom_missing_artifact", f"release_bom.json:artifacts.{name}", "object", a))
+        return None
+    path_str = a.get("path")
+    if not isinstance(path_str, str) or not path_str:
+        issues.append(_issue("bom_invalid_artifact", f"release_bom.json:artifacts.{name}.path", "non-empty string", path_str))
+        return None
+    p = ROOT / path_str
+    if not p.exists():
+        issues.append(_issue("bom_path_missing", f"release_bom.json:artifacts.{name}.path", "existing file", path_str))
+        return None
+    got_sha = _sha256_file(p)
+    want_sha = a.get(sha_field)
+    if want_sha != got_sha:
+        issues.append(_issue(
+            "bom_hash_mismatch",
+            f"release_bom.json:artifacts.{name}.{sha_field}",
+            want_sha,
+            got_sha,
+            {"file": path_str},
+        ))
+    return p
+
 def main() -> int:
-    json_mode = "--json" in sys.argv[1:]
-    errors_text: list[str] = []
-    errors_struct: list[Dict[str, Any]] = []
+    json_mode = "--json" in sys.argv[1:]
+    issues: List[Dict[str, Any]] = []
 
-    _req(BOM_PATH.exists(), f"Missing {BOM_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIFF_REPORT_PATH.exists(), f"Missing {DIFF_REPORT_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIST_CLASSIFIER_MANIFEST_PATH.exists(), f"Missing {DIST_CLASSIFIER_MANIFEST_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIST_POLICY_PATH.exists(), f"Missing {DIST_POLICY_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIST_BOM_SCHEMA_PATH.exists(), f"Missing {DIST_BOM_SCHEMA_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIST_AUDIT_SCHEMA_PATH.exists(), f"Missing {DIST_AUDIT_SCHEMA_PATH.relative_to(ROOT)}", errors_text)
-    _req(DIST_OPENAPI_BEFORE.exists(), f"Missing {DIST_OPENAPI_BEFORE.relative_to(ROOT)}", errors_text)
-    _req(DIST_OPENAPI_AFTER.exists(), f"Missing {DIST_OPENAPI_AFTER.relative_to(ROOT)}", errors_text)
+    ok = True
+    ok &= _req_file(BOM_PATH, "release_bom", issues)
+    ok &= _req_file(DIFF_REPORT_PATH, "openapi_diff_report", issues)
+    ok &= _req_file(DIST_CLASSIFIER_MANIFEST_PATH, "openapi_classifier_manifest", issues)
+    ok &= _req_file(DIST_POLICY_PATH, "openapi_breaking_policy", issues)
+    ok &= _req_file(DIST_BOM_SCHEMA_PATH, "release_bom_schema", issues)
+    ok &= _req_file(DIST_AUDIT_SCHEMA_PATH, "release_audit_failure_schema", issues)
+    ok &= _req_file(DIST_OPENAPI_BEFORE, "openapi_before_snapshot", issues)
+    ok &= _req_file(DIST_OPENAPI_AFTER, "openapi_after_snapshot", issues)
+    if not ok:
+        if json_mode:
+            _emit_failure_json(issues)
+        else:
+            _emit_failure_human(issues)
+        return 1
 
-    bom = _load_json(BOM_PATH)
-    diff = _load_json(DIFF_REPORT_PATH)
-    manifest = _load_json(DIST_CLASSIFIER_MANIFEST_PATH)
-    schema = _load_json(DIST_BOM_SCHEMA_PATH)
-    audit_schema = _load_json(DIST_AUDIT_SCHEMA_PATH)
+    bom = _load_json_or_issue(BOM_PATH, "release_bom", issues)
+    diff = _load_json_or_issue(DIFF_REPORT_PATH, "openapi_diff_report", issues)
+    manifest = _load_json_or_issue(DIST_CLASSIFIER_MANIFEST_PATH, "openapi_classifier_manifest", issues)
+    schema = _load_json_or_issue(DIST_BOM_SCHEMA_PATH, "release_bom_schema", issues)
+    audit_schema = _load_json_or_issue(DIST_AUDIT_SCHEMA_PATH, "release_audit_failure_schema", issues)
+    if any(x is None for x in (bom, diff, manifest, schema, audit_schema)):
+        if json_mode:
+            _emit_failure_json(issues)
+        else:
+            _emit_failure_human(issues)
+        return 1
 
     # Identity/naming contracts for shipped schemas (release audit kit).
     _check_schema_identity(
-        schema_obj=schema,
+        schema_obj=schema,  # type: ignore[arg-type]
         schema_path="dist/release_bom.schema.json",
         schema_version_re=r"^release_bom_schema_v\d+$",
-        errors_text=errors_text,
-        errors_struct=errors_struct,
+        issues=issues,
     )
     _check_schema_identity(
-        schema_obj=audit_schema,
+        schema_obj=audit_schema,  # type: ignore[arg-type]
         schema_path="dist/release_audit_failure.schema.json",
         schema_version_re=r"^release_audit_failure_schema_v\d+$",
-        errors_text=errors_text,
-        errors_struct=errors_struct,
+        issues=issues,
     )
 
     # Strict meta-validation: schemas must themselves be valid Draft 2020-12 schemas.
     _meta_validate_schema(
-        schema_obj=schema,
+        schema_obj=schema,  # type: ignore[arg-type]
         schema_path="dist/release_bom.schema.json",
-        errors_text=errors_text,
-        errors_struct=errors_struct,
+        issues=issues,
     )
     _meta_validate_schema(
-        schema_obj=audit_schema,
+        schema_obj=audit_schema,  # type: ignore[arg-type]
         schema_path="dist/release_audit_failure.schema.json",
-        errors_text=errors_text,
-        errors_struct=errors_struct,
+        issues=issues,
     )
 
+    # BOM -> dist hash coherence checks
+    artifacts = (bom.get("artifacts") or {})  # type: ignore[union-attr]
+    if not isinstance(artifacts, dict):
+        issues.append(_issue("bom_invalid_artifact", "release_bom.json:artifacts", "object", type(artifacts).__name__))
+    else:
+        # Validate required hash artifacts (paths and sha256 values)
+        _check_bom_artifact_file_sha(artifacts=artifacts, name="openapi_breaking_policy", sha_field="sha256", issues=issues)
+        _check_bom_artifact_file_sha(artifacts=artifacts, name="openapi_before_snapshot", sha_field="sha256", issues=issues)
+        _check_bom_artifact_file_sha(artifacts=artifacts, name="openapi_after_snapshot", sha_field="sha256", issues=issues)
+        _check_bom_artifact_file_sha(artifacts=artifacts, name="openapi_diff_report", sha_field="sha256", issues=issues)
+        _check_bom_artifact_file_sha(artifacts=artifacts, name="openapi_classifier_manifest", sha_field="sha256", issues=issues)
+
+        # Canonical schema hashes + identity match
+        rbs = artifacts.get("release_bom_schema")
+        if not isinstance(rbs, dict):
+            issues.append(_issue("bom_missing_artifact", "release_bom.json:artifacts.release_bom_schema", "object", rbs))
+        else:
+            want = rbs.get("canonical_sha256")
+            got = _sha256_canonical_json_file(DIST_BOM_SCHEMA_PATH)
+            if want != got:
+                issues.append(_issue("bom_hash_mismatch", "release_bom.json:artifacts.release_bom_schema.canonical_sha256", want, got))
+            if rbs.get("$id") != (schema.get("$id") if isinstance(schema, dict) else None):
+                issues.append(_issue("bom_identity_mismatch", "release_bom.json:artifacts.release_bom_schema.$id", rbs.get("$id"), schema.get("$id")))
+            if rbs.get("schema_version") != (schema.get("schema_version") if isinstance(schema, dict) else None):
+                issues.append(_issue("bom_identity_mismatch", "release_bom.json:artifacts.release_bom_schema.schema_version", rbs.get("schema_version"), schema.get("schema_version")))
+
+        ras = artifacts.get("release_audit_failure_schema")
+        if not isinstance(ras, dict):
+            issues.append(_issue("bom_missing_artifact", "release_bom.json:artifacts.release_audit_failure_schema", "object", ras))
+        else:
+            want = ras.get("canonical_sha256")
+            got = _sha256_canonical_json_file(DIST_AUDIT_SCHEMA_PATH)
+            if want != got:
+                issues.append(_issue("bom_hash_mismatch", "release_bom.json:artifacts.release_audit_failure_schema.canonical_sha256", want, got))
+            if ras.get("$id") != (audit_schema.get("$id") if isinstance(audit_schema, dict) else None):
+                issues.append(_issue("bom_identity_mismatch", "release_bom.json:artifacts.release_audit_failure_schema.$id", ras.get("$id"), audit_schema.get("$id")))
+            if ras.get("schema_version") != (audit_schema.get("schema_version") if isinstance(audit_schema, dict) else None):
+                issues.append(_issue("bom_identity_mismatch", "release_bom.json:artifacts.release_audit_failure_schema.schema_version", ras.get("schema_version"), audit_schema.get("schema_version")))
+
+    # Cross-coherence: BOM provenance should match diff report hashes.
+    prov = (bom.get("provenance") or {})  # type: ignore[union-attr]
+    if isinstance(prov, dict) and isinstance(diff, dict):
+        if prov.get("before_openapi_sha256") != diff.get("before_openapi_sha256"):
+            issues.append(_issue(
+                "provenance_mismatch",
+                "release_bom.json:provenance.before_openapi_sha256",
+                prov.get("before_openapi_sha256"),
+                diff.get("before_openapi_sha256"),
+            ))
+        if prov.get("after_openapi_sha256") != diff.get("after_openapi_sha256"):
+            issues.append(_issue(
+                "provenance_mismatch",
+                "release_bom.json:provenance.after_openapi_sha256",
+                prov.get("after_openapi_sha256"),
+                diff.get("after_openapi_sha256"),
+            ))
+
     if issues:
         if json_mode:
             _emit_failure_json(issues)
         else:
             _emit_failure_human(issues)
         return 1
 
     print("[release-bom-consistency] OK")
     return 0
 
 if __name__ == "__main__":
     raise SystemExit(main())
What this accomplishes:
•	No “legacy conversion” step at the end
•	Every error is structured from the moment it’s detected
•	Human output is derived from structured items (and is stable)
________________________________________
Diff D — Validate --json output against its schema in the release workflow (hard gate)
We’ll capture the JSON output to a file on failure, then validate it. In GitHub Actions, a failing step stops the job, so the easiest pattern is:
•	run the consistency check in a step that always captures output
•	if it failed, validate the captured output and then fail
Patch: .github/workflows/release.yml
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index ffffffff..00000001 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@
       - name: Release BOM consistency check (hard gate, JSON)
         env:
           CI: "true"
         shell: bash
         run: |
-          python scripts/check_release_bom_consistency.py --json
+          set +e
+          python scripts/check_release_bom_consistency.py --json 2> dist/bom_consistency_result.json
+          status=$?
+          set -e
+          if [ "$status" -ne 0 ]; then
+            python -m pip install jsonschema
+            python scripts/validate_release_bom_consistency_result.py dist/bom_consistency_result.json
+            cat dist/bom_consistency_result.json >&2
+            exit "$status"
+          fi
This guarantees:
•	if the gate fails, the JSON payload is validated against the schema before the workflow fails
•	the validated JSON is printed for debugging
•	on success, no file is created (or it will be empty; if you want zero artifacts, you can delete it when status=0)
If you want to avoid creating dist/bom_consistency_result.json on success, add:
rm -f dist/bom_consistency_result.json
in the success path.
________________________________________
Diff E — (Optional) Add schemas/release_bom_consistency_result.schema.json to the BOM manifest gate
Since this schema is now part of the release governance surface, it should be manifest-locked like the other release schemas.
If you want that, say: “next diff: manifest-lock bom_consistency_result schema” and I’ll ship it.
________________________________________
What’s now true
•	The gate’s own output is stable, structured, and schema-governed.
•	Anyone can parse CI failures reliably without log scraping.
•	The schema meta-validation failures are normalized (no repr noise).
•	You’ve reached “contracted observability” for the release hardening surface.
________________________________________
Next diff options
1.	manifest-lock bom_consistency_result schema (recommended)
2.	extend the same schema-governed JSON discipline to other gate scripts (OpenAPI release gate, BOM generator preflight, etc.)

